{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Handwritten Digits - Project-1 - Tensorboard Introduction\n",
        "- Tensorflow and Keras to create neural network\n",
        "  - takes input output and layers details like\n",
        "    - activcation fucntion\n",
        "    - loss fucntion\n",
        "    - accuracy metrics\n",
        "- Confusion matrix between predcited and test\n",
        "- plot using sns"
      ],
      "metadata": {
        "id": "zzJ3VuJ_i3eh"
      },
      "id": "zzJ3VuJ_i3eh"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "#It tells the notebook to display matplotlib plots directly below the code cells that produce them, rather than in a separate window.\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "-ufdhgfNi7na"
      },
      "id": "-ufdhgfNi7na",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist"
      ],
      "metadata": {
        "id": "ekH16FdAkBgR"
      },
      "id": "ekH16FdAkBgR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Databaset using Keras\n",
        "- Train and Split in Keras"
      ],
      "metadata": {
        "id": "F7BYfvKplv5b"
      },
      "id": "F7BYfvKplv5b"
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "THJm4jCkkRA_"
      },
      "id": "THJm4jCkkRA_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Each Image as 2D array of 28x28\n",
        "\n",
        "len(x_train)\n",
        "len(y_train)\n",
        "len(x_test)\n",
        "# X train and y_train are 2d arrays"
      ],
      "metadata": {
        "id": "YqR4ZzMvkl35"
      },
      "id": "YqR4ZzMvkl35",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MNIST\n",
        "- Handwritten digits data in 2D form\n",
        "- 0 means black 255 means white"
      ],
      "metadata": {
        "id": "u430bXOckukr"
      },
      "id": "u430bXOckukr"
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x_train[0]))\n",
        "print(x_train[0])\n",
        "# This is 2D array"
      ],
      "metadata": {
        "id": "KBkqqsfQk18g"
      },
      "id": "KBkqqsfQk18g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### See Image using Matplot\n",
        "- plt.matshow(2D array)"
      ],
      "metadata": {
        "id": "dXyHJd_vlMOT"
      },
      "id": "dXyHJd_vlMOT"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.matshow(x_train[100])\n",
        "print(y_train[100])"
      ],
      "metadata": {
        "id": "_ecDtgXFlQUR"
      },
      "id": "_ecDtgXFlQUR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Flatten 2D array 28X28 into 1D array"
      ],
      "metadata": {
        "id": "4auSI99-l1ZN"
      },
      "id": "4auSI99-l1ZN"
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape\n",
        "# number of rows\n",
        "# each rows is 2d array"
      ],
      "metadata": {
        "id": "LRl59aQGl6rT"
      },
      "id": "LRl59aQGl6rT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Want (60000,784)\n",
        "x_train_flattened = x_train.reshape(len(x_train),28*28)\n",
        "x_test_flattened = x_test.reshape(len(x_test),28*28)\n",
        "x_train_flattened.shape"
      ],
      "metadata": {
        "id": "G8uE_FGTl53q"
      },
      "id": "G8uE_FGTl53q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_flattened[0]\n",
        "print(len(x_train_flattened[0]))"
      ],
      "metadata": {
        "id": "n3vm7uMkmXJW"
      },
      "id": "n3vm7uMkmXJW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scaling the x_train data between 0 to 1\n",
        "- Divide by 255"
      ],
      "metadata": {
        "id": "OBbKyyCEpoNF"
      },
      "id": "OBbKyyCEpoNF"
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_flattened = x_train_flattened/255"
      ],
      "metadata": {
        "id": "5f1ULrfJptF0"
      },
      "id": "5f1ULrfJptF0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[0:10]"
      ],
      "metadata": {
        "id": "wxiiuO_-613J"
      },
      "id": "wxiiuO_-613J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keras to Create Neural Network\n",
        "- want 784 as x1 x2 ... .x784 to connect with 10 ouputs 0 to 1\n",
        "- acitvation function = sigmoid\n",
        "- Then training using data yeh sab\n",
        "\n",
        "- Single layer input and output for now"
      ],
      "metadata": {
        "id": "c7QkkqnOnE7t"
      },
      "id": "c7QkkqnOnE7t"
    },
    {
      "cell_type": "code",
      "source": [
        "# Dense(10): A fully connected layer with 10 neurons\n",
        "# model = keras.Sequential([\n",
        "#     keras.layers.Dense(10,input_shape=(784,),activation='softmax')\n",
        "# ])\n",
        "\n",
        "# If dont want to flatten image tell its 28*28\n",
        "# Multiple layers hit and trial\n",
        "model = keras.Sequential([\n",
        "      # keras.layers.Flatten(input_shape=(28,28))\n",
        "    keras.layers.Dense(100,input_shape=(784,),activation='relu'),\n",
        "    keras.layers.Dense(10,activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "# optimizer backward going to optain weight attain global minima\n",
        "# loss fucntion ouput and predicted difference used to attain weights\n",
        "# Gradient decent and cost function\n",
        "\n",
        "# metric when neural network compile goal is accuracy input output differtence\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        "    )\n",
        "# epochs number of iteration the neural network going to train data\n",
        "model.fit(x_train_flattened,y_train,epochs=5)"
      ],
      "metadata": {
        "id": "gj94-wPHnNlx"
      },
      "id": "gj94-wPHnNlx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating | Predicition Keral Model on Test Dataset"
      ],
      "metadata": {
        "id": "s3rI94vup9Ex"
      },
      "id": "s3rI94vup9Ex"
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x_test_flattened,y_test)"
      ],
      "metadata": {
        "id": "S_nEsCthqARu"
      },
      "id": "S_nEsCthqARu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.matshow(x_test[50])"
      ],
      "metadata": {
        "id": "02__G9G_qrrV"
      },
      "id": "02__G9G_qrrV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict= model.predict(x_test_flattened)\n",
        "print(y_predict[50])\n",
        "# This is 10 neurons output probability"
      ],
      "metadata": {
        "id": "D7x7ovHQrXty"
      },
      "id": "D7x7ovHQrXty",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exact ouput based on 10 neurons\n",
        "# np.argmax returns the index of the max probability, i.e., the predicted class.\n",
        "print(np.argmax(y_predict[50]))"
      ],
      "metadata": {
        "id": "Co7wulzurnLG"
      },
      "id": "Co7wulzurnLG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test[49:52]"
      ],
      "metadata": {
        "id": "kpWS0X_wtEGz"
      },
      "id": "kpWS0X_wtEGz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert y_predicted into concrete class similar to y_test"
      ],
      "metadata": {
        "id": "mwHX7hH_tNsk"
      },
      "id": "mwHX7hH_tNsk"
    },
    {
      "cell_type": "code",
      "source": [
        "y_predicted_labels = np.argmax(y_predict, axis=1)"
      ],
      "metadata": {
        "id": "4AmrvBjatVGS"
      },
      "id": "4AmrvBjatVGS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predicted_labels[49:52]"
      ],
      "metadata": {
        "id": "2v1sOa6ytWk0"
      },
      "id": "2v1sOa6ytWk0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Confusion Matrix"
      ],
      "metadata": {
        "id": "DT5ArGQes7Rv"
      },
      "id": "DT5ArGQes7Rv"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_predicted_labels)\n",
        "print(conf_matrix)\n",
        "\n",
        "# confusion_matrix = tf.math.confusion.matrix(labels=y_test, predictions=y_predicted_labels)\n",
        "# confusion_matrix"
      ],
      "metadata": {
        "id": "xa-FD69Ms-Q0"
      },
      "id": "xa-FD69Ms-Q0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize using seaborn"
      ],
      "metadata": {
        "id": "ij_jRKB2t2Bm"
      },
      "id": "ij_jRKB2t2Bm"
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "VIs7_8wwt4z9"
      },
      "id": "VIs7_8wwt4z9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=np.arange(10), yticklabels=np.arange(10))\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('MNIST Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WVZur1qZuJh_"
      },
      "id": "WVZur1qZuJh_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PKQFAFOYvT46"
      },
      "id": "PKQFAFOYvT46"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorboard\n",
        "- Visualize tensorflow model accuracy loss etc"
      ],
      "metadata": {
        "id": "eM7ewKK-LFdX"
      },
      "id": "eM7ewKK-LFdX"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(100,input_shape=(784,),activation='relu'),\n",
        "    keras.layers.Dense(10,activation='softmax')\n",
        "])\n",
        "\n",
        "# Tensorboard callback\n",
        "tf_callback= tf.keras.callbacks.TensorBoard(log_dir='/content/logs/',histogram_freq=1)\n",
        "\n",
        "model.compile(\n",
        "    optimizer='SGD',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        "    )\n",
        "# Tensorboard use in fit\n",
        "model.fit(x_train_flattened,y_train,epochs=5,callbacks=[tf_callback])"
      ],
      "metadata": {
        "id": "rR7A-Oj5LN3E"
      },
      "id": "rR7A-Oj5LN3E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize Tensorboard File"
      ],
      "metadata": {
        "id": "F2-TITf0MQMH"
      },
      "id": "F2-TITf0MQMH"
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/logs/"
      ],
      "metadata": {
        "id": "QRiQN6NDMPri"
      },
      "id": "QRiQN6NDMPri",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activation Functions\n",
        "- Use Sigmoid at output layer = [probability 0,1]\n",
        "- Between layers use tanh\n",
        "\n",
        "- Not sure of activation Use Relu max(0,x)\n",
        "- Leaky Relu max(01.x,x)"
      ],
      "metadata": {
        "id": "FRWn90lg-x26"
      },
      "id": "FRWn90lg-x26"
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "neNPjeFr_Ajf"
      },
      "id": "neNPjeFr_Ajf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid\n",
        "def sigmoid(x):\n",
        "  return 1/(1+math.exp(-x))"
      ],
      "metadata": {
        "id": "GQyZg7fs-xag"
      },
      "id": "GQyZg7fs-xag",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sigmoid(-0)"
      ],
      "metadata": {
        "id": "ZR8ikMnD_HGg"
      },
      "id": "ZR8ikMnD_HGg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tanh\n",
        "def tanh(x):\n",
        "  return (math.exp(x)-math.exp(-x))/(math.exp(x)+math.exp(-x))"
      ],
      "metadata": {
        "id": "DyWzbfP4_I5W"
      },
      "id": "DyWzbfP4_I5W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tanh(100)"
      ],
      "metadata": {
        "id": "E6zWV6VH_VKh"
      },
      "id": "E6zWV6VH_VKh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RELU\n",
        "def relu(x):\n",
        "  return max(0,x)\n",
        "def leaky_relu(x):\n",
        "  return max(0.1*x,x)"
      ],
      "metadata": {
        "id": "VEvHJrOp_W-I"
      },
      "id": "VEvHJrOp_W-I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relu(0.5)\n",
        "leaky_relu(-0.5)"
      ],
      "metadata": {
        "id": "ixanqQEn_aZq"
      },
      "id": "ixanqQEn_aZq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent in Neural\n",
        "- Keras directly model leke train and fit\n",
        "- get weights and bias after epochs\n",
        "- Python simple implementation for x and y matrix to get weights"
      ],
      "metadata": {
        "id": "oanPGZ6X0x9n"
      },
      "id": "oanPGZ6X0x9n"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "vtQOfIdh063d"
      },
      "id": "vtQOfIdh063d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"insurance_data.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "6JwCLWHT2Hn7"
      },
      "id": "6JwCLWHT2Hn7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = df[['age','affordibility']]\n",
        "x.head()\n",
        "y= df['bought_insurance']\n",
        "y.head()\n",
        "x.shape"
      ],
      "metadata": {
        "id": "1xAyVnXX2I4H"
      },
      "id": "1xAyVnXX2I4H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and Test and Scaling Data"
      ],
      "metadata": {
        "id": "T5LKOnTd2ZKF"
      },
      "id": "T5LKOnTd2ZKF"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[['age','affordibility']],df.bought_insurance,test_size=0.2, random_state=25)\n"
      ],
      "metadata": {
        "id": "ZOr0v6wl2XYe"
      },
      "id": "ZOr0v6wl2XYe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Scaling Data Age / 100"
      ],
      "metadata": {
        "id": "T5pcCCtC2nOS"
      },
      "id": "T5pcCCtC2nOS"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train_scaled = X_train.copy()\n",
        "X_train_scaled['age'] = X_train_scaled['age'] / 100\n",
        "\n",
        "X_test_scaled = X_test.copy()\n",
        "X_test_scaled['age'] = X_test_scaled['age'] / 100"
      ],
      "metadata": {
        "id": "NTcrwM2V2e3N"
      },
      "id": "NTcrwM2V2e3N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled.head()"
      ],
      "metadata": {
        "id": "feS8dMx32jwL"
      },
      "id": "feS8dMx32jwL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model-1 Keras and Tensorflow"
      ],
      "metadata": {
        "id": "kZIGsH_F2rOE"
      },
      "id": "kZIGsH_F2rOE"
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs to one output can use model from sklearn like logisitc but can be converted into\n",
        "# neurals if written in this way\n",
        "\n",
        "# Logistic Regression in Keras = Neural Network with:\n",
        "# 1 Dense layer\n",
        "\n",
        "# No hidden layers\n",
        "\n",
        "# Sigmoid activation (for binary classification)\n",
        "\n",
        "# Binary crossentropy loss\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(1, input_shape=(2,), activation='sigmoid', kernel_initializer='ones', bias_initializer='zeros')\n",
        "])\n",
        "\n",
        "# since logistic regression hai loss fucntion is log loss\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train_scaled, y_train, epochs=500)"
      ],
      "metadata": {
        "id": "xVj-5sNq2ubP"
      },
      "id": "xVj-5sNq2ubP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate the model"
      ],
      "metadata": {
        "id": "Cg3m7PMb3yQZ"
      },
      "id": "Cg3m7PMb3yQZ"
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test_scaled,y_test)"
      ],
      "metadata": {
        "id": "tyfGopKJ31Sb"
      },
      "id": "tyfGopKJ31Sb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "A9SAZx8y33MT"
      },
      "id": "A9SAZx8y33MT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_scaled"
      ],
      "metadata": {
        "id": "-BfefXEX37HE"
      },
      "id": "-BfefXEX37HE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "id": "Uduloo1k4Gk8"
      },
      "id": "Uduloo1k4Gk8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coeff and Bias from the model which was trained\n",
        "- This means w1=5.060867, w2=1.4086502, bias =-2.9137027"
      ],
      "metadata": {
        "id": "55lII7064LAy"
      },
      "id": "55lII7064LAy"
    },
    {
      "cell_type": "code",
      "source": [
        "coef, intercept = model.get_weights()\n",
        "coef, intercept"
      ],
      "metadata": {
        "id": "Kc2aWP_k4OTK"
      },
      "id": "Kc2aWP_k4OTK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model-2 Using Python Done in Siddharsan Video\n",
        "- Loss function know\n",
        "- x y w1 w2 b and gradiet descent is know\n",
        "- learning rate and number of iteration known"
      ],
      "metadata": {
        "id": "3EMoc4FK4UlB"
      },
      "id": "3EMoc4FK4UlB"
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_numpy(X):\n",
        "   return 1/(1+np.exp(-X))\n",
        "\n",
        "sigmoid_numpy(np.array([12,0,1]))"
      ],
      "metadata": {
        "id": "1pBwfTfJ4ekF"
      },
      "id": "1pBwfTfJ4ekF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_loss(y_true, y_predicted):\n",
        "    epsilon = 1e-15\n",
        "    y_predicted_new = [max(i,epsilon) for i in y_predicted]\n",
        "    y_predicted_new = [min(i,1-epsilon) for i in y_predicted_new]\n",
        "    y_predicted_new = np.array(y_predicted_new)\n",
        "    return -np.mean(y_true*np.log(y_predicted_new)+(1-y_true)*np.log(1-y_predicted_new))"
      ],
      "metadata": {
        "id": "ntTMVSqx5Mtf"
      },
      "id": "ntTMVSqx5Mtf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(age, affordability, y_true, epochs, loss_thresold):\n",
        "    w1 = w2 = 1\n",
        "    bias = 0\n",
        "    rate = 0.5 #Learning rate fix\n",
        "    n = len(age)\n",
        "    for i in range(epochs):\n",
        "        weighted_sum = w1 * age + w2 * affordability + bias\n",
        "        y_predicted = sigmoid_numpy(weighted_sum)\n",
        "        loss = log_loss(y_true, y_predicted)\n",
        "\n",
        "        w1d = (1/n)*np.dot(np.transpose(age),(y_predicted-y_true))\n",
        "        w2d = (1/n)*np.dot(np.transpose(affordability),(y_predicted-y_true))\n",
        "\n",
        "        bias_d = np.mean(y_predicted-y_true)\n",
        "        w1 = w1 - rate * w1d\n",
        "        w2 = w2 - rate * w2d\n",
        "        bias = bias - rate * bias_d\n",
        "\n",
        "        print (f'Epoch:{i}, w1:{w1}, w2:{w2}, bias:{bias}, loss:{loss}')\n",
        "\n",
        "        if loss<=loss_thresold:\n",
        "            break\n",
        "\n",
        "    return w1, w2, bias"
      ],
      "metadata": {
        "id": "9c3YiUK25XuG"
      },
      "id": "9c3YiUK25XuG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_descent(X_train_scaled['age'],X_train_scaled['affordibility'],y_train,500, 0.4631)"
      ],
      "metadata": {
        "id": "UdpONgyB5h2B"
      },
      "id": "UdpONgyB5h2B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coef, intercept"
      ],
      "metadata": {
        "id": "hblwdfW552MY"
      },
      "id": "hblwdfW552MY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network Implement Python\n",
        "- want fit, predict functions like models have inbuild"
      ],
      "metadata": {
        "id": "tJTB98TfoBYX"
      },
      "id": "tJTB98TfoBYX"
    },
    {
      "cell_type": "code",
      "source": [
        "class my_neural_network():\n",
        "  def __init__(self):\n",
        "    self.w1 = 1\n",
        "    self.w2=1\n",
        "    self.bias=0\n",
        "\n",
        "  def gradient_descent(self,age, affordability, y_true, epochs, loss_thresold):\n",
        "    w1 = w2 = 1\n",
        "    bias = 0\n",
        "    rate = 0.5 #Learning rate fix\n",
        "    n = len(age)\n",
        "    for i in range(epochs):\n",
        "        weighted_sum = w1 * age + w2 * affordability + bias\n",
        "        y_predicted = self.sigmoid_numpy(weighted_sum)\n",
        "        loss = self.log_loss(y_true, y_predicted)\n",
        "\n",
        "        w1d = (1/n)*np.dot(np.transpose(age),(y_predicted-y_true))\n",
        "        w2d = (1/n)*np.dot(np.transpose(affordability),(y_predicted-y_true))\n",
        "\n",
        "        bias_d = np.mean(y_predicted-y_true)\n",
        "        w1 = w1 - rate * w1d\n",
        "        w2 = w2 - rate * w2d\n",
        "        bias = bias - rate * bias_d\n",
        "        if i%10==0:\n",
        "          print (f'Epoch:{i}, w1:{w1}, w2:{w2}, bias:{bias}, loss:{loss}')\n",
        "\n",
        "        if loss<=loss_thresold:\n",
        "            break\n",
        "\n",
        "    return w1, w2, bias\n",
        "\n",
        "  def fit(self, x, y, epochs, loss_threshold):\n",
        "    self.w1 , self.w2, self.bias =self.gradient_descent(x['age'],x['affordibility'],y, epochs, loss_threshold)\n",
        "\n",
        "  def log_loss(self,y_true, y_predicted):\n",
        "    epsilon = 1e-15\n",
        "    y_predicted_new = [max(i,epsilon) for i in y_predicted]\n",
        "    y_predicted_new = [min(i,1-epsilon) for i in y_predicted_new]\n",
        "    y_predicted_new = np.array(y_predicted_new)\n",
        "    return -np.mean(y_true*np.log(y_predicted_new)+(1-y_true)*np.log(1-y_predicted_new))\n",
        "\n",
        "  def sigmoid_numpy(self,X):\n",
        "   return 1/(1+np.exp(-X))\n",
        "\n",
        "  def predict(self,x_test):\n",
        "    weighted_sum = self.w1*x_test['age'] + self.w2*x_test['affordibility'] + self.bias\n",
        "    return self.sigmoid_numpy(weighted_sum)"
      ],
      "metadata": {
        "id": "Ftm-7fnxoATm"
      },
      "id": "Ftm-7fnxoATm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customModel= my_neural_network()\n",
        "customModel.fit(X_train_scaled,y_train,epochs=100,loss_threshold=0.4631)"
      ],
      "metadata": {
        "id": "ataWKT6Iow0Q"
      },
      "id": "ataWKT6Iow0Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customModel.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "HVZpfAw0q69Z"
      },
      "id": "HVZpfAw0q69Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradien Descent Types\n",
        "- Bathc Gradient Descent\n",
        "  - All data points for every epoch goes through and calculate loss and adjust weight like we did before\n",
        "  - Problem 10millions rows and 5000 featutes then too much computation\n",
        "  - Good for small data\n",
        "- Stochastic Gradient\n",
        "  - use one(randomply picked) - loss y predict and adjust weight on every epochs\n",
        "  - good for high data\n",
        "- Mini Batch\n",
        "  - Similar to stochatic\n",
        "  - instead of one data use batch of some data"
      ],
      "metadata": {
        "id": "T_lznNSysN6C"
      },
      "id": "T_lznNSysN6C"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "r6DmOU9zsNaQ"
      },
      "id": "r6DmOU9zsNaQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"homeprices_banglore.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "7YAb1oQzugvo"
      },
      "id": "7YAb1oQzugvo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing data - Standard Scaler"
      ],
      "metadata": {
        "id": "PIFJvTMZvJ46"
      },
      "id": "PIFJvTMZvJ46"
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "4e79jUFvvO1F"
      },
      "id": "4e79jUFvvO1F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "c0y4So5VwLK1"
      },
      "id": "c0y4So5VwLK1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "sx = preprocessing.MinMaxScaler()\n",
        "sy = preprocessing.MinMaxScaler()\n",
        "\n",
        "x= df.drop('price',axis=1)\n",
        "y= df['price']\n",
        "scaled_X = sx.fit_transform(x)\n",
        "scaled_y = sy.fit_transform(y.values.reshape(-1,1))\n",
        "# -1 all rows and 1 means one column (rows,feature)\n",
        "scaled_X\n",
        "scaled_y"
      ],
      "metadata": {
        "id": "rMRi7wyBvSsd"
      },
      "id": "rMRi7wyBvSsd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert y to single array again\n",
        "scaled_y = scaled_y.flatten()\n",
        "scaled_y"
      ],
      "metadata": {
        "id": "LSDCTQ6dvxEx"
      },
      "id": "LSDCTQ6dvxEx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.ones(shape=(2,2))"
      ],
      "metadata": {
        "id": "Oue4RgD957p4"
      },
      "id": "Oue4RgD957p4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch Descent Gradient\n",
        "- use all data point of x to go through weights and bias\n",
        "- since our data is continuos\n",
        "- cost function is mean squared error\n",
        "- dJ/dw and dJ/db\n",
        "  - w_grad = -(2/total_samples)*(X.T.dot(y_true-y_predicted))\n",
        "  - b_grad = -(2/total_samples)*np.sum(y_true-y_predicted)\n",
        "        "
      ],
      "metadata": {
        "id": "yEyqE5Q85-Hr"
      },
      "id": "yEyqE5Q85-Hr"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "number_of_features = scaled_X.shape[1]\n",
        "print(scaled_X.T.shape)\n",
        "  # numpy array with 1 row and columns equal to number of features. In\n",
        "    # our case number_of_features = 2 (area, bedroom)\n",
        "w = np.ones(shape=(number_of_features))\n",
        "w.shape\n",
        "w"
      ],
      "metadata": {
        "id": "YpKOlMKf6_RN"
      },
      "id": "YpKOlMKf6_RN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C35iZirQ7Rg_"
      },
      "id": "C35iZirQ7Rg_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_gradient_descent(X, y_true, epochs, learning_rate = 0.01):\n",
        "\n",
        "    number_of_features = X.shape[1]\n",
        "    # numpy array with 1 row and columns equal to number of features. In\n",
        "    # our case number_of_features = 2 (area, bedroom)\n",
        "    w = np.ones(shape=(number_of_features))  # 1Xfeautures\n",
        "    b = 0\n",
        "    total_samples = X.shape[0] # number of rows in X\n",
        "\n",
        "    cost_list = []\n",
        "    epoch_list = []\n",
        "\n",
        "    for i in range(epochs):\n",
        "        y_predicted = np.dot(w, X.T) + b\n",
        "\n",
        "        w_grad = -(2/total_samples)*(X.T.dot(y_true-y_predicted))\n",
        "        b_grad = -(2/total_samples)*np.sum(y_true-y_predicted)\n",
        "\n",
        "        w = w - learning_rate * w_grad\n",
        "        b = b - learning_rate * b_grad\n",
        "\n",
        "        cost = np.mean(np.square(y_true-y_predicted)) # MSE (Mean Squared Error)\n",
        "\n",
        "        if i%10==0:\n",
        "            cost_list.append(cost)\n",
        "            epoch_list.append(i)\n",
        "\n",
        "    return w, b, cost, cost_list, epoch_list\n",
        "\n",
        "w, b, cost, cost_list, epoch_list = batch_gradient_descent(scaled_X,scaled_y.reshape(scaled_y.shape[0],),500)\n",
        "w, b, cost"
      ],
      "metadata": {
        "id": "OEFk2a0V6JcY"
      },
      "id": "OEFk2a0V6JcY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"cost\")\n",
        "plt.plot(epoch_list,cost_list)"
      ],
      "metadata": {
        "id": "K5Sp4-_a8QJg"
      },
      "id": "K5Sp4-_a8QJg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Predict Values using this model weight and bias"
      ],
      "metadata": {
        "id": "H3VqZ9xz82cC"
      },
      "id": "H3VqZ9xz82cC"
    },
    {
      "cell_type": "code",
      "source": [
        "check = sx.transform([[2400, 4]])[0]\n",
        "check\n",
        "# array([[0.55172414, 0.75      ]])\n",
        "# need first array\n",
        "\n",
        "price = w[0] * check[0] + w[1] * check[1] + b\n",
        "price"
      ],
      "metadata": {
        "id": "VqG-T5b_9hTz"
      },
      "id": "VqG-T5b_9hTz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn import preprocessing\n",
        "# sx = preprocessing.MinMaxScaler()\n",
        "# sy = preprocessing.MinMaxScaler()\n",
        "# used for training and standarisation\n",
        "\n",
        "def predict(area,bedrooms,w,b):\n",
        "\n",
        "    scaled_X = sx.transform([[area, bedrooms]])[0]\n",
        "    # pass array of 2d return array of 2d need firrst element\n",
        "\n",
        "    # here w1 = w[0] , w2 = w[1], w3 = w[2] and bias is b\n",
        "    # equation for price is w1*area + w2*bedrooms + w3*age + bias\n",
        "    # scaled_X[0] is area\n",
        "    # scaled_X[1] is bedrooms\n",
        "    # scaled_X[2] is age\n",
        "    scaled_price = w[0] * scaled_X[0] + w[1] * scaled_X[1] + b\n",
        "    # once we get price prediction we need to to rescal it back to original value\n",
        "    # also since it returns 2D array, to get single value we need to do value[0][0]\n",
        "    return sy.inverse_transform([[scaled_price]])[0][0]\n",
        "\n",
        "predict(1056,2,w,b)\n",
        "# w and b from gradient descent"
      ],
      "metadata": {
        "id": "J88oY4cH86qn"
      },
      "id": "J88oY4cH86qn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stochastic Descent Gradient\n",
        "- one random sample each epochs"
      ],
      "metadata": {
        "id": "lLh51fXg-mUf"
      },
      "id": "lLh51fXg-mUf"
    },
    {
      "cell_type": "code",
      "source": [
        "# we will use random libary to pick random training sample.\n",
        "import random\n",
        "random.randint(0,6)\n",
        "\n",
        "\n",
        "# scaled_y.reshape(scaled_y.shape[0],)\n",
        "\n",
        "#2d array (20, 1) needed for tranformation\n",
        "scaled_y.shape\n",
        "\n",
        "# convert to single array (20,)\n",
        "a= scaled_y.reshape(scaled_y.shape[0],)\n",
        "a.shape\n",
        "\n",
        "number_of_features = scaled_X.shape[1]\n",
        "w = np.ones(shape=(number_of_features))\n",
        "sample_x = scaled_X[3]\n",
        "\n",
        "c= np.dot(w, sample_x.T)\n",
        "c"
      ],
      "metadata": {
        "id": "Ds1hDFrs-vGQ"
      },
      "id": "Ds1hDFrs-vGQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stochastic_gradient_descent(X, y_true, epochs, learning_rate = 0.01):\n",
        "\n",
        "    number_of_features = X.shape[1]\n",
        "    # numpy array with 1 row and columns equal to number of features. In\n",
        "    # our case number_of_features = 3 (area, bedroom and age)\n",
        "    w = np.ones(shape=(number_of_features))\n",
        "    b = 0\n",
        "    total_samples = X.shape[0]\n",
        "\n",
        "    cost_list = []\n",
        "    epoch_list = []\n",
        "\n",
        "    for i in range(epochs):\n",
        "        # here randomly one sample le rahe\n",
        "        random_index = random.randint(0,total_samples-1) # random index from total samples\n",
        "        sample_x = X[random_index]\n",
        "        sample_y = y_true[random_index]\n",
        "        # Both sample y and y_predicted is single float value\n",
        "        y_predicted = np.dot(w, sample_x.T) + b\n",
        "\n",
        "        w_grad = -(2/total_samples)*(sample_x.T.dot(sample_y-y_predicted))\n",
        "        b_grad = -(2/total_samples)*(sample_y-y_predicted)\n",
        "\n",
        "        w = w - learning_rate * w_grad\n",
        "        b = b - learning_rate * b_grad\n",
        "\n",
        "        cost = np.square(sample_y-y_predicted)\n",
        "\n",
        "        if i%100==0: # at every 100th iteration record the cost and epoch value\n",
        "            cost_list.append(cost)\n",
        "            epoch_list.append(i)\n",
        "\n",
        "    return w, b, cost, cost_list, epoch_list\n",
        "\n",
        "w_sgd, b_sgd, cost_sgd, cost_list_sgd, epoch_list_sgd = stochastic_gradient_descent(scaled_X,scaled_y.reshape(scaled_y.shape[0],),10000)\n",
        "w_sgd, b_sgd, cost_sgd"
      ],
      "metadata": {
        "id": "1lTI4_yA-v7T"
      },
      "id": "1lTI4_yA-v7T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"cost\")\n",
        "plt.plot(epoch_list_sgd,cost_list_sgd)"
      ],
      "metadata": {
        "id": "pMuIVhyBBuX2"
      },
      "id": "pMuIVhyBBuX2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(1000,2,w_sgd, b_sgd)"
      ],
      "metadata": {
        "id": "He4H07jPCo_p"
      },
      "id": "He4H07jPCo_p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mini-Batch Gradient Descent\n",
        "- Batch of 20 samples aise lo"
      ],
      "metadata": {
        "id": "1wss5SXIC3qs"
      },
      "id": "1wss5SXIC3qs"
    },
    {
      "cell_type": "code",
      "source": [
        "random_indices= np.random.permutation(20)\n",
        "X_tmp = scaled_X[random_indices]\n",
        "X_tmp\n",
        "y_tmp = scaled_y.reshape(scaled_y.shape[0],)[random_indices]\n",
        "y_tmp"
      ],
      "metadata": {
        "id": "_6sRwnCuC5yl"
      },
      "id": "_6sRwnCuC5yl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mini_batch_gradient_descent(X, y_true, epochs = 100, batch_size = 5, learning_rate = 0.01):\n",
        "\n",
        "    number_of_features = X.shape[1]\n",
        "    # numpy array with 1 row and columns equal to number of features. In\n",
        "    # our case number_of_features = 3 (area, bedroom and age)\n",
        "    w = np.ones(shape=(number_of_features))\n",
        "    b = 0\n",
        "    total_samples = X.shape[0] # number of rows in X\n",
        "\n",
        "    if batch_size > total_samples: # In this case mini batch becomes same as batch gradient descent\n",
        "        batch_size = total_samples\n",
        "\n",
        "    cost_list = []\n",
        "    epoch_list = []\n",
        "\n",
        "    num_batches = int(total_samples/batch_size)\n",
        "\n",
        "    for i in range(epochs):\n",
        "        random_indices = np.random.permutation(total_samples)\n",
        "        X_tmp = X[random_indices]\n",
        "        y_tmp = y_true[random_indices]\n",
        "\n",
        "        for j in range(0,total_samples,batch_size):\n",
        "            Xj = X_tmp[j:j+batch_size]\n",
        "            yj = y_tmp[j:j+batch_size]\n",
        "            y_predicted = np.dot(w, Xj.T) + b\n",
        "\n",
        "            w_grad = -(2/len(Xj))*(Xj.T.dot(yj-y_predicted))\n",
        "            b_grad = -(2/len(Xj))*np.sum(yj-y_predicted)\n",
        "\n",
        "            w = w - learning_rate * w_grad\n",
        "            b = b - learning_rate * b_grad\n",
        "\n",
        "            cost = np.mean(np.square(yj-y_predicted)) # MSE (Mean Squared Error)\n",
        "\n",
        "        if i%10==0:\n",
        "            cost_list.append(cost)\n",
        "            epoch_list.append(i)\n",
        "\n",
        "    return w, b, cost, cost_list, epoch_list\n",
        "\n",
        "w, b, cost, cost_list, epoch_list = mini_batch_gradient_descent(\n",
        "    scaled_X,\n",
        "    scaled_y.reshape(scaled_y.shape[0],),\n",
        "    epochs = 120,\n",
        "    batch_size = 5\n",
        ")\n",
        "w, b, cost"
      ],
      "metadata": {
        "id": "AnWNYKbdDD23"
      },
      "id": "AnWNYKbdDD23",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"cost\")\n",
        "plt.plot(epoch_list,cost_list)"
      ],
      "metadata": {
        "id": "0z-4FBmZD5Ys"
      },
      "id": "0z-4FBmZD5Ys",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPU bench-marking with image classification PROJECT-2\n",
        "- CIFAR-10 dataset -"
      ],
      "metadata": {
        "id": "fnFeP4GIxjqY"
      },
      "id": "fnFeP4GIxjqY"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# Version Information\n",
        "# tensorflow 2.2.0 , Cudnn7.6.5 and Cuda 10.1 , python 3.8"
      ],
      "metadata": {
        "id": "z5PrzP-nx7Mf"
      },
      "id": "z5PrzP-nx7Mf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get GPU information on you computer\n",
        "tf.config.experimental.list_physical_devices()"
      ],
      "metadata": {
        "id": "0SpAbgY8yFdA"
      },
      "id": "0SpAbgY8yFdA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "metadata": {
        "id": "M6l2FrGFyVPb"
      },
      "id": "M6l2FrGFyVPb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Means for Deep Learning can use GPU\n",
        "tf.test.is_built_with_cuda()"
      ],
      "metadata": {
        "id": "3DcnziPSyWIt"
      },
      "id": "3DcnziPSyWIt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "- Keras\n",
        "  - CIFAR-10 dataset - 60k images 10classes\n",
        "    - airplance, bird, cat, dog, horse\n",
        "  - Use Artifical Neural Network ANN\n",
        "  - Colourful images is made of [Red, Green, Blue] array\n",
        "  - Like handdigit project uses [white and black]"
      ],
      "metadata": {
        "id": "6Zy8Ldolyepe"
      },
      "id": "6Zy8Ldolyepe"
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test,y_test) = tf.keras.datasets.cifar10.load_data()"
      ],
      "metadata": {
        "id": "nr_a5dJzytyR"
      },
      "id": "nr_a5dJzytyR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape # (50000, 32, 32, 3)\n",
        "X_train[0][31][30]\n",
        "# ndarray (32, 32, 3)\n",
        "# each image is 32 rows 32 columns and each pixel is [R G B] value\n",
        "# 0 to 255 values for each R G B"
      ],
      "metadata": {
        "id": "t_DmFzn6zESq"
      },
      "id": "t_DmFzn6zESq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figsize=(10,1)\n",
        "plt.imshow(X_train[4])"
      ],
      "metadata": {
        "id": "aUzs1NVD0kjE"
      },
      "id": "aUzs1NVD0kjE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Y already into 10 classes\n",
        "y_train.shape #(50000, 1) 2d array hai single column\n",
        "y_train[0]\n",
        "y_train[0][0]"
      ],
      "metadata": {
        "id": "wzKOsNxdzGmQ"
      },
      "id": "wzKOsNxdzGmQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"]"
      ],
      "metadata": {
        "id": "s3gnjQe61Ce8"
      },
      "id": "s3gnjQe61Ce8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes[y_train[0][0]]\n",
        "# Index from y_train mapped to correct name"
      ],
      "metadata": {
        "id": "YZ6QWLWg1Cgv"
      },
      "id": "YZ6QWLWg1Cgv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normaling x_train data\n",
        "- divide by 255"
      ],
      "metadata": {
        "id": "-QJA94QA1TxO"
      },
      "id": "-QJA94QA1TxO"
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_scaled = X_train / 255\n",
        "x_test_scaled = X_test /255"
      ],
      "metadata": {
        "id": "Ll9prVK_1YN5"
      },
      "id": "Ll9prVK_1YN5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_scaled.shape"
      ],
      "metadata": {
        "id": "D6mN_DZx1fF9"
      },
      "id": "D6mN_DZx1fF9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One hot encoding\n",
        "- y_train ko 10 classes mein each row ko convert\n",
        "- array([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
        "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
        "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
        "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
        "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ],
      "metadata": {
        "id": "tyoIJkAR1xST"
      },
      "id": "tyoIJkAR1xST"
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[:3]"
      ],
      "metadata": {
        "id": "GRw-9tgQ19wP"
      },
      "id": "GRw-9tgQ19wP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_categorical = keras.utils.to_categorical(\n",
        "    y_train, num_classes=10\n",
        ")\n",
        "y_test_categorical = keras.utils.to_categorical(\n",
        "    y_test, num_classes=10\n",
        ")"
      ],
      "metadata": {
        "id": "HpMYslJl126v"
      },
      "id": "HpMYslJl126v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting each y value into array of categories\n",
        "y_train_categorical[0:5]"
      ],
      "metadata": {
        "id": "W6IzbJX42CbO"
      },
      "id": "W6IzbJX42CbO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model- Training and Prediciton\n",
        "- need to flatten data like we did in digit recognistion\n",
        "- 28*28 one iamge convert into 784 xi values\n",
        "- similar here 32 32 3 one image represent\n",
        "- Hidden layer PREFERD Relu used\n",
        "- Classification output sigmoid or sigmoid use"
      ],
      "metadata": {
        "id": "d3qpwZno2li4"
      },
      "id": "d3qpwZno2li4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Ouput layer has 10 classes\n",
        "# Activation signmoidn as it is good with classificaltion\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(32,32,3)),\n",
        "    keras.layers.Dense(3000,activation='relu'),\n",
        "    keras.layers.Dense(3000,activation='relu'),\n",
        "    keras.layers.Dense(10,activation='sigmoid')\n",
        "\n",
        "])\n",
        "# Hidden layer PREFERD Relu used\n",
        "\n",
        "model.compile(optimizer='SGD',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy']\n",
        "              )\n",
        "\n",
        "# Since my y_train_categorcial data\n",
        "# y_train_cateogrical [[0,0,0,1],[1,0,0,]] categories distributed\n",
        "# Use categorical_crossentropy\n",
        "# In Handwrittten digit y_train was [1,2,3,4]\n",
        "# SO used sparse_categoriacal\n",
        "\n",
        "model.fit(x_train_scaled, y_train_categorical, epochs=1)\n",
        "\n",
        "\n",
        "\n",
        "# If y_train is not categorized same way as in Handdigit\n",
        "# model.compile(optimizer='adam',\n",
        "#                   loss='sparse_categorical_crossentropy',\n",
        "#                   metrics=['accuracy'])\n",
        "# model.fit(x_train_scaled, y_train, epochs=1)"
      ],
      "metadata": {
        "id": "2DZpJKM_3CRu"
      },
      "id": "2DZpJKM_3CRu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction"
      ],
      "metadata": {
        "id": "kKbYBLFj68wB"
      },
      "id": "kKbYBLFj68wB"
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmax(model.predict(x_test_scaled)[0])"
      ],
      "metadata": {
        "id": "XEFq5YEf4Cq2"
      },
      "id": "XEFq5YEf4Cq2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test[0]"
      ],
      "metadata": {
        "id": "KRNNvzBZ7PQZ"
      },
      "id": "KRNNvzBZ7PQZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Measure training time on a CPU"
      ],
      "metadata": {
        "id": "XnEp-JVG7Tnk"
      },
      "id": "XnEp-JVG7Tnk"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model():\n",
        "    model = keras.Sequential([\n",
        "            keras.layers.Flatten(input_shape=(32,32,3)),\n",
        "            keras.layers.Dense(3000, activation='relu'),\n",
        "            keras.layers.Dense(1000, activation='relu'),\n",
        "            keras.layers.Dense(10, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "tJ2FzaG17UaD"
      },
      "id": "tJ2FzaG17UaD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.experimental.list_physical_devices()\n",
        "# [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ],
      "metadata": {
        "id": "IW3_ZquF7Y7p"
      },
      "id": "IW3_ZquF7Y7p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n1 -r1\n",
        "with tf.device('/CPU:0'):\n",
        "    cpu_model = get_model()\n",
        "    cpu_model.fit(X_train_scaled, y_train_categorical, epochs=1)\n",
        "\n",
        "# return. 3.6 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)"
      ],
      "metadata": {
        "id": "3Gy4nSFi7WPh"
      },
      "id": "3Gy4nSFi7WPh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Customer Churn Project-3 Using ANN\n",
        "- Measure how many customers are leaving the business\n",
        "- why customers are leaving the business\n",
        "- Telco Customer Churn Data from Kaggle"
      ],
      "metadata": {
        "id": "iILqRlGwgTfj"
      },
      "id": "iILqRlGwgTfj"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "DCEMf3iFhEXu"
      },
      "id": "DCEMf3iFhEXu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/CustomerChurn.csv\")\n",
        "df.sample(5)"
      ],
      "metadata": {
        "id": "8dhNkcnghFSk"
      },
      "id": "8dhNkcnghFSk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "wqWfFbPShJcu"
      },
      "id": "wqWfFbPShJcu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Exploration\n",
        "- customerId is useless\n",
        "- All unique values - 1 and 0 convert\n",
        "- if column has 3,4 unique values create dummies column\n",
        "- Float values need to be scaled MinMax StandardScaler"
      ],
      "metadata": {
        "id": "EG04m-NEhPte"
      },
      "id": "EG04m-NEhPte"
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop('customerID',axis='columns',inplace=True)"
      ],
      "metadata": {
        "id": "rI-y-rUUhVJJ"
      },
      "id": "rI-y-rUUhVJJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "OZLL-ItIhXPi"
      },
      "id": "OZLL-ItIhXPi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Total Charges is object\n",
        "Monthly Charges is Number\n"
      ],
      "metadata": {
        "id": "gEXjhDl9hf82"
      },
      "id": "gEXjhDl9hf82"
    },
    {
      "cell_type": "code",
      "source": [
        "df.TotalCharges.values\n",
        "# String convert into number"
      ],
      "metadata": {
        "id": "VGUnSeIchk9a"
      },
      "id": "VGUnSeIchk9a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.MonthlyCharges.values"
      ],
      "metadata": {
        "id": "J8ImTwNqhpPV"
      },
      "id": "J8ImTwNqhpPV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "HK1F-ZyOhrn9"
      },
      "id": "HK1F-ZyOhrn9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.to_numeric(df['TotalCharges'])\n",
        "# Creating problem as some places ' ' is present remove them"
      ],
      "metadata": {
        "id": "SUecMjp8hyES"
      },
      "id": "SUecMjp8hyES",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding Total Charges Blank"
      ],
      "metadata": {
        "id": "aTDQwd7didfn"
      },
      "id": "aTDQwd7didfn"
    },
    {
      "cell_type": "code",
      "source": [
        "invalid_total_charges = df[pd.to_numeric(df['TotalCharges'], errors='coerce').isnull()]"
      ],
      "metadata": {
        "id": "VsZtDVjCiItL"
      },
      "id": "VsZtDVjCiItL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "invalid_total_charges.shape"
      ],
      "metadata": {
        "id": "k7cn3BuXiRtd"
      },
      "id": "k7cn3BuXiRtd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing and updating the df"
      ],
      "metadata": {
        "id": "CzopbBJejT87"
      },
      "id": "CzopbBJejT87"
    },
    {
      "cell_type": "code",
      "source": [
        "# df1= df[df.TotalCharges != ' ']\n",
        "# df1.shape\n",
        "# Or\n",
        "df = df[pd.to_numeric(df['TotalCharges'], errors='coerce').notnull()]\n",
        "df.shape"
      ],
      "metadata": {
        "id": "OBCm-P47jBFv"
      },
      "id": "OBCm-P47jBFv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting Object to String now"
      ],
      "metadata": {
        "id": "JIPOP3wijWiC"
      },
      "id": "JIPOP3wijWiC"
    },
    {
      "cell_type": "code",
      "source": [
        "df['TotalCharges']=pd.to_numeric(df['TotalCharges'])"
      ],
      "metadata": {
        "id": "7JaHN_3WjV9B"
      },
      "id": "7JaHN_3WjV9B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['TotalCharges'].dtypes"
      ],
      "metadata": {
        "id": "kRGxQXFMjhdv"
      },
      "id": "kRGxQXFMjhdv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Histogram Visualization\n",
        "- Tenure x axis\n",
        "- y axis count using Churn i.e. leaving"
      ],
      "metadata": {
        "id": "2VryzQjXjxjf"
      },
      "id": "2VryzQjXjxjf"
    },
    {
      "cell_type": "code",
      "source": [
        "((df['Churn'] == 'No') & (df['tenure'] >= 70)).sum()"
      ],
      "metadata": {
        "id": "S1SMgFUlj7Dg"
      },
      "id": "S1SMgFUlj7Dg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tenure_churn_no = df[df['Churn'] == 'No'].tenure\n",
        "tenure_churn_yes = df[df['Churn'] == 'Yes'].tenure\n",
        "plt.hist([tenure_churn_no,tenure_churn_yes],color=['red','green'],label=['Churn=Yes','Churn=No'])\n",
        "plt.legend()\n",
        "plt.xlabel(\"tenure\")\n",
        "plt.ylabel(\"Number of customer\")"
      ],
      "metadata": {
        "id": "x4Uf3w47khGe"
      },
      "id": "x4Uf3w47khGe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Monthly charges"
      ],
      "metadata": {
        "id": "7uEXizVdl6oo"
      },
      "id": "7uEXizVdl6oo"
    },
    {
      "cell_type": "code",
      "source": [
        "mc_churn_no = df[df.Churn=='No'].MonthlyCharges\n",
        "mc_churn_yes = df[df.Churn=='Yes'].MonthlyCharges\n",
        "\n",
        "plt.xlabel(\"Monthly Charges\")\n",
        "plt.ylabel(\"Number Of Customers\")\n",
        "plt.title(\"Customer Churn Prediction Visualiztion\")\n",
        "\n",
        "plt.hist([mc_churn_yes, mc_churn_no], rwidth=0.95, color=['green','red'],label=['Churn=Yes','Churn=No'])\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "I42YTaSDl74V"
      },
      "id": "I42YTaSDl74V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label Encoding\n",
        "Columns with Yes and No"
      ],
      "metadata": {
        "id": "pLkg8dy-mRc0"
      },
      "id": "pLkg8dy-mRc0"
    },
    {
      "cell_type": "code",
      "source": [
        "df['gender'].unique()"
      ],
      "metadata": {
        "id": "wh5YlJQ8mXRJ"
      },
      "id": "wh5YlJQ8mXRJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for column in df actually iterates over column names (i.e., strings), not rows\n",
        "# df is a pandas DataFrame, not a basic array.\n",
        "\n",
        "for column in df:\n",
        "  if df[column].dtypes=='object':\n",
        "    print({column}, df[column].unique())"
      ],
      "metadata": {
        "id": "Xz6WCqU-mmDH"
      },
      "id": "Xz6WCqU-mmDH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replacing Values with Yes and No Then converting into numbers"
      ],
      "metadata": {
        "id": "g4vT9QU2nVnZ"
      },
      "id": "g4vT9QU2nVnZ"
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_clean = [\n",
        "    'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n",
        "    'TechSupport', 'StreamingTV', 'StreamingMovies'\n",
        "]\n",
        "\n",
        "for col in columns_to_clean:\n",
        "    df[col] = df[col].replace('No internet service', 'No')\n",
        "\n",
        "df['MultipleLines']= df['MultipleLines'].replace('No phone service', 'No')\n",
        "df['StreamingTV'].unique()"
      ],
      "metadata": {
        "id": "JrPqrBxSnZdy"
      },
      "id": "JrPqrBxSnZdy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for column in df:\n",
        "  if df[column].dtypes=='object':\n",
        "    print({column}, df[column].unique())"
      ],
      "metadata": {
        "id": "MP3QB62hnmRD"
      },
      "id": "MP3QB62hnmRD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replace Yes and No with 1 and 0"
      ],
      "metadata": {
        "id": "VW79XCKEoHel"
      },
      "id": "VW79XCKEoHel"
    },
    {
      "cell_type": "code",
      "source": [
        "yes_no_columns = [\n",
        "    'Partner', 'Dependents', 'PhoneService', 'MultipleLines',\n",
        "    'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',\n",
        "    'StreamingTV', 'StreamingMovies', 'PaperlessBilling', 'Churn'\n",
        "]\n",
        "df[yes_no_columns] = df[yes_no_columns].replace({'Yes': 1, 'No': 0})"
      ],
      "metadata": {
        "id": "XY3itTHZoJqY"
      },
      "id": "XY3itTHZoJqY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df:\n",
        "  print(col,df[col].unique())"
      ],
      "metadata": {
        "id": "4wRCOEwzoSmi"
      },
      "id": "4wRCOEwzoSmi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gender Male=0 Female=1"
      ],
      "metadata": {
        "id": "fFBxgWk1oltN"
      },
      "id": "fFBxgWk1oltN"
    },
    {
      "cell_type": "code",
      "source": [
        "df['gender'].replace({'Female':1,'Male':0},inplace=True)"
      ],
      "metadata": {
        "id": "aAEv8PVtolWT"
      },
      "id": "aAEv8PVtolWT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One Hot Encoding for categorical columns\n",
        "- Columns InternetServi ce has three object\n",
        "- Create 3 columns InternetService1,2,3 with value 1,0\n",
        "- pd.get_dummies()"
      ],
      "metadata": {
        "id": "9PiUipQwo_8b"
      },
      "id": "9PiUipQwo_8b"
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.get_dummies(data=df, columns=['InternetService', 'Contract', 'PaymentMethod'], dtype=int)\n",
        "df1.columns"
      ],
      "metadata": {
        "id": "ih1ftcurpMZz"
      },
      "id": "ih1ftcurpMZz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.head()"
      ],
      "metadata": {
        "id": "7cVPXhjnpl63"
      },
      "id": "7cVPXhjnpl63",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.dtypes"
      ],
      "metadata": {
        "id": "rZVTU4R9pYv6"
      },
      "id": "rZVTU4R9pYv6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tenure, Monthly Charges and TotalCharges need scaling"
      ],
      "metadata": {
        "id": "mNnkWMUgtemV"
      },
      "id": "mNnkWMUgtemV"
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_scale = ['tenure','MonthlyCharges','TotalCharges']\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "df1[cols_to_scale] = scaler.fit_transform(df1[cols_to_scale])"
      ],
      "metadata": {
        "id": "X4eh0Mgnt1pR"
      },
      "id": "X4eh0Mgnt1pR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df1:\n",
        "    print(f'{col}: {df1[col].unique()}')"
      ],
      "metadata": {
        "id": "4e2rGOGdt9PN"
      },
      "id": "4e2rGOGdt9PN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training and Testing"
      ],
      "metadata": {
        "id": "vLo12fYFuGtN"
      },
      "id": "vLo12fYFuGtN"
    },
    {
      "cell_type": "code",
      "source": [
        "X = df1.drop('Churn',axis='columns')\n",
        "y = df1['Churn']\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=5)"
      ],
      "metadata": {
        "id": "l6aQ-OITuJpE"
      },
      "id": "l6aQ-OITuJpE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "Ud6ZW2XNuR9y"
      },
      "id": "Ud6ZW2XNuR9y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "id": "AivyNGtauT4v"
      },
      "id": "AivyNGtauT4v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build ANN Model"
      ],
      "metadata": {
        "id": "ZsBwoo2ouXhT"
      },
      "id": "ZsBwoo2ouXhT"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(26, input_shape=(26,), activation='relu'),\n",
        "    # keras.layers.Dense(15, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "    # output is 1 and 0 toh probability dega using sigmoid\n",
        "])\n",
        "\n",
        "# opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "# output is 0 and 1 binary_crossentropy categorical\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=5)"
      ],
      "metadata": {
        "id": "oCKYy063uaLT"
      },
      "id": "oCKYy063uaLT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "iGNb4oPiu13-"
      },
      "id": "iGNb4oPiu13-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yp = model.predict(X_test)\n",
        "yp[:5]"
      ],
      "metadata": {
        "id": "Bjn9hjoou5i4"
      },
      "id": "Bjn9hjoou5i4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test[:5]"
      ],
      "metadata": {
        "id": "tCZOKc8PvH8f"
      },
      "id": "tCZOKc8PvH8f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output is 1 and 0 toh probability dega using sigmoid\n",
        "y_pred = []\n",
        "for element in yp:\n",
        "    if element > 0.5:\n",
        "        y_pred.append(1)\n",
        "    else:\n",
        "        y_pred.append(0)"
      ],
      "metadata": {
        "id": "tASZ-ERGu-C8"
      },
      "id": "tASZ-ERGu-C8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred[:5]"
      ],
      "metadata": {
        "id": "iK6U-w7vvOhF"
      },
      "id": "iK6U-w7vvOhF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion Metrics and Precision F-1 Call"
      ],
      "metadata": {
        "id": "S-jj9LQJvVHu"
      },
      "id": "S-jj9LQJvVHu"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix , classification_report\n",
        "\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "s4A8CaN8vfXM"
      },
      "id": "s4A8CaN8vfXM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sn\n",
        "cm = tf.math.confusion_matrix(labels=y_test,predictions=y_pred)\n",
        "\n",
        "plt.figure(figsize = (10,7))\n",
        "sn.heatmap(cm, annot=True, fmt='d')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Truth')"
      ],
      "metadata": {
        "id": "1DRrH51Rvhgt"
      },
      "id": "1DRrH51Rvhgt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling Imbalanced Dataset - Customer Churn Project\n",
        "- Due to imbalance in customer churn f-1 score is low for 0 and 1 case\n",
        "- Examples\n",
        "  - customer churn ,cancer prediction, device failure"
      ],
      "metadata": {
        "id": "KJG3Ey0JNM1R"
      },
      "id": "KJG3Ey0JNM1R"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Techniques\n",
        "1. Undersampling majority class 900 F 100 R\n",
        "  - Pick random 100 F and 100 R\n",
        "2. Oversampling\n",
        "  - Duplicate 100*9 R 900R and 900F\n",
        "3. SMOTE - Synthetic minority element ka over-sampling tecnique\n",
        "  - generate examples using k nearest neighbors algo\n",
        "  - 100*9 R 900R and 900F\n",
        "  - use library imblearn.over_sampling import\n",
        "  SMOTE\n",
        "4. Ensemble Method Avg Lena Divide Karke\n",
        "  - 900 F -> Divide 300 300 300 Take 100 F and 100 R combo 1, 2 ,3\n",
        "  - Final result avg/ majority vote of all three\n",
        "5. Focal Loss\n",
        "  - penalize to majority class and more weight to minorty class"
      ],
      "metadata": {
        "id": "KcgLzwpLOXn2"
      },
      "id": "KcgLzwpLOXn2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that by using dropout layer test accuracy increased from 0.77 to 0.81"
      ],
      "metadata": {
        "id": "PSrDfROgM-aF"
      },
      "id": "PSrDfROgM-aF"
    },
    {
      "cell_type": "code",
      "source": [
        "# df1 is the final dataframe after data processing\n",
        "df1.Churn.value_counts()\n",
        "# Churn\n",
        "# 0\t5163\n",
        "# 1\t1869"
      ],
      "metadata": {
        "id": "_VlkSmEAUORR"
      },
      "id": "_VlkSmEAUORR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.columns"
      ],
      "metadata": {
        "id": "tYCpJ515VAkf"
      },
      "id": "tYCpJ515VAkf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BUILD ANN MODEL Function to be used by all methods"
      ],
      "metadata": {
        "id": "-rSfnZxxXMUj"
      },
      "id": "-rSfnZxxXMUj"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import confusion_matrix , classification_report"
      ],
      "metadata": {
        "id": "bst_aBdjXL4c"
      },
      "id": "bst_aBdjXL4c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ANN(X_train, y_train, X_test, y_test, loss, weights):\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Dense(26, input_dim=26, activation='relu'),\n",
        "        keras.layers.Dense(15, activation='relu'),\n",
        "        keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
        "\n",
        "    if weights == -1:\n",
        "        model.fit(X_train, y_train, epochs=100)\n",
        "    else:\n",
        "        model.fit(X_train, y_train, epochs=100, class_weight = weights)\n",
        "\n",
        "    print(model.evaluate(X_test, y_test))\n",
        "\n",
        "    y_preds = model.predict(X_test)\n",
        "    y_preds = np.round(y_preds)\n",
        "\n",
        "    print(\"Classification Report: \\n\", classification_report(y_test, y_preds))\n",
        "\n",
        "    return y_preds"
      ],
      "metadata": {
        "id": "TxqvN93_XTzc"
      },
      "id": "TxqvN93_XTzc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1- Undersampling"
      ],
      "metadata": {
        "id": "5HFRmsHUVYoo"
      },
      "id": "5HFRmsHUVYoo"
    },
    {
      "cell_type": "code",
      "source": [
        "# Same amount of zero and ones\n",
        "class_0_count, class_1_count= df1.Churn.value_counts()\n",
        "class_0_count, class_1_count"
      ],
      "metadata": {
        "id": "nLYAj744VaLC"
      },
      "id": "nLYAj744VaLC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_class_0 = df1[df1['Churn']==0]\n",
        "df_class_1 = df1[df1['Churn']==1]"
      ],
      "metadata": {
        "id": "TvfTZ49CV3o_"
      },
      "id": "TvfTZ49CV3o_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_class_0.shape"
      ],
      "metadata": {
        "id": "6mPDoSeAV-gl"
      },
      "id": "6mPDoSeAV-gl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_class_1.shape"
      ],
      "metadata": {
        "id": "Pq8rSFzDV_ot"
      },
      "id": "Pq8rSFzDV_ot",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class 0 se sample collect of size class 1"
      ],
      "metadata": {
        "id": "yimGVkrsWBhz"
      },
      "id": "yimGVkrsWBhz"
    },
    {
      "cell_type": "code",
      "source": [
        "df_class_0_sameas1= df_class_0.sample(class_1_count)\n",
        "df_class_0_sameas1.shape"
      ],
      "metadata": {
        "id": "8k5upSI-WFQW"
      },
      "id": "8k5upSI-WFQW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine to create a new df\n",
        "df_underfitting = pd.concat([df_class_0_sameas1,df_class_1])\n",
        "df_underfitting.shape"
      ],
      "metadata": {
        "id": "hEwUgZxqVz_q"
      },
      "id": "hEwUgZxqVz_q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_underfitting['Churn'].value_counts()\n",
        "# Churn\n",
        "# 0\t1869\n",
        "# 1\t1869\n"
      ],
      "metadata": {
        "id": "VViEXBaSWWzh"
      },
      "id": "VViEXBaSWWzh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training using this df_underfitting"
      ],
      "metadata": {
        "id": "jcBJlzMxWjbp"
      },
      "id": "jcBJlzMxWjbp"
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_underfitting.drop('Churn',axis='columns')\n",
        "y = df_underfitting['Churn']\n",
        "\n",
        "#Stratity y meand y_train  and y_test equal number of 0 and 1 divide\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15, stratify=y)"
      ],
      "metadata": {
        "id": "yVSVIFmvWjLO"
      },
      "id": "yVSVIFmvWjLO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of classes in training Data\n",
        "y_train.value_counts()\n",
        "# 0\t1495\n",
        "# 1\t1495"
      ],
      "metadata": {
        "id": "hQ9gSpOIWsx1"
      },
      "id": "hQ9gSpOIWsx1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.value_counts()\n",
        "\n",
        "# 1\t374\n",
        "# 0\t374\n"
      ],
      "metadata": {
        "id": "eh0YfDmBW4tE"
      },
      "id": "eh0YfDmBW4tE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds_undersampling = ANN(X_train, y_train, X_test, y_test, 'binary_crossentropy', -1)\n",
        "# f1 score for 0 and 1 imporved now"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_33OCfaIXq4z"
      },
      "id": "_33OCfaIXq4z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2- Oversampling"
      ],
      "metadata": {
        "id": "D9kKkQTZYJAa"
      },
      "id": "D9kKkQTZYJAa"
    },
    {
      "cell_type": "code",
      "source": [
        "# Same amount of zero and ones\n",
        "class_0_count, class_1_count= df1.Churn.value_counts()\n",
        "class_0_count, class_1_count\n",
        "df_class_0 = df1[df1['Churn']==0]\n",
        "df_class_1 = df1[df1['Churn']==1]"
      ],
      "metadata": {
        "id": "EztKnKXuYzjK"
      },
      "id": "EztKnKXuYzjK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Duplicate the class1 to equal to class0 number\n",
        "# Blindly duplicating the values from df_class1\n",
        "df_class1_oversample=df_class_1.sample(class_0_count,replace=True)\n",
        "df_class1_oversample.shape"
      ],
      "metadata": {
        "id": "6oNmpAvmY7It"
      },
      "id": "6oNmpAvmY7It",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# join and one data frame\n",
        "df_oversample = pd.concat([df_class1_oversample,df_class_0])\n",
        "df_oversample.shape"
      ],
      "metadata": {
        "id": "8QTBeAoaZYE0"
      },
      "id": "8QTBeAoaZYE0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_oversample['Churn'].value_counts()"
      ],
      "metadata": {
        "id": "aYOsZ1_nZfOo"
      },
      "id": "aYOsZ1_nZfOo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_oversample.drop('Churn',axis='columns')\n",
        "y = df_oversample['Churn']\n",
        "\n",
        "#Stratity y meand y_train  and y_test equal number of 0 and 1 divide\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15, stratify=y)"
      ],
      "metadata": {
        "id": "OWk0lhLgZh22"
      },
      "id": "OWk0lhLgZh22",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of classes in training Data\n",
        "y_train.value_counts()\n",
        "# 0\t1495\n",
        "# 1\t1495"
      ],
      "metadata": {
        "id": "yT_06KQiZlT_"
      },
      "id": "yT_06KQiZlT_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.value_counts()\n",
        "\n",
        "# 1\t374\n",
        "# 0\t374\n"
      ],
      "metadata": {
        "id": "EV3DFekmZo4Y"
      },
      "id": "EV3DFekmZo4Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds_oversampling = ANN(X_train, y_train, X_test, y_test, 'binary_crossentropy', -1)\n",
        "# f1 score for 0 and 1 imporved now"
      ],
      "metadata": {
        "id": "dtdQfoVvZt6R"
      },
      "id": "dtdQfoVvZt6R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3- SOMET\n",
        "- Instead blindly duplicating values in class-1\n",
        "- use k mean neighbour algo"
      ],
      "metadata": {
        "id": "SfG9kCTtaMnm"
      },
      "id": "SfG9kCTtaMnm"
    },
    {
      "cell_type": "code",
      "source": [
        "X = df1.drop('Churn',axis='columns')\n",
        "y = df1['Churn']"
      ],
      "metadata": {
        "id": "8EKg5piwap_D"
      },
      "id": "8EKg5piwap_D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.value_counts()\n",
        "# 0\t5163\n",
        "# 1\t1869"
      ],
      "metadata": {
        "id": "3yZdGF7WavZY"
      },
      "id": "3yZdGF7WavZY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote = SMOTE(sampling_strategy='minority')\n",
        "X_sm, y_sm = smote.fit_resample(X, y)\n",
        "\n",
        "# now y ka minority 1 is equal to 0\n",
        "# not blindly duplicated used k mean insternally for those rows\n",
        "y_sm.value_counts()"
      ],
      "metadata": {
        "id": "MgTUcXrea1_h"
      },
      "id": "MgTUcXrea1_h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "New x and y are ready now do train split and build model"
      ],
      "metadata": {
        "id": "8u_1vKdrbNqr"
      },
      "id": "8u_1vKdrbNqr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4- Ensemble with undersampling\n",
        "- decrease majority 0 to number of minority 1\n",
        "- use majority vote from multiple division"
      ],
      "metadata": {
        "id": "3k7OPBoPbYji"
      },
      "id": "3k7OPBoPbYji"
    },
    {
      "cell_type": "code",
      "source": [
        "df1.Churn.value_counts()"
      ],
      "metadata": {
        "id": "XHKlj5tybhh_"
      },
      "id": "XHKlj5tybhh_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Regain Original features and labels\n",
        "X = df1.drop('Churn',axis='columns')\n",
        "y = df1['Churn']"
      ],
      "metadata": {
        "id": "t9o4L_CNblm7"
      },
      "id": "t9o4L_CNblm7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training data need to be divided and Test can remain same"
      ],
      "metadata": {
        "id": "KeSqQqgUd7ld"
      },
      "id": "KeSqQqgUd7ld"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15, stratify=y)"
      ],
      "metadata": {
        "id": "iQrprF85eAbk"
      },
      "id": "iQrprF85eAbk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.value_counts()\n",
        "# 0\t4130\n",
        "# 1\t1495\n",
        "\n",
        "# need 1st split 0:1495 1495 2nd 1496:1496+1495 1495\n",
        "# Train in 3 data split of majority"
      ],
      "metadata": {
        "id": "NRDLK_bKeC7_"
      },
      "id": "NRDLK_bKeC7_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2= X_train\n",
        "df2['Churn']=y_train\n",
        "df2.shape\n",
        "df2.head()"
      ],
      "metadata": {
        "id": "2YRkz8pdcuna"
      },
      "id": "2YRkz8pdcuna",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['Churn'].value_counts()"
      ],
      "metadata": {
        "id": "_73YzfD3ebE0"
      },
      "id": "_73YzfD3ebE0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2_class0 = df2[df2['Churn']==0]\n",
        "df2_class1 = df2[df2['Churn']==1]"
      ],
      "metadata": {
        "id": "lqvyMvi8dqzu"
      },
      "id": "lqvyMvi8dqzu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2_class0.shape,df2_class1.shape\n",
        "# ((4130, 27), (1495, 27))"
      ],
      "metadata": {
        "id": "oBl8UuYNdyeV"
      },
      "id": "oBl8UuYNdyeV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_divide1 = pd.concat([df2_class0[0:1495],df2_class1])\n",
        "df_train_divide1.shape"
      ],
      "metadata": {
        "id": "oKhfp0JLd0nk"
      },
      "id": "oKhfp0JLd0nk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_divide1['Churn'].value_counts()"
      ],
      "metadata": {
        "id": "0c95FfsGe3qd"
      },
      "id": "0c95FfsGe3qd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create A fucntion to create a batch"
      ],
      "metadata": {
        "id": "HWQ20WRGe8Y1"
      },
      "id": "HWQ20WRGe8Y1"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_batch(df_majority, df_minority, start, end):\n",
        "    df_train = pd.concat([df_majority[start:end], df_minority], axis=0)\n",
        "\n",
        "    X_train = df_train.drop('Churn', axis='columns')\n",
        "    y_train = df_train.Churn\n",
        "    return X_train, y_train"
      ],
      "metadata": {
        "id": "-6PuSM2-fCqH"
      },
      "id": "-6PuSM2-fCqH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction 1: First Division used as training"
      ],
      "metadata": {
        "id": "EyejQjeCfRB7"
      },
      "id": "EyejQjeCfRB7"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8aq-5cMnmPuy"
      },
      "id": "8aq-5cMnmPuy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = get_train_batch(df2_class0, df2_class1, 0, 1495)\n",
        "y_pred1 = ANN(X_train, y_train, X_test, y_test, 'binary_crossentropy', -1)"
      ],
      "metadata": {
        "id": "hGgsLqBwfPDn"
      },
      "id": "hGgsLqBwfPDn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction 2: Second Division used as training"
      ],
      "metadata": {
        "id": "sujHlpeNmQmz"
      },
      "id": "sujHlpeNmQmz"
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = get_train_batch(df3_class0, df3_class1, 1495, 2990)\n",
        "y_pred2 = ANN(X_train, y_train, X_test, y_test, 'binary_crossentropy', -1)"
      ],
      "metadata": {
        "id": "qFEfYz5UloZF"
      },
      "id": "qFEfYz5UloZF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction 3: Third Division used as training"
      ],
      "metadata": {
        "id": "z9JLFQy1mSrE"
      },
      "id": "z9JLFQy1mSrE"
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = get_train_batch(df3_class0, df3_class1, 2990, 4130)\n",
        "\n",
        "y_pred3 = ANN(X_train, y_train, X_test, y_test, 'binary_crossentropy', -1)"
      ],
      "metadata": {
        "id": "dcLLzerdlptN"
      },
      "id": "dcLLzerdlptN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_test)\n",
        "y_pred1[0]"
      ],
      "metadata": {
        "id": "LaGw2xsclrtm"
      },
      "id": "LaGw2xsclrtm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trained the model- YPred\n",
        "- xtrain and ytrain divided equal distribution wala\n",
        "- Net ypred -> majority of 1 2 3"
      ],
      "metadata": {
        "id": "6Zecs_rSlwOI"
      },
      "id": "6Zecs_rSlwOI"
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_final = y_pred1.copy()\n",
        "for i in range(len(y_pred1)):\n",
        "    n_ones = y_pred1[i] + y_pred2[i] + y_pred3[i]\n",
        "    if n_ones>1:\n",
        "        y_pred_final[i] = 1\n",
        "    else:\n",
        "        y_pred_final[i] = 0"
      ],
      "metadata": {
        "id": "cFOKX7x2lvrv"
      },
      "id": "cFOKX7x2lvrv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cl_rep = classification_report(y_test, y_pred_final)\n",
        "print(cl_rep)\n",
        "\n",
        "#  Final report based on y_test and y_prediction_ensemble"
      ],
      "metadata": {
        "id": "nCMnjFHpmICF"
      },
      "id": "nCMnjFHpmICF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dropout Regularisation Project-4 Sonar Mine Detection\n",
        "- dealing with overfit and underfit problem\n",
        "- neural network tends to overfit when eopchs are high\n",
        "- training sample 1 -> drop 50% neurons from hidden layers\n",
        "\n",
        "### Dropout helps in overfiting\n",
        "- dropping helps neurons dont be biased with some feature kyuki woh kabhi bhi randomly drop ho sakte\n",
        "- can;t rely on one input as it might be droipout randomly\n",
        "- neurons will not learn redundant detials of input"
      ],
      "metadata": {
        "id": "8WHmfSNmGloZ"
      },
      "id": "8WHmfSNmGloZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset - Connectionist Bench Solar Mines Vs Rocks\n",
        "- binary classification problem"
      ],
      "metadata": {
        "id": "Ycd3a6iZHXfZ"
      },
      "id": "Ycd3a6iZHXfZ"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "MmQ0gWb7H2Tj"
      },
      "id": "MmQ0gWb7H2Tj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "9ZEwmapnH367"
      },
      "id": "9ZEwmapnH367",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/sonar_dataset.csv\", header=None)\n",
        "# header none since this data dont have column name so indexing\n",
        "df.sample(5)"
      ],
      "metadata": {
        "id": "grdATglWH5M9"
      },
      "id": "grdATglWH5M9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "clCEmS6uIFls"
      },
      "id": "clCEmS6uIFls",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "mF2Lvj7KIHdP"
      },
      "id": "mF2Lvj7KIHdP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "rFa2M68-IccK"
      },
      "id": "rFa2M68-IccK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[60].unique()"
      ],
      "metadata": {
        "id": "i3iEegfuIgsz"
      },
      "id": "i3iEegfuIgsz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[60].value_counts()"
      ],
      "metadata": {
        "id": "saQvRSbNIJEW"
      },
      "id": "saQvRSbNIJEW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### x and y"
      ],
      "metadata": {
        "id": "t7CXCR_AIo2M"
      },
      "id": "t7CXCR_AIo2M"
    },
    {
      "cell_type": "code",
      "source": [
        "x= df.drop(60,axis=1)\n",
        "y= df[60]\n",
        "x"
      ],
      "metadata": {
        "id": "beqlShS4IqVt"
      },
      "id": "beqlShS4IqVt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "Bxhdu58PIwLZ"
      },
      "id": "Bxhdu58PIwLZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Label Encoding y\n",
        "- M = 1\n",
        "- R = 0\n",
        "\n",
        "OR\n",
        "### Dummies create Y-> R and M column mein\n",
        "- y = pd.get_dummies(y, drop_first=True) ek hi columns R naam ka bacha\n",
        "- y.sample(5) # R --> 1 and M --> 0"
      ],
      "metadata": {
        "id": "hiYQvXOQJChY"
      },
      "id": "hiYQvXOQJChY"
    },
    {
      "cell_type": "code",
      "source": [
        "y = y.map({'R': 0, 'M': 1})"
      ],
      "metadata": {
        "id": "IMLr6zawJGbX"
      },
      "id": "IMLr6zawJGbX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.value_counts()"
      ],
      "metadata": {
        "id": "O80Z8POTJGrt"
      },
      "id": "O80Z8POTJGrt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=1)"
      ],
      "metadata": {
        "id": "ECe3-2S3Jm3p"
      },
      "id": "ECe3-2S3Jm3p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head()"
      ],
      "metadata": {
        "id": "bzexK_mLJrwI"
      },
      "id": "bzexK_mLJrwI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "gxO9kosQJ3OC"
      },
      "id": "gxO9kosQJ3OC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Learning Model\n",
        "- Use sigmoid + binary_crossentropy when the output is one unit (1 neuron) with 0/1 label.\n",
        "- One hot ouput\n",
        "  -  labels are one-hot encoded (e.g., [1, 0] or [0, 1]), you'd need:\n",
        "  - 2 output units  softmax activation categorical_crossentropy loss\n",
        "  - keras.layers.Dense(2, activation='softmax')  # 2 output units for one-hot"
      ],
      "metadata": {
        "id": "0573UHx_Ju8E"
      },
      "id": "0573UHx_Ju8E"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Method-1 Model without Dropout Layer"
      ],
      "metadata": {
        "id": "vArYMGLmJv4d"
      },
      "id": "vArYMGLmJv4d"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "6j7AxwdcJzmg"
      },
      "id": "6j7AxwdcJzmg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(60, input_dim=60, activation='relu'),\n",
        "    keras.layers.Dense(30, activation='relu'),\n",
        "    keras.layers.Dense(15, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# binary ouput sigmoid\n",
        "# error loss = binary_crossenetr\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# batch size randomly 8 samples\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=8)"
      ],
      "metadata": {
        "id": "XP8MM46hJ08b"
      },
      "id": "XP8MM46hJ08b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Overfit training time accuracy 1 but here problem\n",
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "6b5YJIm_LYvY"
      },
      "id": "6b5YJIm_LYvY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model.predict(X_test)\n",
        "print(pred,pred.shape)\n",
        "y_pred= pred.reshape(-1)\n",
        "#  (52, 1) to single array convert\n",
        "print(y_pred[:10])\n",
        "\n",
        "# round the values to nearest integer ie 0 or 1\n",
        "y_pred = np.round(y_pred)\n",
        "print(y_pred[:10])\n",
        "print(y_test[:10])"
      ],
      "metadata": {
        "id": "iFtkYW0LLoG7"
      },
      "id": "iFtkYW0LLoG7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Confusion matrix and report y true and predicted"
      ],
      "metadata": {
        "id": "3V2pFbf4MH64"
      },
      "id": "3V2pFbf4MH64"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix , classification_report\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "e9uTs-XCMM3x"
      },
      "id": "e9uTs-XCMM3x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Method-2 Model with Dropout Layer\n"
      ],
      "metadata": {
        "id": "ENIwTWmyMW-t"
      },
      "id": "ENIwTWmyMW-t"
    },
    {
      "cell_type": "code",
      "source": [
        "modeld = keras.Sequential([\n",
        "    keras.layers.Dense(60, input_dim=60, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    # Declaring dropout for each epcoh\n",
        "    keras.layers.Dense(30, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(15, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "modeld.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "modeld.fit(X_train, y_train, epochs=100, batch_size=8)"
      ],
      "metadata": {
        "id": "js17Ry_aMa3H"
      },
      "id": "js17Ry_aMa3H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modeld.evaluate(X_test, y_test)\n",
        "# Test set accuracy improved"
      ],
      "metadata": {
        "id": "yN7I75HGMotj"
      },
      "id": "yN7I75HGMotj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = modeld.predict(X_test).reshape(-1)\n",
        "print(y_pred[:10])\n",
        "\n",
        "# round the values to nearest integer ie 0 or 1\n",
        "y_pred = np.round(y_pred)\n",
        "print(y_pred[:10])"
      ],
      "metadata": {
        "id": "6ro_CPyTM5gE"
      },
      "id": "6ro_CPyTM5gE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix , classification_report\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "d94_EjsxM6UH"
      },
      "id": "d94_EjsxM6UH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computer Vision\n",
        "- Area of computer science dealing with image classification, object detection\n",
        "- Image Classification - > phone peoples category\n",
        "- Object Detection -> Passbook extract information\n",
        "  - Agriculture Crop/Pest Analysize\n",
        "  - Autonomous Cars\n",
        "- Possible due to neural network training acha ho raha now"
      ],
      "metadata": {
        "id": "kNPZNxc1pjqw"
      },
      "id": "kNPZNxc1pjqw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional Neural Network\n",
        "- ANN\n",
        "  - earlier ANN images array break (100,100,3) then we need 30000 neurons in first layer computation is very high\n",
        "  - Too much complexity\n",
        "  - Treats local pixels same as pixels far apart\n",
        "  - Sensitive to location of an image = array changes values\n"
      ],
      "metadata": {
        "id": "2Wg1RL8Nqx6Q"
      },
      "id": "2Wg1RL8Nqx6Q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Classification using CNN- CIFAR10 dataset\n",
        "  - y = one hot encoded [0 0 0 1] instead of 4 use categorical_crossentropy\n",
        "  - y= simple value 9   use sparse_categorical_crossentropy\n",
        "  - y = 0/1 use binary crossentropy\n",
        "\n",
        "- With CNN, at the end 5 epochs, accuracy was at around 70% which is a significant improvement over ANN. CNN's are best for image classification and gives superb accuracy. Also computation is much less compared to simple ANN as maxpooling reduces the image dimensions while still preserving the features"
      ],
      "metadata": {
        "id": "sH8ENr8hneLF"
      },
      "id": "sH8ENr8hneLF"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "# Keras has data, layers for models everything\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "BLv_0yRCoFGe"
      },
      "id": "BLv_0yRCoFGe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test,y_test) = datasets.cifar10.load_data()\n",
        "X_train.shape"
      ],
      "metadata": {
        "id": "uEZQE7nRoIrX"
      },
      "id": "uEZQE7nRoIrX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[0]\n",
        "# ouput is from 0 to 9\n",
        "# unique, counts = np.unique(y_train, return_counts=True)\n",
        "# value_counts = dict(zip(unique, counts))\n",
        "# print(value_counts)"
      ],
      "metadata": {
        "id": "6IUHXBxroMXj"
      },
      "id": "6IUHXBxroMXj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape\n",
        "# 2D array"
      ],
      "metadata": {
        "id": "LlnfbENVo6qR"
      },
      "id": "LlnfbENVo6qR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = y_train.reshape(-1)\n",
        "y_test = y_test.reshape(-1)\n",
        "y_train.shape\n",
        "y_train[:5]"
      ],
      "metadata": {
        "id": "Fo4XbgRzo-ju"
      },
      "id": "Fo4XbgRzo-ju",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0]\n",
        "# (32, 32, 3)"
      ],
      "metadata": {
        "id": "FnucNesIonXY"
      },
      "id": "FnucNesIonXY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"]"
      ],
      "metadata": {
        "id": "yM3rDDF1ozIf"
      },
      "id": "yM3rDDF1ozIf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(X_train[1])"
      ],
      "metadata": {
        "id": "KaZRZXvRoqjD"
      },
      "id": "KaZRZXvRoqjD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_sample(x,y,index):\n",
        "  plt.figure(figsize=(2,2))\n",
        "  plt.imshow(x[index])\n",
        "  plt.xlabel(classes[y[index]])"
      ],
      "metadata": {
        "id": "nKDLwV3fpGrP"
      },
      "id": "nKDLwV3fpGrP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_sample(X_train,y_train,3)"
      ],
      "metadata": {
        "id": "Xv3fQLZspQMn"
      },
      "id": "Xv3fQLZspQMn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalize x_train since pixel 0 to 255 value RGB"
      ],
      "metadata": {
        "id": "oc21B-YEpfXb"
      },
      "id": "oc21B-YEpfXb"
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train/255\n",
        "X_test = X_test/255"
      ],
      "metadata": {
        "id": "t-JArmU6pi_L"
      },
      "id": "t-JArmU6pi_L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0]"
      ],
      "metadata": {
        "id": "xIYsn6j5plEA"
      },
      "id": "xIYsn6j5plEA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ANN Model vs CNN Model Comparision"
      ],
      "metadata": {
        "id": "hVPW9GQfpuVv"
      },
      "id": "hVPW9GQfpuVv"
    },
    {
      "cell_type": "code",
      "source": [
        "ann = models.Sequential([\n",
        "        layers.Flatten(input_shape=(32,32,3)),\n",
        "        layers.Dense(3000, activation='relu'),\n",
        "        layers.Dense(1000, activation='relu'),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "ann.compile(optimizer='SGD',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "ann.fit(X_train, y_train, epochs=1)\n",
        "\n",
        "\n",
        "# y = one hot encoded [0 0 0 1] use categorical_crossentropy\n",
        "# y= simple value 9   use sparse_categorical_crossentropy\n",
        "# y = 0/1 use binary crossentropy"
      ],
      "metadata": {
        "id": "9591Oqw8py0t"
      },
      "id": "9591Oqw8py0t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix , classification_report\n",
        "import numpy as np\n",
        "y_pred = ann.predict(X_test)\n",
        "print(y_pred,y_pred.shape)\n",
        "# y_pred 10 classes total ka probability lega\n",
        "y_pred_classes = [np.argmax(element) for element in y_pred]\n",
        "# indexes of highest probability in each element\n",
        "print(\"Classification Report: \\n\", classification_report(y_test, y_pred_classes))"
      ],
      "metadata": {
        "id": "l-SF34Mvr-IL"
      },
      "id": "l-SF34Mvr-IL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN Method -> Keras provide\n",
        "- Layer1 Convolution Relu Pooling\n",
        "- Layer2 Again same\n",
        "- Layer 3 Goes to ANN neurons Dense Layer"
      ],
      "metadata": {
        "id": "ZkPyAXojq3L7"
      },
      "id": "ZkPyAXojq3L7"
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = models.Sequential([\n",
        "    # cnn automatically creates filters jaise 9-> upper cirle, lower cuver etc khud pata kar lega\n",
        "    # img-> Convolution: number of filter, each filter size, and activatoion function\n",
        "    layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "\n",
        "    # Pooling to reduce dimension 2X2 box mein max value to 1 value\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    # now after cnn pass to ann type with flatten\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "0rAlT70gr7JD"
      },
      "id": "0rAlT70gr7JD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "38tmgskCuAIC"
      },
      "id": "38tmgskCuAIC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.fit(X_train, y_train, epochs=1)"
      ],
      "metadata": {
        "id": "rIjuYXjauBkQ"
      },
      "id": "rIjuYXjauBkQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.evaluate(X_test,y_test)"
      ],
      "metadata": {
        "id": "2YFCl6H-vd6C"
      },
      "id": "2YFCl6H-vd6C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = cnn.predict(X_test)\n",
        "y_pred[:5]\n",
        "y_classes = [np.argmax(element) for element in y_pred]\n",
        "y_classes[:5]\n",
        "print(classes[np.argmax(y_pred[30])])"
      ],
      "metadata": {
        "id": "bMwFxrORvpIg"
      },
      "id": "bMwFxrORvpIg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_sample(X_test, y_test,30)"
      ],
      "metadata": {
        "id": "GhsD2DPPvrbX"
      },
      "id": "GhsD2DPPvrbX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix , classification_report\n",
        "print(\"Classification Report: \\n\", classification_report(y_test, y_classes))"
      ],
      "metadata": {
        "id": "d1-v0nG-wWCw"
      },
      "id": "d1-v0nG-wWCw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Difference between sigmoid and softmax actication fucntion\n",
        "- Sigmoid ouput\n",
        "1: 0.45\n",
        "2: 0.67\n",
        "\n",
        "-Softmax ouput\n",
        "1: 0.45/(0.45/0.67)\n",
        "2: 0.67/(0.45/0.67)\n",
        "\n",
        "So total sum of probability is 1 in softmax"
      ],
      "metadata": {
        "id": "CgQIY5NwrdfI"
      },
      "id": "CgQIY5NwrdfI"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
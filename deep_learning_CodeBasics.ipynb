{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Handwritten Digits - Project-1 - Tensorboard Introduction\n",
        "- Tensorflow and Keras to create neural network\n",
        "  - takes input output and layers details like\n",
        "    - activcation fucntion\n",
        "    - loss fucntion\n",
        "    - accuracy metrics\n",
        "- Confusion matrix between predcited and test\n",
        "- plot using sns"
      ],
      "metadata": {
        "id": "zzJ3VuJ_i3eh"
      },
      "id": "zzJ3VuJ_i3eh"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "#It tells the notebook to display matplotlib plots directly below the code cells that produce them, rather than in a separate window.\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "-ufdhgfNi7na"
      },
      "id": "-ufdhgfNi7na",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist"
      ],
      "metadata": {
        "id": "ekH16FdAkBgR"
      },
      "id": "ekH16FdAkBgR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Databaset using Keras\n",
        "- Train and Split in Keras"
      ],
      "metadata": {
        "id": "F7BYfvKplv5b"
      },
      "id": "F7BYfvKplv5b"
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "THJm4jCkkRA_"
      },
      "id": "THJm4jCkkRA_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Each Image as 2D array of 28x28\n",
        "\n",
        "len(x_train)\n",
        "len(y_train)\n",
        "len(x_test)\n",
        "# X train and y_train are 2d arrays"
      ],
      "metadata": {
        "id": "YqR4ZzMvkl35"
      },
      "id": "YqR4ZzMvkl35",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MNIST\n",
        "- Handwritten digits data in 2D form\n",
        "- 0 means black 255 means white"
      ],
      "metadata": {
        "id": "u430bXOckukr"
      },
      "id": "u430bXOckukr"
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x_train[0]))\n",
        "print(x_train[0])\n",
        "# This is 2D array"
      ],
      "metadata": {
        "id": "KBkqqsfQk18g"
      },
      "id": "KBkqqsfQk18g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### See Image using Matplot\n",
        "- plt.matshow(2D array)"
      ],
      "metadata": {
        "id": "dXyHJd_vlMOT"
      },
      "id": "dXyHJd_vlMOT"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.matshow(x_train[100])\n",
        "print(y_train[100])"
      ],
      "metadata": {
        "id": "_ecDtgXFlQUR"
      },
      "id": "_ecDtgXFlQUR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Flatten 2D array 28X28 into 1D array"
      ],
      "metadata": {
        "id": "4auSI99-l1ZN"
      },
      "id": "4auSI99-l1ZN"
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape\n",
        "# number of rows\n",
        "# each rows is 2d array"
      ],
      "metadata": {
        "id": "LRl59aQGl6rT"
      },
      "id": "LRl59aQGl6rT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Want (60000,784)\n",
        "x_train_flattened = x_train.reshape(len(x_train),28*28)\n",
        "x_test_flattened = x_test.reshape(len(x_test),28*28)\n",
        "x_train_flattened.shape"
      ],
      "metadata": {
        "id": "G8uE_FGTl53q"
      },
      "id": "G8uE_FGTl53q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_flattened[0]\n",
        "print(len(x_train_flattened[0]))"
      ],
      "metadata": {
        "id": "n3vm7uMkmXJW"
      },
      "id": "n3vm7uMkmXJW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scaling the x_train data between 0 to 1\n",
        "- Divide by 255"
      ],
      "metadata": {
        "id": "OBbKyyCEpoNF"
      },
      "id": "OBbKyyCEpoNF"
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_flattened = x_train_flattened/255"
      ],
      "metadata": {
        "id": "5f1ULrfJptF0"
      },
      "id": "5f1ULrfJptF0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[0:10]"
      ],
      "metadata": {
        "id": "wxiiuO_-613J"
      },
      "id": "wxiiuO_-613J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keras to Create Neural Network\n",
        "- want 784 as x1 x2 ... .x784 to connect with 10 ouputs 0 to 1\n",
        "- acitvation function = sigmoid\n",
        "- Then training using data yeh sab\n",
        "\n",
        "- Single layer input and output for now"
      ],
      "metadata": {
        "id": "c7QkkqnOnE7t"
      },
      "id": "c7QkkqnOnE7t"
    },
    {
      "cell_type": "code",
      "source": [
        "# Dense(10): A fully connected layer with 10 neurons\n",
        "# model = keras.Sequential([\n",
        "#     keras.layers.Dense(10,input_shape=(784,),activation='softmax')\n",
        "# ])\n",
        "\n",
        "# If dont want to flatten image tell its 28*28\n",
        "# Multiple layers hit and trial\n",
        "model = keras.Sequential([\n",
        "      # keras.layers.Flatten(input_shape=(28,28))\n",
        "    keras.layers.Dense(100,input_shape=(784,),activation='relu'),\n",
        "    keras.layers.Dense(10,activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "# optimizer backward going to optain weight attain global minima\n",
        "# loss fucntion ouput and predicted difference used to attain weights\n",
        "# Gradient decent and cost function\n",
        "\n",
        "# metric when neural network compile goal is accuracy input output differtence\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        "    )\n",
        "# epochs number of iteration the neural network going to train data\n",
        "model.fit(x_train_flattened,y_train,epochs=5)"
      ],
      "metadata": {
        "id": "gj94-wPHnNlx"
      },
      "id": "gj94-wPHnNlx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating | Predicition Keral Model on Test Dataset"
      ],
      "metadata": {
        "id": "s3rI94vup9Ex"
      },
      "id": "s3rI94vup9Ex"
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x_test_flattened,y_test)"
      ],
      "metadata": {
        "id": "S_nEsCthqARu"
      },
      "id": "S_nEsCthqARu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.matshow(x_test[50])"
      ],
      "metadata": {
        "id": "02__G9G_qrrV"
      },
      "id": "02__G9G_qrrV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict= model.predict(x_test_flattened)\n",
        "print(y_predict[50])\n",
        "# This is 10 neurons output probability"
      ],
      "metadata": {
        "id": "D7x7ovHQrXty"
      },
      "id": "D7x7ovHQrXty",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exact ouput based on 10 neurons\n",
        "# np.argmax returns the index of the max probability, i.e., the predicted class.\n",
        "print(np.argmax(y_predict[50]))"
      ],
      "metadata": {
        "id": "Co7wulzurnLG"
      },
      "id": "Co7wulzurnLG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test[49:52]"
      ],
      "metadata": {
        "id": "kpWS0X_wtEGz"
      },
      "id": "kpWS0X_wtEGz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert y_predicted into concrete class similar to y_test"
      ],
      "metadata": {
        "id": "mwHX7hH_tNsk"
      },
      "id": "mwHX7hH_tNsk"
    },
    {
      "cell_type": "code",
      "source": [
        "y_predicted_labels = np.argmax(y_predict, axis=1)"
      ],
      "metadata": {
        "id": "4AmrvBjatVGS"
      },
      "id": "4AmrvBjatVGS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predicted_labels[49:52]"
      ],
      "metadata": {
        "id": "2v1sOa6ytWk0"
      },
      "id": "2v1sOa6ytWk0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Confusion Matrix"
      ],
      "metadata": {
        "id": "DT5ArGQes7Rv"
      },
      "id": "DT5ArGQes7Rv"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_predicted_labels)\n",
        "print(conf_matrix)\n",
        "\n",
        "# confusion_matrix = tf.math.confusion.matrix(labels=y_test, predictions=y_predicted_labels)\n",
        "# confusion_matrix"
      ],
      "metadata": {
        "id": "xa-FD69Ms-Q0"
      },
      "id": "xa-FD69Ms-Q0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize using seaborn"
      ],
      "metadata": {
        "id": "ij_jRKB2t2Bm"
      },
      "id": "ij_jRKB2t2Bm"
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "VIs7_8wwt4z9"
      },
      "id": "VIs7_8wwt4z9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=np.arange(10), yticklabels=np.arange(10))\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('MNIST Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WVZur1qZuJh_"
      },
      "id": "WVZur1qZuJh_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PKQFAFOYvT46"
      },
      "id": "PKQFAFOYvT46"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorboard\n",
        "- Visualize tensorflow model accuracy loss etc"
      ],
      "metadata": {
        "id": "eM7ewKK-LFdX"
      },
      "id": "eM7ewKK-LFdX"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(100,input_shape=(784,),activation='relu'),\n",
        "    keras.layers.Dense(10,activation='softmax')\n",
        "])\n",
        "\n",
        "# Tensorboard callback\n",
        "tf_callback= tf.keras.callbacks.TensorBoard(log_dir='/content/logs/',histogram_freq=1)\n",
        "\n",
        "model.compile(\n",
        "    optimizer='SGD',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        "    )\n",
        "# Tensorboard use in fit\n",
        "model.fit(x_train_flattened,y_train,epochs=5,callbacks=[tf_callback])"
      ],
      "metadata": {
        "id": "rR7A-Oj5LN3E"
      },
      "id": "rR7A-Oj5LN3E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize Tensorboard File"
      ],
      "metadata": {
        "id": "F2-TITf0MQMH"
      },
      "id": "F2-TITf0MQMH"
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/logs/"
      ],
      "metadata": {
        "id": "QRiQN6NDMPri"
      },
      "id": "QRiQN6NDMPri",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activation Functions\n",
        "- Use Sigmoid at output layer = [probability 0,1]\n",
        "- Between layers use tanh\n",
        "\n",
        "- Not sure of activation Use Relu max(0,x)\n",
        "- Leaky Relu max(01.x,x)"
      ],
      "metadata": {
        "id": "FRWn90lg-x26"
      },
      "id": "FRWn90lg-x26"
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "neNPjeFr_Ajf"
      },
      "id": "neNPjeFr_Ajf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid\n",
        "def sigmoid(x):\n",
        "  return 1/(1+math.exp(-x))"
      ],
      "metadata": {
        "id": "GQyZg7fs-xag"
      },
      "id": "GQyZg7fs-xag",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sigmoid(-0)"
      ],
      "metadata": {
        "id": "ZR8ikMnD_HGg"
      },
      "id": "ZR8ikMnD_HGg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tanh\n",
        "def tanh(x):\n",
        "  return (math.exp(x)-math.exp(-x))/(math.exp(x)+math.exp(-x))"
      ],
      "metadata": {
        "id": "DyWzbfP4_I5W"
      },
      "id": "DyWzbfP4_I5W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tanh(100)"
      ],
      "metadata": {
        "id": "E6zWV6VH_VKh"
      },
      "id": "E6zWV6VH_VKh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RELU\n",
        "def relu(x):\n",
        "  return max(0,x)\n",
        "def leaky_relu(x):\n",
        "  return max(0.1*x,x)"
      ],
      "metadata": {
        "id": "VEvHJrOp_W-I"
      },
      "id": "VEvHJrOp_W-I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relu(0.5)\n",
        "leaky_relu(-0.5)"
      ],
      "metadata": {
        "id": "ixanqQEn_aZq"
      },
      "id": "ixanqQEn_aZq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent in Neural\n",
        "- Keras directly model leke train and fit\n",
        "- get weights and bias after epochs\n",
        "- Python simple implementation for x and y matrix to get weights"
      ],
      "metadata": {
        "id": "oanPGZ6X0x9n"
      },
      "id": "oanPGZ6X0x9n"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "vtQOfIdh063d"
      },
      "id": "vtQOfIdh063d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"insurance_data.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "6JwCLWHT2Hn7"
      },
      "id": "6JwCLWHT2Hn7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = df[['age','affordibility']]\n",
        "x.head()\n",
        "y= df['bought_insurance']\n",
        "y.head()\n",
        "x.shape"
      ],
      "metadata": {
        "id": "1xAyVnXX2I4H"
      },
      "id": "1xAyVnXX2I4H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and Test and Scaling Data"
      ],
      "metadata": {
        "id": "T5LKOnTd2ZKF"
      },
      "id": "T5LKOnTd2ZKF"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[['age','affordibility']],df.bought_insurance,test_size=0.2, random_state=25)\n"
      ],
      "metadata": {
        "id": "ZOr0v6wl2XYe"
      },
      "id": "ZOr0v6wl2XYe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Scaling Data Age / 100"
      ],
      "metadata": {
        "id": "T5pcCCtC2nOS"
      },
      "id": "T5pcCCtC2nOS"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train_scaled = X_train.copy()\n",
        "X_train_scaled['age'] = X_train_scaled['age'] / 100\n",
        "\n",
        "X_test_scaled = X_test.copy()\n",
        "X_test_scaled['age'] = X_test_scaled['age'] / 100"
      ],
      "metadata": {
        "id": "NTcrwM2V2e3N"
      },
      "id": "NTcrwM2V2e3N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled.head()"
      ],
      "metadata": {
        "id": "feS8dMx32jwL"
      },
      "id": "feS8dMx32jwL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model-1 Keras and Tensorflow"
      ],
      "metadata": {
        "id": "kZIGsH_F2rOE"
      },
      "id": "kZIGsH_F2rOE"
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs to one output can use model from sklearn like logisitc but can be converted into\n",
        "# neurals if written in this way\n",
        "\n",
        "# Logistic Regression in Keras = Neural Network with:\n",
        "# 1 Dense layer\n",
        "\n",
        "# No hidden layers\n",
        "\n",
        "# Sigmoid activation (for binary classification)\n",
        "\n",
        "# Binary crossentropy loss\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(1, input_shape=(2,), activation='sigmoid', kernel_initializer='ones', bias_initializer='zeros')\n",
        "])\n",
        "\n",
        "# since logistic regression hai loss fucntion is log loss\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train_scaled, y_train, epochs=500)"
      ],
      "metadata": {
        "id": "xVj-5sNq2ubP"
      },
      "id": "xVj-5sNq2ubP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate the model"
      ],
      "metadata": {
        "id": "Cg3m7PMb3yQZ"
      },
      "id": "Cg3m7PMb3yQZ"
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test_scaled,y_test)"
      ],
      "metadata": {
        "id": "tyfGopKJ31Sb"
      },
      "id": "tyfGopKJ31Sb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "A9SAZx8y33MT"
      },
      "id": "A9SAZx8y33MT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_scaled"
      ],
      "metadata": {
        "id": "-BfefXEX37HE"
      },
      "id": "-BfefXEX37HE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "id": "Uduloo1k4Gk8"
      },
      "id": "Uduloo1k4Gk8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coeff and Bias from the model which was trained\n",
        "- This means w1=5.060867, w2=1.4086502, bias =-2.9137027"
      ],
      "metadata": {
        "id": "55lII7064LAy"
      },
      "id": "55lII7064LAy"
    },
    {
      "cell_type": "code",
      "source": [
        "coef, intercept = model.get_weights()\n",
        "coef, intercept"
      ],
      "metadata": {
        "id": "Kc2aWP_k4OTK"
      },
      "id": "Kc2aWP_k4OTK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model-2 Using Python Done in Siddharsan Video\n",
        "- Loss function know\n",
        "- x y w1 w2 b and gradiet descent is know\n",
        "- learning rate and number of iteration known"
      ],
      "metadata": {
        "id": "3EMoc4FK4UlB"
      },
      "id": "3EMoc4FK4UlB"
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_numpy(X):\n",
        "   return 1/(1+np.exp(-X))\n",
        "\n",
        "sigmoid_numpy(np.array([12,0,1]))"
      ],
      "metadata": {
        "id": "1pBwfTfJ4ekF"
      },
      "id": "1pBwfTfJ4ekF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_loss(y_true, y_predicted):\n",
        "    epsilon = 1e-15\n",
        "    y_predicted_new = [max(i,epsilon) for i in y_predicted]\n",
        "    y_predicted_new = [min(i,1-epsilon) for i in y_predicted_new]\n",
        "    y_predicted_new = np.array(y_predicted_new)\n",
        "    return -np.mean(y_true*np.log(y_predicted_new)+(1-y_true)*np.log(1-y_predicted_new))"
      ],
      "metadata": {
        "id": "ntTMVSqx5Mtf"
      },
      "id": "ntTMVSqx5Mtf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(age, affordability, y_true, epochs, loss_thresold):\n",
        "    w1 = w2 = 1\n",
        "    bias = 0\n",
        "    rate = 0.5 #Learning rate fix\n",
        "    n = len(age)\n",
        "    for i in range(epochs):\n",
        "        weighted_sum = w1 * age + w2 * affordability + bias\n",
        "        y_predicted = sigmoid_numpy(weighted_sum)\n",
        "        loss = log_loss(y_true, y_predicted)\n",
        "\n",
        "        w1d = (1/n)*np.dot(np.transpose(age),(y_predicted-y_true))\n",
        "        w2d = (1/n)*np.dot(np.transpose(affordability),(y_predicted-y_true))\n",
        "\n",
        "        bias_d = np.mean(y_predicted-y_true)\n",
        "        w1 = w1 - rate * w1d\n",
        "        w2 = w2 - rate * w2d\n",
        "        bias = bias - rate * bias_d\n",
        "\n",
        "        print (f'Epoch:{i}, w1:{w1}, w2:{w2}, bias:{bias}, loss:{loss}')\n",
        "\n",
        "        if loss<=loss_thresold:\n",
        "            break\n",
        "\n",
        "    return w1, w2, bias"
      ],
      "metadata": {
        "id": "9c3YiUK25XuG"
      },
      "id": "9c3YiUK25XuG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_descent(X_train_scaled['age'],X_train_scaled['affordibility'],y_train,500, 0.4631)"
      ],
      "metadata": {
        "id": "UdpONgyB5h2B"
      },
      "id": "UdpONgyB5h2B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coef, intercept"
      ],
      "metadata": {
        "id": "hblwdfW552MY"
      },
      "id": "hblwdfW552MY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network Implement Python\n",
        "- want fit, predict functions like models have inbuild"
      ],
      "metadata": {
        "id": "tJTB98TfoBYX"
      },
      "id": "tJTB98TfoBYX"
    },
    {
      "cell_type": "code",
      "source": [
        "class my_neural_network():\n",
        "  def __init__(self):\n",
        "    self.w1 = 1\n",
        "    self.w2=1\n",
        "    self.bias=0\n",
        "\n",
        "  def gradient_descent(self,age, affordability, y_true, epochs, loss_thresold):\n",
        "    w1 = w2 = 1\n",
        "    bias = 0\n",
        "    rate = 0.5 #Learning rate fix\n",
        "    n = len(age)\n",
        "    for i in range(epochs):\n",
        "        weighted_sum = w1 * age + w2 * affordability + bias\n",
        "        y_predicted = self.sigmoid_numpy(weighted_sum)\n",
        "        loss = self.log_loss(y_true, y_predicted)\n",
        "\n",
        "        w1d = (1/n)*np.dot(np.transpose(age),(y_predicted-y_true))\n",
        "        w2d = (1/n)*np.dot(np.transpose(affordability),(y_predicted-y_true))\n",
        "\n",
        "        bias_d = np.mean(y_predicted-y_true)\n",
        "        w1 = w1 - rate * w1d\n",
        "        w2 = w2 - rate * w2d\n",
        "        bias = bias - rate * bias_d\n",
        "        if i%10==0:\n",
        "          print (f'Epoch:{i}, w1:{w1}, w2:{w2}, bias:{bias}, loss:{loss}')\n",
        "\n",
        "        if loss<=loss_thresold:\n",
        "            break\n",
        "\n",
        "    return w1, w2, bias\n",
        "\n",
        "  def fit(self, x, y, epochs, loss_threshold):\n",
        "    self.w1 , self.w2, self.bias =self.gradient_descent(x['age'],x['affordibility'],y, epochs, loss_threshold)\n",
        "\n",
        "  def log_loss(self,y_true, y_predicted):\n",
        "    epsilon = 1e-15\n",
        "    y_predicted_new = [max(i,epsilon) for i in y_predicted]\n",
        "    y_predicted_new = [min(i,1-epsilon) for i in y_predicted_new]\n",
        "    y_predicted_new = np.array(y_predicted_new)\n",
        "    return -np.mean(y_true*np.log(y_predicted_new)+(1-y_true)*np.log(1-y_predicted_new))\n",
        "\n",
        "  def sigmoid_numpy(self,X):\n",
        "   return 1/(1+np.exp(-X))\n",
        "\n",
        "  def predict(self,x_test):\n",
        "    weighted_sum = self.w1*x_test['age'] + self.w2*x_test['affordibility'] + self.bias\n",
        "    return self.sigmoid_numpy(weighted_sum)"
      ],
      "metadata": {
        "id": "Ftm-7fnxoATm"
      },
      "id": "Ftm-7fnxoATm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customModel= my_neural_network()\n",
        "customModel.fit(X_train_scaled,y_train,epochs=100,loss_threshold=0.4631)"
      ],
      "metadata": {
        "id": "ataWKT6Iow0Q"
      },
      "id": "ataWKT6Iow0Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customModel.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "HVZpfAw0q69Z"
      },
      "id": "HVZpfAw0q69Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradien Descent Types\n",
        "- Bathc Gradient Descent\n",
        "  - All data points for every epoch goes through and calculate loss and adjust weight like we did before\n",
        "  - Problem 10millions rows and 5000 featutes then too much computation\n",
        "  - Good for small data\n",
        "- Stochastic Gradient\n",
        "  - use one(randomply picked) - loss y predict and adjust weight on every epochs\n",
        "  - good for high data\n",
        "- Mini Batch\n",
        "  - Similar to stochatic\n",
        "  - instead of one data use batch of some data"
      ],
      "metadata": {
        "id": "T_lznNSysN6C"
      },
      "id": "T_lznNSysN6C"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "r6DmOU9zsNaQ"
      },
      "id": "r6DmOU9zsNaQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"homeprices_banglore.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "7YAb1oQzugvo"
      },
      "id": "7YAb1oQzugvo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing data - Standard Scaler"
      ],
      "metadata": {
        "id": "PIFJvTMZvJ46"
      },
      "id": "PIFJvTMZvJ46"
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "4e79jUFvvO1F"
      },
      "id": "4e79jUFvvO1F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "c0y4So5VwLK1"
      },
      "id": "c0y4So5VwLK1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "sx = preprocessing.MinMaxScaler()\n",
        "sy = preprocessing.MinMaxScaler()\n",
        "\n",
        "x= df.drop('price',axis=1)\n",
        "y= df['price']\n",
        "scaled_X = sx.fit_transform(x)\n",
        "scaled_y = sy.fit_transform(y.values.reshape(-1,1))\n",
        "# -1 all rows and 1 means one column (rows,feature)\n",
        "scaled_X\n",
        "scaled_y"
      ],
      "metadata": {
        "id": "rMRi7wyBvSsd"
      },
      "id": "rMRi7wyBvSsd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert y to single array again\n",
        "scaled_y = scaled_y.flatten()\n",
        "scaled_y"
      ],
      "metadata": {
        "id": "LSDCTQ6dvxEx"
      },
      "id": "LSDCTQ6dvxEx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.ones(shape=(2,2))"
      ],
      "metadata": {
        "id": "Oue4RgD957p4"
      },
      "id": "Oue4RgD957p4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch Descent Gradient\n",
        "- use all data point of x to go through weights and bias\n",
        "- since our data is continuos\n",
        "- cost function is mean squared error\n",
        "- dJ/dw and dJ/db\n",
        "  - w_grad = -(2/total_samples)*(X.T.dot(y_true-y_predicted))\n",
        "  - b_grad = -(2/total_samples)*np.sum(y_true-y_predicted)\n",
        "        "
      ],
      "metadata": {
        "id": "yEyqE5Q85-Hr"
      },
      "id": "yEyqE5Q85-Hr"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "number_of_features = scaled_X.shape[1]\n",
        "print(scaled_X.T.shape)\n",
        "  # numpy array with 1 row and columns equal to number of features. In\n",
        "    # our case number_of_features = 2 (area, bedroom)\n",
        "w = np.ones(shape=(number_of_features))\n",
        "w.shape\n",
        "w"
      ],
      "metadata": {
        "id": "YpKOlMKf6_RN"
      },
      "id": "YpKOlMKf6_RN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C35iZirQ7Rg_"
      },
      "id": "C35iZirQ7Rg_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_gradient_descent(X, y_true, epochs, learning_rate = 0.01):\n",
        "\n",
        "    number_of_features = X.shape[1]\n",
        "    # numpy array with 1 row and columns equal to number of features. In\n",
        "    # our case number_of_features = 2 (area, bedroom)\n",
        "    w = np.ones(shape=(number_of_features))  # 1Xfeautures\n",
        "    b = 0\n",
        "    total_samples = X.shape[0] # number of rows in X\n",
        "\n",
        "    cost_list = []\n",
        "    epoch_list = []\n",
        "\n",
        "    for i in range(epochs):\n",
        "        y_predicted = np.dot(w, X.T) + b\n",
        "\n",
        "        w_grad = -(2/total_samples)*(X.T.dot(y_true-y_predicted))\n",
        "        b_grad = -(2/total_samples)*np.sum(y_true-y_predicted)\n",
        "\n",
        "        w = w - learning_rate * w_grad\n",
        "        b = b - learning_rate * b_grad\n",
        "\n",
        "        cost = np.mean(np.square(y_true-y_predicted)) # MSE (Mean Squared Error)\n",
        "\n",
        "        if i%10==0:\n",
        "            cost_list.append(cost)\n",
        "            epoch_list.append(i)\n",
        "\n",
        "    return w, b, cost, cost_list, epoch_list\n",
        "\n",
        "w, b, cost, cost_list, epoch_list = batch_gradient_descent(scaled_X,scaled_y.reshape(scaled_y.shape[0],),500)\n",
        "w, b, cost"
      ],
      "metadata": {
        "id": "OEFk2a0V6JcY"
      },
      "id": "OEFk2a0V6JcY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"cost\")\n",
        "plt.plot(epoch_list,cost_list)"
      ],
      "metadata": {
        "id": "K5Sp4-_a8QJg"
      },
      "id": "K5Sp4-_a8QJg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Predict Values using this model weight and bias"
      ],
      "metadata": {
        "id": "H3VqZ9xz82cC"
      },
      "id": "H3VqZ9xz82cC"
    },
    {
      "cell_type": "code",
      "source": [
        "check = sx.transform([[2400, 4]])[0]\n",
        "check\n",
        "# array([[0.55172414, 0.75      ]])\n",
        "# need first array\n",
        "\n",
        "price = w[0] * check[0] + w[1] * check[1] + b\n",
        "price"
      ],
      "metadata": {
        "id": "VqG-T5b_9hTz"
      },
      "id": "VqG-T5b_9hTz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn import preprocessing\n",
        "# sx = preprocessing.MinMaxScaler()\n",
        "# sy = preprocessing.MinMaxScaler()\n",
        "# used for training and standarisation\n",
        "\n",
        "def predict(area,bedrooms,w,b):\n",
        "\n",
        "    scaled_X = sx.transform([[area, bedrooms]])[0]\n",
        "    # pass array of 2d return array of 2d need firrst element\n",
        "\n",
        "    # here w1 = w[0] , w2 = w[1], w3 = w[2] and bias is b\n",
        "    # equation for price is w1*area + w2*bedrooms + w3*age + bias\n",
        "    # scaled_X[0] is area\n",
        "    # scaled_X[1] is bedrooms\n",
        "    # scaled_X[2] is age\n",
        "    scaled_price = w[0] * scaled_X[0] + w[1] * scaled_X[1] + b\n",
        "    # once we get price prediction we need to to rescal it back to original value\n",
        "    # also since it returns 2D array, to get single value we need to do value[0][0]\n",
        "    return sy.inverse_transform([[scaled_price]])[0][0]\n",
        "\n",
        "predict(1056,2,w,b)\n",
        "# w and b from gradient descent"
      ],
      "metadata": {
        "id": "J88oY4cH86qn"
      },
      "id": "J88oY4cH86qn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stochastic Descent Gradient\n",
        "- one random sample each epochs"
      ],
      "metadata": {
        "id": "lLh51fXg-mUf"
      },
      "id": "lLh51fXg-mUf"
    },
    {
      "cell_type": "code",
      "source": [
        "# we will use random libary to pick random training sample.\n",
        "import random\n",
        "random.randint(0,6)\n",
        "\n",
        "\n",
        "# scaled_y.reshape(scaled_y.shape[0],)\n",
        "\n",
        "#2d array (20, 1) needed for tranformation\n",
        "scaled_y.shape\n",
        "\n",
        "# convert to single array (20,)\n",
        "a= scaled_y.reshape(scaled_y.shape[0],)\n",
        "a.shape\n",
        "\n",
        "number_of_features = scaled_X.shape[1]\n",
        "w = np.ones(shape=(number_of_features))\n",
        "sample_x = scaled_X[3]\n",
        "\n",
        "c= np.dot(w, sample_x.T)\n",
        "c"
      ],
      "metadata": {
        "id": "Ds1hDFrs-vGQ"
      },
      "id": "Ds1hDFrs-vGQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stochastic_gradient_descent(X, y_true, epochs, learning_rate = 0.01):\n",
        "\n",
        "    number_of_features = X.shape[1]\n",
        "    # numpy array with 1 row and columns equal to number of features. In\n",
        "    # our case number_of_features = 3 (area, bedroom and age)\n",
        "    w = np.ones(shape=(number_of_features))\n",
        "    b = 0\n",
        "    total_samples = X.shape[0]\n",
        "\n",
        "    cost_list = []\n",
        "    epoch_list = []\n",
        "\n",
        "    for i in range(epochs):\n",
        "        # here randomly one sample le rahe\n",
        "        random_index = random.randint(0,total_samples-1) # random index from total samples\n",
        "        sample_x = X[random_index]\n",
        "        sample_y = y_true[random_index]\n",
        "        # Both sample y and y_predicted is single float value\n",
        "        y_predicted = np.dot(w, sample_x.T) + b\n",
        "\n",
        "        w_grad = -(2/total_samples)*(sample_x.T.dot(sample_y-y_predicted))\n",
        "        b_grad = -(2/total_samples)*(sample_y-y_predicted)\n",
        "\n",
        "        w = w - learning_rate * w_grad\n",
        "        b = b - learning_rate * b_grad\n",
        "\n",
        "        cost = np.square(sample_y-y_predicted)\n",
        "\n",
        "        if i%100==0: # at every 100th iteration record the cost and epoch value\n",
        "            cost_list.append(cost)\n",
        "            epoch_list.append(i)\n",
        "\n",
        "    return w, b, cost, cost_list, epoch_list\n",
        "\n",
        "w_sgd, b_sgd, cost_sgd, cost_list_sgd, epoch_list_sgd = stochastic_gradient_descent(scaled_X,scaled_y.reshape(scaled_y.shape[0],),10000)\n",
        "w_sgd, b_sgd, cost_sgd"
      ],
      "metadata": {
        "id": "1lTI4_yA-v7T"
      },
      "id": "1lTI4_yA-v7T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"cost\")\n",
        "plt.plot(epoch_list_sgd,cost_list_sgd)"
      ],
      "metadata": {
        "id": "pMuIVhyBBuX2"
      },
      "id": "pMuIVhyBBuX2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(1000,2,w_sgd, b_sgd)"
      ],
      "metadata": {
        "id": "He4H07jPCo_p"
      },
      "id": "He4H07jPCo_p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mini-Batch Gradient Descent\n",
        "- Batch of 20 samples aise lo"
      ],
      "metadata": {
        "id": "1wss5SXIC3qs"
      },
      "id": "1wss5SXIC3qs"
    },
    {
      "cell_type": "code",
      "source": [
        "random_indices= np.random.permutation(20)\n",
        "X_tmp = scaled_X[random_indices]\n",
        "X_tmp\n",
        "y_tmp = scaled_y.reshape(scaled_y.shape[0],)[random_indices]\n",
        "y_tmp"
      ],
      "metadata": {
        "id": "_6sRwnCuC5yl"
      },
      "id": "_6sRwnCuC5yl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mini_batch_gradient_descent(X, y_true, epochs = 100, batch_size = 5, learning_rate = 0.01):\n",
        "\n",
        "    number_of_features = X.shape[1]\n",
        "    # numpy array with 1 row and columns equal to number of features. In\n",
        "    # our case number_of_features = 3 (area, bedroom and age)\n",
        "    w = np.ones(shape=(number_of_features))\n",
        "    b = 0\n",
        "    total_samples = X.shape[0] # number of rows in X\n",
        "\n",
        "    if batch_size > total_samples: # In this case mini batch becomes same as batch gradient descent\n",
        "        batch_size = total_samples\n",
        "\n",
        "    cost_list = []\n",
        "    epoch_list = []\n",
        "\n",
        "    num_batches = int(total_samples/batch_size)\n",
        "\n",
        "    for i in range(epochs):\n",
        "        random_indices = np.random.permutation(total_samples)\n",
        "        X_tmp = X[random_indices]\n",
        "        y_tmp = y_true[random_indices]\n",
        "\n",
        "        for j in range(0,total_samples,batch_size):\n",
        "            Xj = X_tmp[j:j+batch_size]\n",
        "            yj = y_tmp[j:j+batch_size]\n",
        "            y_predicted = np.dot(w, Xj.T) + b\n",
        "\n",
        "            w_grad = -(2/len(Xj))*(Xj.T.dot(yj-y_predicted))\n",
        "            b_grad = -(2/len(Xj))*np.sum(yj-y_predicted)\n",
        "\n",
        "            w = w - learning_rate * w_grad\n",
        "            b = b - learning_rate * b_grad\n",
        "\n",
        "            cost = np.mean(np.square(yj-y_predicted)) # MSE (Mean Squared Error)\n",
        "\n",
        "        if i%10==0:\n",
        "            cost_list.append(cost)\n",
        "            epoch_list.append(i)\n",
        "\n",
        "    return w, b, cost, cost_list, epoch_list\n",
        "\n",
        "w, b, cost, cost_list, epoch_list = mini_batch_gradient_descent(\n",
        "    scaled_X,\n",
        "    scaled_y.reshape(scaled_y.shape[0],),\n",
        "    epochs = 120,\n",
        "    batch_size = 5\n",
        ")\n",
        "w, b, cost"
      ],
      "metadata": {
        "id": "AnWNYKbdDD23"
      },
      "id": "AnWNYKbdDD23",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"cost\")\n",
        "plt.plot(epoch_list,cost_list)"
      ],
      "metadata": {
        "id": "0z-4FBmZD5Ys"
      },
      "id": "0z-4FBmZD5Ys",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPU bench-marking with image classification PROJECT-2\n",
        "- CIFAR-10 dataset -"
      ],
      "metadata": {
        "id": "fnFeP4GIxjqY"
      },
      "id": "fnFeP4GIxjqY"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# Version Information\n",
        "# tensorflow 2.2.0 , Cudnn7.6.5 and Cuda 10.1 , python 3.8"
      ],
      "metadata": {
        "id": "z5PrzP-nx7Mf"
      },
      "id": "z5PrzP-nx7Mf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get GPU information on you computer\n",
        "tf.config.experimental.list_physical_devices()"
      ],
      "metadata": {
        "id": "0SpAbgY8yFdA"
      },
      "id": "0SpAbgY8yFdA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "metadata": {
        "id": "M6l2FrGFyVPb"
      },
      "id": "M6l2FrGFyVPb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Means for Deep Learning can use GPU\n",
        "tf.test.is_built_with_cuda()"
      ],
      "metadata": {
        "id": "3DcnziPSyWIt"
      },
      "id": "3DcnziPSyWIt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "- Keras\n",
        "  - CIFAR-10 dataset - 60k images 10classes\n",
        "    - airplance, bird, cat, dog, horse\n",
        "  - Use Artifical Neural Network ANN\n",
        "  - Colourful images is made of [Red, Green, Blue] array\n",
        "  - Like handdigit project uses [white and black]"
      ],
      "metadata": {
        "id": "6Zy8Ldolyepe"
      },
      "id": "6Zy8Ldolyepe"
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test,y_test) = tf.keras.datasets.cifar10.load_data()"
      ],
      "metadata": {
        "id": "nr_a5dJzytyR"
      },
      "id": "nr_a5dJzytyR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape # (50000, 32, 32, 3)\n",
        "X_train[0][31][30]\n",
        "# ndarray (32, 32, 3)\n",
        "# each image is 32 rows 32 columns and each pixel is [R G B] value\n",
        "# 0 to 255 values for each R G B"
      ],
      "metadata": {
        "id": "t_DmFzn6zESq"
      },
      "id": "t_DmFzn6zESq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figsize=(10,1)\n",
        "plt.imshow(X_train[4])"
      ],
      "metadata": {
        "id": "aUzs1NVD0kjE"
      },
      "id": "aUzs1NVD0kjE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Y already into 10 classes\n",
        "y_train.shape #(50000, 1) 2d array hai single column\n",
        "y_train[0]\n",
        "y_train[0][0]"
      ],
      "metadata": {
        "id": "wzKOsNxdzGmQ"
      },
      "id": "wzKOsNxdzGmQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"]"
      ],
      "metadata": {
        "id": "s3gnjQe61Ce8"
      },
      "id": "s3gnjQe61Ce8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes[y_train[0][0]]\n",
        "# Index from y_train mapped to correct name"
      ],
      "metadata": {
        "id": "YZ6QWLWg1Cgv"
      },
      "id": "YZ6QWLWg1Cgv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normaling x_train data\n",
        "- divide by 255"
      ],
      "metadata": {
        "id": "-QJA94QA1TxO"
      },
      "id": "-QJA94QA1TxO"
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_scaled = X_train / 255\n",
        "x_test_scaled = X_test /255"
      ],
      "metadata": {
        "id": "Ll9prVK_1YN5"
      },
      "id": "Ll9prVK_1YN5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_scaled.shape"
      ],
      "metadata": {
        "id": "D6mN_DZx1fF9"
      },
      "id": "D6mN_DZx1fF9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One hot encoding\n",
        "- y_train ko 10 classes mein each row ko convert\n",
        "- array([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
        "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
        "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
        "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
        "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ],
      "metadata": {
        "id": "tyoIJkAR1xST"
      },
      "id": "tyoIJkAR1xST"
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[:3]"
      ],
      "metadata": {
        "id": "GRw-9tgQ19wP"
      },
      "id": "GRw-9tgQ19wP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_categorical = keras.utils.to_categorical(\n",
        "    y_train, num_classes=10\n",
        ")\n",
        "y_test_categorical = keras.utils.to_categorical(\n",
        "    y_test, num_classes=10\n",
        ")"
      ],
      "metadata": {
        "id": "HpMYslJl126v"
      },
      "id": "HpMYslJl126v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting each y value into array of categories\n",
        "y_train_categorical[0:5]"
      ],
      "metadata": {
        "id": "W6IzbJX42CbO"
      },
      "id": "W6IzbJX42CbO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model- Training and Prediciton\n",
        "- need to flatten data like we did in digit recognistion\n",
        "- 28*28 one iamge convert into 784 xi values\n",
        "- similar here 32 32 3 one image represent\n",
        "- Hidden layer PREFERD Relu used\n",
        "- Classification output sigmoid or sigmoid use"
      ],
      "metadata": {
        "id": "d3qpwZno2li4"
      },
      "id": "d3qpwZno2li4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Ouput layer has 10 classes\n",
        "# Activation signmoidn as it is good with classificaltion\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(32,32,3)),\n",
        "    keras.layers.Dense(3000,activation='relu'),\n",
        "    keras.layers.Dense(3000,activation='relu'),\n",
        "    keras.layers.Dense(10,activation='sigmoid')\n",
        "\n",
        "])\n",
        "# Hidden layer PREFERD Relu used\n",
        "\n",
        "model.compile(optimizer='SGD',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy']\n",
        "              )\n",
        "\n",
        "# Since my y_train_categorcial data\n",
        "# y_train_cateogrical [[0,0,0,1],[1,0,0,]] categories distributed\n",
        "# Use categorical_crossentropy\n",
        "# In Handwrittten digit y_train was [1,2,3,4]\n",
        "# SO used sparse_categoriacal\n",
        "\n",
        "model.fit(x_train_scaled, y_train_categorical, epochs=1)\n",
        "\n",
        "\n",
        "\n",
        "# If y_train is not categorized same way as in Handdigit\n",
        "# model.compile(optimizer='adam',\n",
        "#                   loss='sparse_categorical_crossentropy',\n",
        "#                   metrics=['accuracy'])\n",
        "# model.fit(x_train_scaled, y_train, epochs=1)"
      ],
      "metadata": {
        "id": "2DZpJKM_3CRu"
      },
      "id": "2DZpJKM_3CRu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction"
      ],
      "metadata": {
        "id": "kKbYBLFj68wB"
      },
      "id": "kKbYBLFj68wB"
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmax(model.predict(x_test_scaled)[0])"
      ],
      "metadata": {
        "id": "XEFq5YEf4Cq2"
      },
      "id": "XEFq5YEf4Cq2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test[0]"
      ],
      "metadata": {
        "id": "KRNNvzBZ7PQZ"
      },
      "id": "KRNNvzBZ7PQZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Measure training time on a CPU"
      ],
      "metadata": {
        "id": "XnEp-JVG7Tnk"
      },
      "id": "XnEp-JVG7Tnk"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model():\n",
        "    model = keras.Sequential([\n",
        "            keras.layers.Flatten(input_shape=(32,32,3)),\n",
        "            keras.layers.Dense(3000, activation='relu'),\n",
        "            keras.layers.Dense(1000, activation='relu'),\n",
        "            keras.layers.Dense(10, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "tJ2FzaG17UaD"
      },
      "id": "tJ2FzaG17UaD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.experimental.list_physical_devices()\n",
        "# [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ],
      "metadata": {
        "id": "IW3_ZquF7Y7p"
      },
      "id": "IW3_ZquF7Y7p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n1 -r1\n",
        "with tf.device('/CPU:0'):\n",
        "    cpu_model = get_model()\n",
        "    cpu_model.fit(X_train_scaled, y_train_categorical, epochs=1)\n",
        "\n",
        "# return. 3.6 s  0 ns per loop (mean  std. dev. of 1 run, 1 loop each)"
      ],
      "metadata": {
        "id": "3Gy4nSFi7WPh"
      },
      "id": "3Gy4nSFi7WPh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Customer Churn Project-3 Using ANN\n",
        "- Measure how many customers are leaving the business\n",
        "- why customers are leaving the business\n",
        "- Telco Customer Churn Data from Kaggle"
      ],
      "metadata": {
        "id": "iILqRlGwgTfj"
      },
      "id": "iILqRlGwgTfj"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "DCEMf3iFhEXu"
      },
      "id": "DCEMf3iFhEXu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/CustomerChurn.csv\")\n",
        "df.sample(5)"
      ],
      "metadata": {
        "id": "8dhNkcnghFSk"
      },
      "id": "8dhNkcnghFSk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "wqWfFbPShJcu"
      },
      "id": "wqWfFbPShJcu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Exploration\n",
        "- customerId is useless\n",
        "- All unique values - 1 and 0 convert\n",
        "- if column has 3,4 unique values create dummies column\n",
        "- Float values need to be scaled MinMax StandardScaler"
      ],
      "metadata": {
        "id": "EG04m-NEhPte"
      },
      "id": "EG04m-NEhPte"
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop('customerID',axis='columns',inplace=True)"
      ],
      "metadata": {
        "id": "rI-y-rUUhVJJ"
      },
      "id": "rI-y-rUUhVJJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "OZLL-ItIhXPi"
      },
      "id": "OZLL-ItIhXPi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Total Charges is object\n",
        "Monthly Charges is Number\n"
      ],
      "metadata": {
        "id": "gEXjhDl9hf82"
      },
      "id": "gEXjhDl9hf82"
    },
    {
      "cell_type": "code",
      "source": [
        "df.TotalCharges.values\n",
        "# String convert into number"
      ],
      "metadata": {
        "id": "VGUnSeIchk9a"
      },
      "id": "VGUnSeIchk9a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.MonthlyCharges.values"
      ],
      "metadata": {
        "id": "J8ImTwNqhpPV"
      },
      "id": "J8ImTwNqhpPV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "HK1F-ZyOhrn9"
      },
      "id": "HK1F-ZyOhrn9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.to_numeric(df['TotalCharges'])\n",
        "# Creating problem as some places ' ' is present remove them"
      ],
      "metadata": {
        "id": "SUecMjp8hyES"
      },
      "id": "SUecMjp8hyES",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding Total Charges Blank"
      ],
      "metadata": {
        "id": "aTDQwd7didfn"
      },
      "id": "aTDQwd7didfn"
    },
    {
      "cell_type": "code",
      "source": [
        "invalid_total_charges = df[pd.to_numeric(df['TotalCharges'], errors='coerce').isnull()]"
      ],
      "metadata": {
        "id": "VsZtDVjCiItL"
      },
      "id": "VsZtDVjCiItL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "invalid_total_charges.shape"
      ],
      "metadata": {
        "id": "k7cn3BuXiRtd"
      },
      "id": "k7cn3BuXiRtd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing and updating the df"
      ],
      "metadata": {
        "id": "CzopbBJejT87"
      },
      "id": "CzopbBJejT87"
    },
    {
      "cell_type": "code",
      "source": [
        "# df1= df[df.TotalCharges != ' ']\n",
        "# df1.shape\n",
        "# Or\n",
        "df = df[pd.to_numeric(df['TotalCharges'], errors='coerce').notnull()]\n",
        "df.shape"
      ],
      "metadata": {
        "id": "OBCm-P47jBFv"
      },
      "id": "OBCm-P47jBFv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting Object to String now"
      ],
      "metadata": {
        "id": "JIPOP3wijWiC"
      },
      "id": "JIPOP3wijWiC"
    },
    {
      "cell_type": "code",
      "source": [
        "df['TotalCharges']=pd.to_numeric(df['TotalCharges'])"
      ],
      "metadata": {
        "id": "7JaHN_3WjV9B"
      },
      "id": "7JaHN_3WjV9B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['TotalCharges'].dtypes"
      ],
      "metadata": {
        "id": "kRGxQXFMjhdv"
      },
      "id": "kRGxQXFMjhdv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Histogram Visualization\n",
        "- Tenure x axis\n",
        "- y axis count using Churn i.e. leaving"
      ],
      "metadata": {
        "id": "2VryzQjXjxjf"
      },
      "id": "2VryzQjXjxjf"
    },
    {
      "cell_type": "code",
      "source": [
        "((df['Churn'] == 'No') & (df['tenure'] >= 70)).sum()"
      ],
      "metadata": {
        "id": "S1SMgFUlj7Dg"
      },
      "id": "S1SMgFUlj7Dg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tenure_churn_no = df[df['Churn'] == 'No'].tenure\n",
        "tenure_churn_yes = df[df['Churn'] == 'Yes'].tenure\n",
        "plt.hist([tenure_churn_no,tenure_churn_yes],color=['red','green'],label=['Churn=Yes','Churn=No'])\n",
        "plt.legend()\n",
        "plt.xlabel(\"tenure\")\n",
        "plt.ylabel(\"Number of customer\")"
      ],
      "metadata": {
        "id": "x4Uf3w47khGe"
      },
      "id": "x4Uf3w47khGe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Monthly charges"
      ],
      "metadata": {
        "id": "7uEXizVdl6oo"
      },
      "id": "7uEXizVdl6oo"
    },
    {
      "cell_type": "code",
      "source": [
        "mc_churn_no = df[df.Churn=='No'].MonthlyCharges\n",
        "mc_churn_yes = df[df.Churn=='Yes'].MonthlyCharges\n",
        "\n",
        "plt.xlabel(\"Monthly Charges\")\n",
        "plt.ylabel(\"Number Of Customers\")\n",
        "plt.title(\"Customer Churn Prediction Visualiztion\")\n",
        "\n",
        "plt.hist([mc_churn_yes, mc_churn_no], rwidth=0.95, color=['green','red'],label=['Churn=Yes','Churn=No'])\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "I42YTaSDl74V"
      },
      "id": "I42YTaSDl74V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label Encoding\n",
        "Columns with Yes and No"
      ],
      "metadata": {
        "id": "pLkg8dy-mRc0"
      },
      "id": "pLkg8dy-mRc0"
    },
    {
      "cell_type": "code",
      "source": [
        "df['gender'].unique()"
      ],
      "metadata": {
        "id": "wh5YlJQ8mXRJ"
      },
      "id": "wh5YlJQ8mXRJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for column in df actually iterates over column names (i.e., strings), not rows\n",
        "# df is a pandas DataFrame, not a basic array.\n",
        "\n",
        "for column in df:\n",
        "  if df[column].dtypes=='object':\n",
        "    print({column}, df[column].unique())"
      ],
      "metadata": {
        "id": "Xz6WCqU-mmDH"
      },
      "id": "Xz6WCqU-mmDH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replacing Values with Yes and No Then converting into numbers"
      ],
      "metadata": {
        "id": "g4vT9QU2nVnZ"
      },
      "id": "g4vT9QU2nVnZ"
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_clean = [\n",
        "    'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n",
        "    'TechSupport', 'StreamingTV', 'StreamingMovies'\n",
        "]\n",
        "\n",
        "for col in columns_to_clean:\n",
        "    df[col] = df[col].replace('No internet service', 'No')\n",
        "\n",
        "df['MultipleLines']= df['MultipleLines'].replace('No phone service', 'No')\n",
        "df['StreamingTV'].unique()"
      ],
      "metadata": {
        "id": "JrPqrBxSnZdy"
      },
      "id": "JrPqrBxSnZdy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for column in df:\n",
        "  if df[column].dtypes=='object':\n",
        "    print({column}, df[column].unique())"
      ],
      "metadata": {
        "id": "MP3QB62hnmRD"
      },
      "id": "MP3QB62hnmRD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replace Yes and No with 1 and 0"
      ],
      "metadata": {
        "id": "VW79XCKEoHel"
      },
      "id": "VW79XCKEoHel"
    },
    {
      "cell_type": "code",
      "source": [
        "yes_no_columns = [\n",
        "    'Partner', 'Dependents', 'PhoneService', 'MultipleLines',\n",
        "    'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',\n",
        "    'StreamingTV', 'StreamingMovies', 'PaperlessBilling', 'Churn'\n",
        "]\n",
        "df[yes_no_columns] = df[yes_no_columns].replace({'Yes': 1, 'No': 0})"
      ],
      "metadata": {
        "id": "XY3itTHZoJqY"
      },
      "id": "XY3itTHZoJqY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df:\n",
        "  print(col,df[col].unique())"
      ],
      "metadata": {
        "id": "4wRCOEwzoSmi"
      },
      "id": "4wRCOEwzoSmi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gender Male=0 Female=1"
      ],
      "metadata": {
        "id": "fFBxgWk1oltN"
      },
      "id": "fFBxgWk1oltN"
    },
    {
      "cell_type": "code",
      "source": [
        "df['gender'].replace({'Female':1,'Male':0},inplace=True)"
      ],
      "metadata": {
        "id": "aAEv8PVtolWT"
      },
      "id": "aAEv8PVtolWT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One Hot Encoding for categorical columns\n",
        "- Columns InternetServi ce has three object\n",
        "- Create 3 columns InternetService1,2,3 with value 1,0\n",
        "- pd.get_dummies()"
      ],
      "metadata": {
        "id": "9PiUipQwo_8b"
      },
      "id": "9PiUipQwo_8b"
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.get_dummies(data=df, columns=['InternetService', 'Contract', 'PaymentMethod'], dtype=int)\n",
        "df1.columns"
      ],
      "metadata": {
        "id": "ih1ftcurpMZz"
      },
      "id": "ih1ftcurpMZz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.head()"
      ],
      "metadata": {
        "id": "7cVPXhjnpl63"
      },
      "id": "7cVPXhjnpl63",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.dtypes"
      ],
      "metadata": {
        "id": "rZVTU4R9pYv6"
      },
      "id": "rZVTU4R9pYv6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tenure, Monthly Charges and TotalCharges need scaling"
      ],
      "metadata": {
        "id": "mNnkWMUgtemV"
      },
      "id": "mNnkWMUgtemV"
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_scale = ['tenure','MonthlyCharges','TotalCharges']\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "df1[cols_to_scale] = scaler.fit_transform(df1[cols_to_scale])"
      ],
      "metadata": {
        "id": "X4eh0Mgnt1pR"
      },
      "id": "X4eh0Mgnt1pR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df1:\n",
        "    print(f'{col}: {df1[col].unique()}')"
      ],
      "metadata": {
        "id": "4e2rGOGdt9PN"
      },
      "id": "4e2rGOGdt9PN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training and Testing"
      ],
      "metadata": {
        "id": "vLo12fYFuGtN"
      },
      "id": "vLo12fYFuGtN"
    },
    {
      "cell_type": "code",
      "source": [
        "X = df1.drop('Churn',axis='columns')\n",
        "y = df1['Churn']\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=5)"
      ],
      "metadata": {
        "id": "l6aQ-OITuJpE"
      },
      "id": "l6aQ-OITuJpE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "Ud6ZW2XNuR9y"
      },
      "id": "Ud6ZW2XNuR9y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "id": "AivyNGtauT4v"
      },
      "id": "AivyNGtauT4v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build ANN Model"
      ],
      "metadata": {
        "id": "ZsBwoo2ouXhT"
      },
      "id": "ZsBwoo2ouXhT"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(26, input_shape=(26,), activation='relu'),\n",
        "    # keras.layers.Dense(15, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "    # output is 1 and 0 toh probability dega using sigmoid\n",
        "])\n",
        "\n",
        "# opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "# output is 0 and 1 binary_crossentropy categorical\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=5)"
      ],
      "metadata": {
        "id": "oCKYy063uaLT"
      },
      "id": "oCKYy063uaLT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "iGNb4oPiu13-"
      },
      "id": "iGNb4oPiu13-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yp = model.predict(X_test)\n",
        "yp[:5]"
      ],
      "metadata": {
        "id": "Bjn9hjoou5i4"
      },
      "id": "Bjn9hjoou5i4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test[:5]"
      ],
      "metadata": {
        "id": "tCZOKc8PvH8f"
      },
      "id": "tCZOKc8PvH8f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output is 1 and 0 toh probability dega using sigmoid\n",
        "y_pred = []\n",
        "for element in yp:\n",
        "    if element > 0.5:\n",
        "        y_pred.append(1)\n",
        "    else:\n",
        "        y_pred.append(0)"
      ],
      "metadata": {
        "id": "tASZ-ERGu-C8"
      },
      "id": "tASZ-ERGu-C8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred[:5]"
      ],
      "metadata": {
        "id": "iK6U-w7vvOhF"
      },
      "id": "iK6U-w7vvOhF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion Metrics and Precision F-1 Call"
      ],
      "metadata": {
        "id": "S-jj9LQJvVHu"
      },
      "id": "S-jj9LQJvVHu"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix , classification_report\n",
        "\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "s4A8CaN8vfXM"
      },
      "id": "s4A8CaN8vfXM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sn\n",
        "cm = tf.math.confusion_matrix(labels=y_test,predictions=y_pred)\n",
        "\n",
        "plt.figure(figsize = (10,7))\n",
        "sn.heatmap(cm, annot=True, fmt='d')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Truth')"
      ],
      "metadata": {
        "id": "1DRrH51Rvhgt"
      },
      "id": "1DRrH51Rvhgt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling Imbalanced Dataset - Customer Churn Project\n",
        "- Due to imbalance in customer churn f-1 score is low for 0 and 1 case\n",
        "- Examples\n",
        "  - customer churn ,cancer prediction, device failure"
      ],
      "metadata": {
        "id": "KJG3Ey0JNM1R"
      },
      "id": "KJG3Ey0JNM1R"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Techniques\n",
        "1. Undersampling majority class 900 F 100 R\n",
        "  - Pick random 100 F and 100 R\n",
        "2. Oversampling\n",
        "  - Duplicate 100*9 R 900R and 900F\n",
        "3. SMOTE - Synthetic minority element ka over-sampling tecnique\n",
        "  - generate examples using k nearest neighbors algo\n",
        "  - 100*9 R 900R and 900F\n",
        "  - use library imblearn.over_sampling import\n",
        "  SMOTE\n",
        "4. Ensemble Method Avg Lena Divide Karke\n",
        "  - 900 F -> Divide 300 300 300 Take 100 F and 100 R combo 1, 2 ,3\n",
        "  - Final result avg/ majority vote of all three\n",
        "5. Focal Loss\n",
        "  - penalize to majority class and more weight to minorty class"
      ],
      "metadata": {
        "id": "KcgLzwpLOXn2"
      },
      "id": "KcgLzwpLOXn2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that by using dropout layer test accuracy increased from 0.77 to 0.81"
      ],
      "metadata": {
        "id": "PSrDfROgM-aF"
      },
      "id": "PSrDfROgM-aF"
    },
    {
      "cell_type": "code",
      "source": [
        "# df1 is the final dataframe after data processing\n",
        "df1.Churn.value_counts()\n",
        "# Churn\n",
        "# 0\t5163\n",
        "# 1\t1869"
      ],
      "metadata": {
        "id": "_VlkSmEAUORR"
      },
      "id": "_VlkSmEAUORR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.columns"
      ],
      "metadata": {
        "id": "tYCpJ515VAkf"
      },
      "id": "tYCpJ515VAkf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BUILD ANN MODEL Function to be used by all methods"
      ],
      "metadata": {
        "id": "-rSfnZxxXMUj"
      },
      "id": "-rSfnZxxXMUj"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import confusion_matrix , classification_report"
      ],
      "metadata": {
        "id": "bst_aBdjXL4c"
      },
      "id": "bst_aBdjXL4c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ANN(X_train, y_train, X_test, y_test, loss, weights):\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Dense(26, input_dim=26, activation='relu'),\n",
        "        keras.layers.Dense(15, activation='relu'),\n",
        "        keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
        "\n",
        "    if weights == -1:\n",
        "        model.fit(X_train, y_train, epochs=100)\n",
        "    else:\n",
        "        model.fit(X_train, y_train, epochs=100, class_weight = weights)\n",
        "\n",
        "    print(model.evaluate(X_test, y_test))\n",
        "\n",
        "    y_preds = model.predict(X_test)\n",
        "    y_preds = np.round(y_preds)\n",
        "\n",
        "    print(\"Classification Report: \\n\", classification_report(y_test, y_preds))\n",
        "\n",
        "    return y_preds"
      ],
      "metadata": {
        "id": "TxqvN93_XTzc"
      },
      "id": "TxqvN93_XTzc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1- Undersampling"
      ],
      "metadata": {
        "id": "5HFRmsHUVYoo"
      },
      "id": "5HFRmsHUVYoo"
    },
    {
      "cell_type": "code",
      "source": [
        "# Same amount of zero and ones\n",
        "class_0_count, class_1_count= df1.Churn.value_counts()\n",
        "class_0_count, class_1_count"
      ],
      "metadata": {
        "id": "nLYAj744VaLC"
      },
      "id": "nLYAj744VaLC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_class_0 = df1[df1['Churn']==0]\n",
        "df_class_1 = df1[df1['Churn']==1]"
      ],
      "metadata": {
        "id": "TvfTZ49CV3o_"
      },
      "id": "TvfTZ49CV3o_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_class_0.shape"
      ],
      "metadata": {
        "id": "6mPDoSeAV-gl"
      },
      "id": "6mPDoSeAV-gl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_class_1.shape"
      ],
      "metadata": {
        "id": "Pq8rSFzDV_ot"
      },
      "id": "Pq8rSFzDV_ot",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class 0 se sample collect of size class 1"
      ],
      "metadata": {
        "id": "yimGVkrsWBhz"
      },
      "id": "yimGVkrsWBhz"
    },
    {
      "cell_type": "code",
      "source": [
        "df_class_0_sameas1= df_class_0.sample(class_1_count)\n",
        "df_class_0_sameas1.shape"
      ],
      "metadata": {
        "id": "8k5upSI-WFQW"
      },
      "id": "8k5upSI-WFQW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine to create a new df\n",
        "df_underfitting = pd.concat([df_class_0_sameas1,df_class_1])\n",
        "df_underfitting.shape"
      ],
      "metadata": {
        "id": "hEwUgZxqVz_q"
      },
      "id": "hEwUgZxqVz_q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_underfitting['Churn'].value_counts()\n",
        "# Churn\n",
        "# 0\t1869\n",
        "# 1\t1869\n"
      ],
      "metadata": {
        "id": "VViEXBaSWWzh"
      },
      "id": "VViEXBaSWWzh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training using this df_underfitting"
      ],
      "metadata": {
        "id": "jcBJlzMxWjbp"
      },
      "id": "jcBJlzMxWjbp"
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_underfitting.drop('Churn',axis='columns')\n",
        "y = df_underfitting['Churn']\n",
        "\n",
        "#Stratity y meand y_train  and y_test equal number of 0 and 1 divide\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15, stratify=y)"
      ],
      "metadata": {
        "id": "yVSVIFmvWjLO"
      },
      "id": "yVSVIFmvWjLO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of classes in training Data\n",
        "y_train.value_counts()\n",
        "# 0\t1495\n",
        "# 1\t1495"
      ],
      "metadata": {
        "id": "hQ9gSpOIWsx1"
      },
      "id": "hQ9gSpOIWsx1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.value_counts()\n",
        "\n",
        "# 1\t374\n",
        "# 0\t374\n"
      ],
      "metadata": {
        "id": "eh0YfDmBW4tE"
      },
      "id": "eh0YfDmBW4tE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds_undersampling = ANN(X_train, y_train, X_test, y_test, 'binary_crossentropy', -1)\n",
        "# f1 score for 0 and 1 imporved now"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_33OCfaIXq4z"
      },
      "id": "_33OCfaIXq4z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2- Oversampling"
      ],
      "metadata": {
        "id": "D9kKkQTZYJAa"
      },
      "id": "D9kKkQTZYJAa"
    },
    {
      "cell_type": "code",
      "source": [
        "# Same amount of zero and ones\n",
        "class_0_count, class_1_count= df1.Churn.value_counts()\n",
        "class_0_count, class_1_count\n",
        "df_class_0 = df1[df1['Churn']==0]\n",
        "df_class_1 = df1[df1['Churn']==1]"
      ],
      "metadata": {
        "id": "EztKnKXuYzjK"
      },
      "id": "EztKnKXuYzjK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Duplicate the class1 to equal to class0 number\n",
        "# Blindly duplicating the values from df_class1\n",
        "df_class1_oversample=df_class_1.sample(class_0_count,replace=True)\n",
        "df_class1_oversample.shape"
      ],
      "metadata": {
        "id": "6oNmpAvmY7It"
      },
      "id": "6oNmpAvmY7It",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# join and one data frame\n",
        "df_oversample = pd.concat([df_class1_oversample,df_class_0])\n",
        "df_oversample.shape"
      ],
      "metadata": {
        "id": "8QTBeAoaZYE0"
      },
      "id": "8QTBeAoaZYE0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_oversample['Churn'].value_counts()"
      ],
      "metadata": {
        "id": "aYOsZ1_nZfOo"
      },
      "id": "aYOsZ1_nZfOo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_oversample.drop('Churn',axis='columns')\n",
        "y = df_oversample['Churn']\n",
        "\n",
        "#Stratity y meand y_train  and y_test equal number of 0 and 1 divide\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15, stratify=y)"
      ],
      "metadata": {
        "id": "OWk0lhLgZh22"
      },
      "id": "OWk0lhLgZh22",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of classes in training Data\n",
        "y_train.value_counts()\n",
        "# 0\t1495\n",
        "# 1\t1495"
      ],
      "metadata": {
        "id": "yT_06KQiZlT_"
      },
      "id": "yT_06KQiZlT_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.value_counts()\n",
        "\n",
        "# 1\t374\n",
        "# 0\t374\n"
      ],
      "metadata": {
        "id": "EV3DFekmZo4Y"
      },
      "id": "EV3DFekmZo4Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds_oversampling = ANN(X_train, y_train, X_test, y_test, 'binary_crossentropy', -1)\n",
        "# f1 score for 0 and 1 imporved now"
      ],
      "metadata": {
        "id": "dtdQfoVvZt6R"
      },
      "id": "dtdQfoVvZt6R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3- SOMET\n",
        "- Instead blindly duplicating values in class-1\n",
        "- use k mean neighbour algo"
      ],
      "metadata": {
        "id": "SfG9kCTtaMnm"
      },
      "id": "SfG9kCTtaMnm"
    },
    {
      "cell_type": "code",
      "source": [
        "X = df1.drop('Churn',axis='columns')\n",
        "y = df1['Churn']"
      ],
      "metadata": {
        "id": "8EKg5piwap_D"
      },
      "id": "8EKg5piwap_D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.value_counts()\n",
        "# 0\t5163\n",
        "# 1\t1869"
      ],
      "metadata": {
        "id": "3yZdGF7WavZY"
      },
      "id": "3yZdGF7WavZY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote = SMOTE(sampling_strategy='minority')\n",
        "X_sm, y_sm = smote.fit_resample(X, y)\n",
        "\n",
        "# now y ka minority 1 is equal to 0\n",
        "# not blindly duplicated used k mean insternally for those rows\n",
        "y_sm.value_counts()"
      ],
      "metadata": {
        "id": "MgTUcXrea1_h"
      },
      "id": "MgTUcXrea1_h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "New x and y are ready now do train split and build model"
      ],
      "metadata": {
        "id": "8u_1vKdrbNqr"
      },
      "id": "8u_1vKdrbNqr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4- Ensemble with undersampling\n",
        "- decrease majority 0 to number of minority 1\n",
        "- use majority vote from multiple division"
      ],
      "metadata": {
        "id": "3k7OPBoPbYji"
      },
      "id": "3k7OPBoPbYji"
    },
    {
      "cell_type": "code",
      "source": [
        "df1.Churn.value_counts()"
      ],
      "metadata": {
        "id": "XHKlj5tybhh_"
      },
      "id": "XHKlj5tybhh_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Regain Original features and labels\n",
        "X = df1.drop('Churn',axis='columns')\n",
        "y = df1['Churn']"
      ],
      "metadata": {
        "id": "t9o4L_CNblm7"
      },
      "id": "t9o4L_CNblm7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training data need to be divided and Test can remain same"
      ],
      "metadata": {
        "id": "KeSqQqgUd7ld"
      },
      "id": "KeSqQqgUd7ld"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15, stratify=y)"
      ],
      "metadata": {
        "id": "iQrprF85eAbk"
      },
      "id": "iQrprF85eAbk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.value_counts()\n",
        "# 0\t4130\n",
        "# 1\t1495\n",
        "\n",
        "# need 1st split 0:1495 1495 2nd 1496:1496+1495 1495\n",
        "# Train in 3 data split of majority"
      ],
      "metadata": {
        "id": "NRDLK_bKeC7_"
      },
      "id": "NRDLK_bKeC7_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2= X_train\n",
        "df2['Churn']=y_train\n",
        "df2.shape\n",
        "df2.head()"
      ],
      "metadata": {
        "id": "2YRkz8pdcuna"
      },
      "id": "2YRkz8pdcuna",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['Churn'].value_counts()"
      ],
      "metadata": {
        "id": "_73YzfD3ebE0"
      },
      "id": "_73YzfD3ebE0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2_class0 = df2[df2['Churn']==0]\n",
        "df2_class1 = df2[df2['Churn']==1]"
      ],
      "metadata": {
        "id": "lqvyMvi8dqzu"
      },
      "id": "lqvyMvi8dqzu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2_class0.shape,df2_class1.shape\n",
        "# ((4130, 27), (1495, 27))"
      ],
      "metadata": {
        "id": "oBl8UuYNdyeV"
      },
      "id": "oBl8UuYNdyeV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_divide1 = pd.concat([df2_class0[0:1495],df2_class1])\n",
        "df_train_divide1.shape"
      ],
      "metadata": {
        "id": "oKhfp0JLd0nk"
      },
      "id": "oKhfp0JLd0nk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_divide1['Churn'].value_counts()"
      ],
      "metadata": {
        "id": "0c95FfsGe3qd"
      },
      "id": "0c95FfsGe3qd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create A fucntion to create a batch"
      ],
      "metadata": {
        "id": "HWQ20WRGe8Y1"
      },
      "id": "HWQ20WRGe8Y1"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_batch(df_majority, df_minority, start, end):\n",
        "    df_train = pd.concat([df_majority[start:end], df_minority], axis=0)\n",
        "\n",
        "    X_train = df_train.drop('Churn', axis='columns')\n",
        "    y_train = df_train.Churn\n",
        "    return X_train, y_train"
      ],
      "metadata": {
        "id": "-6PuSM2-fCqH"
      },
      "id": "-6PuSM2-fCqH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction 1: First Division used as training"
      ],
      "metadata": {
        "id": "EyejQjeCfRB7"
      },
      "id": "EyejQjeCfRB7"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8aq-5cMnmPuy"
      },
      "id": "8aq-5cMnmPuy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = get_train_batch(df2_class0, df2_class1, 0, 1495)\n",
        "y_pred1 = ANN(X_train, y_train, X_test, y_test, 'binary_crossentropy', -1)"
      ],
      "metadata": {
        "id": "hGgsLqBwfPDn"
      },
      "id": "hGgsLqBwfPDn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction 2: Second Division used as training"
      ],
      "metadata": {
        "id": "sujHlpeNmQmz"
      },
      "id": "sujHlpeNmQmz"
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = get_train_batch(df3_class0, df3_class1, 1495, 2990)\n",
        "y_pred2 = ANN(X_train, y_train, X_test, y_test, 'binary_crossentropy', -1)"
      ],
      "metadata": {
        "id": "qFEfYz5UloZF"
      },
      "id": "qFEfYz5UloZF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction 3: Third Division used as training"
      ],
      "metadata": {
        "id": "z9JLFQy1mSrE"
      },
      "id": "z9JLFQy1mSrE"
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = get_train_batch(df3_class0, df3_class1, 2990, 4130)\n",
        "\n",
        "y_pred3 = ANN(X_train, y_train, X_test, y_test, 'binary_crossentropy', -1)"
      ],
      "metadata": {
        "id": "dcLLzerdlptN"
      },
      "id": "dcLLzerdlptN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_test)\n",
        "y_pred1[0]"
      ],
      "metadata": {
        "id": "LaGw2xsclrtm"
      },
      "id": "LaGw2xsclrtm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trained the model- YPred\n",
        "- xtrain and ytrain divided equal distribution wala\n",
        "- Net ypred -> majority of 1 2 3"
      ],
      "metadata": {
        "id": "6Zecs_rSlwOI"
      },
      "id": "6Zecs_rSlwOI"
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_final = y_pred1.copy()\n",
        "for i in range(len(y_pred1)):\n",
        "    n_ones = y_pred1[i] + y_pred2[i] + y_pred3[i]\n",
        "    if n_ones>1:\n",
        "        y_pred_final[i] = 1\n",
        "    else:\n",
        "        y_pred_final[i] = 0"
      ],
      "metadata": {
        "id": "cFOKX7x2lvrv"
      },
      "id": "cFOKX7x2lvrv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cl_rep = classification_report(y_test, y_pred_final)\n",
        "print(cl_rep)\n",
        "\n",
        "#  Final report based on y_test and y_prediction_ensemble"
      ],
      "metadata": {
        "id": "nCMnjFHpmICF"
      },
      "id": "nCMnjFHpmICF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dropout Regularisation Project-4 Sonar Mine Detection\n",
        "- dealing with overfit and underfit problem\n",
        "- neural network tends to overfit when eopchs are high\n",
        "- training sample 1 -> drop 50% neurons from hidden layers\n",
        "\n",
        "### Dropout helps in overfiting\n",
        "- dropping helps neurons dont be biased with some feature kyuki woh kabhi bhi randomly drop ho sakte\n",
        "- can;t rely on one input as it might be droipout randomly\n",
        "- neurons will not learn redundant detials of input"
      ],
      "metadata": {
        "id": "8WHmfSNmGloZ"
      },
      "id": "8WHmfSNmGloZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset - Connectionist Bench Solar Mines Vs Rocks\n",
        "- binary classification problem"
      ],
      "metadata": {
        "id": "Ycd3a6iZHXfZ"
      },
      "id": "Ycd3a6iZHXfZ"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "MmQ0gWb7H2Tj"
      },
      "id": "MmQ0gWb7H2Tj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "9ZEwmapnH367"
      },
      "id": "9ZEwmapnH367",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/sonar_dataset.csv\", header=None)\n",
        "# header none since this data dont have column name so indexing\n",
        "df.sample(5)"
      ],
      "metadata": {
        "id": "grdATglWH5M9"
      },
      "id": "grdATglWH5M9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "clCEmS6uIFls"
      },
      "id": "clCEmS6uIFls",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "mF2Lvj7KIHdP"
      },
      "id": "mF2Lvj7KIHdP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "rFa2M68-IccK"
      },
      "id": "rFa2M68-IccK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[60].unique()"
      ],
      "metadata": {
        "id": "i3iEegfuIgsz"
      },
      "id": "i3iEegfuIgsz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[60].value_counts()"
      ],
      "metadata": {
        "id": "saQvRSbNIJEW"
      },
      "id": "saQvRSbNIJEW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### x and y"
      ],
      "metadata": {
        "id": "t7CXCR_AIo2M"
      },
      "id": "t7CXCR_AIo2M"
    },
    {
      "cell_type": "code",
      "source": [
        "x= df.drop(60,axis=1)\n",
        "y= df[60]\n",
        "x"
      ],
      "metadata": {
        "id": "beqlShS4IqVt"
      },
      "id": "beqlShS4IqVt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "Bxhdu58PIwLZ"
      },
      "id": "Bxhdu58PIwLZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Label Encoding y\n",
        "- M = 1\n",
        "- R = 0\n",
        "\n",
        "OR\n",
        "### Dummies create Y-> R and M column mein\n",
        "- y = pd.get_dummies(y, drop_first=True) ek hi columns R naam ka bacha\n",
        "- y.sample(5) # R --> 1 and M --> 0"
      ],
      "metadata": {
        "id": "hiYQvXOQJChY"
      },
      "id": "hiYQvXOQJChY"
    },
    {
      "cell_type": "code",
      "source": [
        "y = y.map({'R': 0, 'M': 1})"
      ],
      "metadata": {
        "id": "IMLr6zawJGbX"
      },
      "id": "IMLr6zawJGbX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.value_counts()"
      ],
      "metadata": {
        "id": "O80Z8POTJGrt"
      },
      "id": "O80Z8POTJGrt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=1)"
      ],
      "metadata": {
        "id": "ECe3-2S3Jm3p"
      },
      "id": "ECe3-2S3Jm3p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head()"
      ],
      "metadata": {
        "id": "bzexK_mLJrwI"
      },
      "id": "bzexK_mLJrwI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "gxO9kosQJ3OC"
      },
      "id": "gxO9kosQJ3OC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Learning Model\n",
        "- Use sigmoid + binary_crossentropy when the output is one unit (1 neuron) with 0/1 label.\n",
        "- One hot ouput\n",
        "  -  labels are one-hot encoded (e.g., [1, 0] or [0, 1]), you'd need:\n",
        "  - 2 output units  softmax activation categorical_crossentropy loss\n",
        "  - keras.layers.Dense(2, activation='softmax')  # 2 output units for one-hot"
      ],
      "metadata": {
        "id": "0573UHx_Ju8E"
      },
      "id": "0573UHx_Ju8E"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Method-1 Model without Dropout Layer"
      ],
      "metadata": {
        "id": "vArYMGLmJv4d"
      },
      "id": "vArYMGLmJv4d"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "6j7AxwdcJzmg"
      },
      "id": "6j7AxwdcJzmg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(60, input_dim=60, activation='relu'),\n",
        "    keras.layers.Dense(30, activation='relu'),\n",
        "    keras.layers.Dense(15, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# binary ouput sigmoid\n",
        "# error loss = binary_crossenetr\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# batch size randomly 8 samples\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=8)"
      ],
      "metadata": {
        "id": "XP8MM46hJ08b"
      },
      "id": "XP8MM46hJ08b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Overfit training time accuracy 1 but here problem\n",
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "6b5YJIm_LYvY"
      },
      "id": "6b5YJIm_LYvY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model.predict(X_test)\n",
        "print(pred,pred.shape)\n",
        "y_pred= pred.reshape(-1)\n",
        "#  (52, 1) to single array convert\n",
        "print(y_pred[:10])\n",
        "\n",
        "# round the values to nearest integer ie 0 or 1\n",
        "y_pred = np.round(y_pred)\n",
        "print(y_pred[:10])\n",
        "print(y_test[:10])"
      ],
      "metadata": {
        "id": "iFtkYW0LLoG7"
      },
      "id": "iFtkYW0LLoG7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Confusion matrix and report y true and predicted"
      ],
      "metadata": {
        "id": "3V2pFbf4MH64"
      },
      "id": "3V2pFbf4MH64"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix , classification_report\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "e9uTs-XCMM3x"
      },
      "id": "e9uTs-XCMM3x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Method-2 Model with Dropout Layer\n"
      ],
      "metadata": {
        "id": "ENIwTWmyMW-t"
      },
      "id": "ENIwTWmyMW-t"
    },
    {
      "cell_type": "code",
      "source": [
        "modeld = keras.Sequential([\n",
        "    keras.layers.Dense(60, input_dim=60, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    # Declaring dropout for each epcoh\n",
        "    keras.layers.Dense(30, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(15, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "modeld.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "modeld.fit(X_train, y_train, epochs=100, batch_size=8)"
      ],
      "metadata": {
        "id": "js17Ry_aMa3H"
      },
      "id": "js17Ry_aMa3H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modeld.evaluate(X_test, y_test)\n",
        "# Test set accuracy improved"
      ],
      "metadata": {
        "id": "yN7I75HGMotj"
      },
      "id": "yN7I75HGMotj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = modeld.predict(X_test).reshape(-1)\n",
        "print(y_pred[:10])\n",
        "\n",
        "# round the values to nearest integer ie 0 or 1\n",
        "y_pred = np.round(y_pred)\n",
        "print(y_pred[:10])"
      ],
      "metadata": {
        "id": "6ro_CPyTM5gE"
      },
      "id": "6ro_CPyTM5gE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix , classification_report\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "d94_EjsxM6UH"
      },
      "id": "d94_EjsxM6UH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computer Vision\n",
        "- Area of computer science dealing with image classification, object detection\n",
        "- Image Classification - > phone peoples category into classes\n",
        "- Image classification with localization -> image category + their location(drawing boxes) -> this is called object detection\n",
        "- Object Detection -> Drawing Boxes\n",
        "  - Passbook extract information\n",
        "  - Agriculture Crop/Pest Analysize\n",
        "  - Autonomous Cars\n",
        "- Possible due to neural network training acha ho raha now\n",
        "- Image Segmentation -> Pixel differentiate\n",
        "  - image category then particular image pixels belong to which object\n",
        "\n",
        "#### Datasets for Computer Vision\n",
        "- imagenet -> labels images dog cat , with bounding images object detection\n",
        "- coco -> image segmented\n",
        "  - position detection standing sitiing\n",
        "- google open images\n",
        "- tensorflow hub - pretrained models"
      ],
      "metadata": {
        "id": "kNPZNxc1pjqw"
      },
      "id": "kNPZNxc1pjqw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional Neural Network\n",
        "- ANN\n",
        "  - earlier ANN images array break (100,100,3) then we need 30000 neurons in first layer computation is very high\n",
        "  - Too much complexity\n",
        "  - Treats local pixels same as pixels far apart\n",
        "  - Sensitive to location of an image = array changes values\n"
      ],
      "metadata": {
        "id": "2Wg1RL8Nqx6Q"
      },
      "id": "2Wg1RL8Nqx6Q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Classification using CNN- CIFAR10 dataset\n",
        "  - y = one hot encoded [0 0 0 1] instead of 4 use categorical_crossentropy\n",
        "  - y= simple value 9   use sparse_categorical_crossentropy\n",
        "  - y = 0/1 use binary crossentropy\n",
        "\n",
        "- With CNN, at the end 5 epochs, accuracy was at around 70% which is a significant improvement over ANN. CNN's are best for image classification and gives superb accuracy. Also computation is much less compared to simple ANN as maxpooling reduces the image dimensions while still preserving the features"
      ],
      "metadata": {
        "id": "sH8ENr8hneLF"
      },
      "id": "sH8ENr8hneLF"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "# Keras has data, layers for models everything\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "BLv_0yRCoFGe"
      },
      "id": "BLv_0yRCoFGe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test,y_test) = datasets.cifar10.load_data()\n",
        "X_train.shape"
      ],
      "metadata": {
        "id": "uEZQE7nRoIrX"
      },
      "id": "uEZQE7nRoIrX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[0]\n",
        "# ouput is from 0 to 9\n",
        "# unique, counts = np.unique(y_train, return_counts=True)\n",
        "# value_counts = dict(zip(unique, counts))\n",
        "# print(value_counts)"
      ],
      "metadata": {
        "id": "6IUHXBxroMXj"
      },
      "id": "6IUHXBxroMXj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape\n",
        "# 2D array"
      ],
      "metadata": {
        "id": "LlnfbENVo6qR"
      },
      "id": "LlnfbENVo6qR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = y_train.reshape(-1)\n",
        "y_test = y_test.reshape(-1)\n",
        "y_train.shape\n",
        "y_train[:5]"
      ],
      "metadata": {
        "id": "Fo4XbgRzo-ju"
      },
      "id": "Fo4XbgRzo-ju",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0]\n",
        "# (32, 32, 3)"
      ],
      "metadata": {
        "id": "FnucNesIonXY"
      },
      "id": "FnucNesIonXY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"]"
      ],
      "metadata": {
        "id": "yM3rDDF1ozIf"
      },
      "id": "yM3rDDF1ozIf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(X_train[1])"
      ],
      "metadata": {
        "id": "KaZRZXvRoqjD"
      },
      "id": "KaZRZXvRoqjD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_sample(x,y,index):\n",
        "  plt.figure(figsize=(2,2))\n",
        "  plt.imshow(x[index])\n",
        "  plt.xlabel(classes[y[index]])"
      ],
      "metadata": {
        "id": "nKDLwV3fpGrP"
      },
      "id": "nKDLwV3fpGrP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_sample(X_train,y_train,3)"
      ],
      "metadata": {
        "id": "Xv3fQLZspQMn"
      },
      "id": "Xv3fQLZspQMn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalize x_train since pixel 0 to 255 value RGB"
      ],
      "metadata": {
        "id": "oc21B-YEpfXb"
      },
      "id": "oc21B-YEpfXb"
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train/255\n",
        "X_test = X_test/255"
      ],
      "metadata": {
        "id": "t-JArmU6pi_L"
      },
      "id": "t-JArmU6pi_L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0]"
      ],
      "metadata": {
        "id": "xIYsn6j5plEA"
      },
      "id": "xIYsn6j5plEA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ANN Model vs CNN Model Comparision"
      ],
      "metadata": {
        "id": "hVPW9GQfpuVv"
      },
      "id": "hVPW9GQfpuVv"
    },
    {
      "cell_type": "code",
      "source": [
        "ann = models.Sequential([\n",
        "        layers.Flatten(input_shape=(32,32,3)),\n",
        "        layers.Dense(3000, activation='relu'),\n",
        "        layers.Dense(1000, activation='relu'),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "ann.compile(optimizer='SGD',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "ann.fit(X_train, y_train, epochs=1)\n",
        "\n",
        "\n",
        "# y = one hot encoded [0 0 0 1] use categorical_crossentropy\n",
        "# y= simple value 9   use sparse_categorical_crossentropy\n",
        "# y = 0/1 use binary crossentropy"
      ],
      "metadata": {
        "id": "9591Oqw8py0t"
      },
      "id": "9591Oqw8py0t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix , classification_report\n",
        "import numpy as np\n",
        "y_pred = ann.predict(X_test)\n",
        "print(y_pred,y_pred.shape)\n",
        "# y_pred 10 classes total ka probability lega\n",
        "y_pred_classes = [np.argmax(element) for element in y_pred]\n",
        "# indexes of highest probability in each element\n",
        "print(\"Classification Report: \\n\", classification_report(y_test, y_pred_classes))"
      ],
      "metadata": {
        "id": "l-SF34Mvr-IL"
      },
      "id": "l-SF34Mvr-IL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN Method -> Keras provide\n",
        "- Layer1 Convolution Relu Pooling\n",
        "- Layer2 Again same\n",
        "- Layer 3 Goes to ANN neurons Dense Layer"
      ],
      "metadata": {
        "id": "ZkPyAXojq3L7"
      },
      "id": "ZkPyAXojq3L7"
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = models.Sequential([\n",
        "    # cnn automatically creates filters jaise 9-> upper cirle, lower cuver etc khud pata kar lega\n",
        "    # img-> Convolution: number of filter, each filter size, and activatoion function\n",
        "    layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "\n",
        "    # Pooling to reduce dimension 2X2 box mein max value to 1 value\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    # now after cnn pass to ann type with flatten\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "0rAlT70gr7JD"
      },
      "id": "0rAlT70gr7JD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "38tmgskCuAIC"
      },
      "id": "38tmgskCuAIC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.fit(X_train, y_train, epochs=1)"
      ],
      "metadata": {
        "id": "rIjuYXjauBkQ"
      },
      "id": "rIjuYXjauBkQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.evaluate(X_test,y_test)"
      ],
      "metadata": {
        "id": "2YFCl6H-vd6C"
      },
      "id": "2YFCl6H-vd6C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = cnn.predict(X_test)\n",
        "y_pred[:5]\n",
        "y_classes = [np.argmax(element) for element in y_pred]\n",
        "y_classes[:5]\n",
        "print(classes[np.argmax(y_pred[30])])"
      ],
      "metadata": {
        "id": "bMwFxrORvpIg"
      },
      "id": "bMwFxrORvpIg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_sample(X_test, y_test,30)"
      ],
      "metadata": {
        "id": "GhsD2DPPvrbX"
      },
      "id": "GhsD2DPPvrbX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix , classification_report\n",
        "print(\"Classification Report: \\n\", classification_report(y_test, y_classes))"
      ],
      "metadata": {
        "id": "d1-v0nG-wWCw"
      },
      "id": "d1-v0nG-wWCw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Difference between sigmoid and softmax actication fucntion\n",
        "- Sigmoid ouput\n",
        "1: 0.45\n",
        "2: 0.67\n",
        "\n",
        "-Softmax ouput\n",
        "1: 0.45/(0.45/0.67)\n",
        "2: 0.67/(0.45/0.67)\n",
        "\n",
        "So total sum of probability is 1 in softmax"
      ],
      "metadata": {
        "id": "CgQIY5NwrdfI"
      },
      "id": "CgQIY5NwrdfI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation | Flower Detection Project-6\n",
        "- CNN automatically not able to scale and rotate\n",
        "  - Train with samples orgiianl but once rotated image diye problem\n",
        "- Data augmentation\n",
        "  - original image -> rotate, flip, different types create new image\n",
        "  - now use these multiple images to train\n",
        "  - now test data if get rotated image no problem\n"
      ],
      "metadata": {
        "id": "6Z8DZYMDB3EO"
      },
      "id": "6Z8DZYMDB3EO"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "# helps convert image into numpy array needed for training\n",
        "# cv2.imread(path of image)\n",
        "# cv2.resize into pixels 100*100\n",
        "import os\n",
        "# opertaing system fucntion laata\n",
        "import PIL\n",
        "# open images yeh sab\n",
        "# Pyhton library to work with images\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "metadata": {
        "id": "pAPfaa5_Ej08"
      },
      "id": "pAPfaa5_Ej08",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
        "data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url,  cache_dir='/content/Photos', untar=True)\n",
        "# cache_dir indicates where to download data. I specified . which means current directory\n",
        "# untar true will unzip it"
      ],
      "metadata": {
        "id": "WOOXgUqZElXW"
      },
      "id": "WOOXgUqZElXW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Playaround with photos inside directory\n",
        "- use pathlib: listing images with .jpg these things"
      ],
      "metadata": {
        "id": "h3Hz7KwVFFx5"
      },
      "id": "h3Hz7KwVFFx5"
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir"
      ],
      "metadata": {
        "id": "uKebHf_0FFD_"
      },
      "id": "uKebHf_0FFD_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "data_dir = pathlib.Path(data_dir)\n",
        "data_dir\n",
        "#PosixPath('/content/Photos/datasets/flower_photos')"
      ],
      "metadata": {
        "id": "M7osHHX6FkPV"
      },
      "id": "M7osHHX6FkPV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = data_dir / 'flower_photos'\n",
        "# going to actual folder where flowers are present"
      ],
      "metadata": {
        "id": "KnehWeEzHAc7"
      },
      "id": "KnehWeEzHAc7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.path.exists(data_dir))  # Should return True\n",
        "print(os.listdir(data_dir))"
      ],
      "metadata": {
        "id": "JsoEsIkYGF-_"
      },
      "id": "JsoEsIkYGF-_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(data_dir.glob('*/*.jpg'))[:5]\n",
        " # No subfolder match\n",
        "# Can perform listing using data_dir path\n",
        "len(list(data_dir.glob('*/*.jpg')))"
      ],
      "metadata": {
        "id": "vVPt9muOForL"
      },
      "id": "vVPt9muOForL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GET Roses images use glob->comes under pathlib\n",
        "\n",
        "data_dir= /content/Photos/datasets/flower_photos/flower_photos\n",
        "\n",
        "now inside this directory use glob function"
      ],
      "metadata": {
        "id": "33QD53KqGrPp"
      },
      "id": "33QD53KqGrPp"
    },
    {
      "cell_type": "code",
      "source": [
        "roses = list(data_dir.glob('roses/*'))\n",
        "roses[:5]"
      ],
      "metadata": {
        "id": "sU9J5hm2Gq3J"
      },
      "id": "sU9J5hm2Gq3J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Open image in pyhton using PIL"
      ],
      "metadata": {
        "id": "OcWw_wnIHncy"
      },
      "id": "OcWw_wnIHncy"
    },
    {
      "cell_type": "code",
      "source": [
        "PIL.Image.open(str(roses[1]))"
      ],
      "metadata": {
        "id": "1T6E4F1rHp7I"
      },
      "id": "1T6E4F1rHp7I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tulips = list(data_dir.glob('tulips/*'))\n",
        "tulip[:5]\n",
        "PIL.Image.open(str(tulip[1]))"
      ],
      "metadata": {
        "id": "6ahWh2kVHx1X"
      },
      "id": "6ahWh2kVHx1X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Images Dictionary All Images of Differnet Types"
      ],
      "metadata": {
        "id": "CX2NyUCYICmT"
      },
      "id": "CX2NyUCYICmT"
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete Data of images with label unka path hai apne paas\n",
        "flowers_images_dict = {\n",
        "    'roses': list(data_dir.glob('roses/*')),\n",
        "    'daisy': list(data_dir.glob('daisy/*')),\n",
        "    'dandelion': list(data_dir.glob('dandelion/*')),\n",
        "    'sunflowers': list(data_dir.glob('sunflowers/*')),\n",
        "    'tulips': list(data_dir.glob('tulips/*')),\n",
        "}\n"
      ],
      "metadata": {
        "id": "3H8tFKONIFVk"
      },
      "id": "3H8tFKONIFVk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PIL.Image.open(str(flowers_images_dict['roses'][100]))"
      ],
      "metadata": {
        "id": "gcXBLfQ_IIPP"
      },
      "id": "gcXBLfQ_IIPP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flowers_labels_dict = {\n",
        "    'roses': 0,\n",
        "    'daisy': 1,\n",
        "    'dandelion': 2,\n",
        "    'sunflowers': 3,\n",
        "    'tulips': 4,\n",
        "}"
      ],
      "metadata": {
        "id": "v-BJX0hLIXQc"
      },
      "id": "v-BJX0hLIXQc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CV2 convert image into numpy array"
      ],
      "metadata": {
        "id": "ldR_WslfIm6D"
      },
      "id": "ldR_WslfIm6D"
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.imread(str(flowers_images_dict['roses'][0]))\n",
        "img"
      ],
      "metadata": {
        "id": "iGlD95c4IqW1"
      },
      "id": "iGlD95c4IqW1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img.shape"
      ],
      "metadata": {
        "id": "dfMgE_hSIuXV"
      },
      "id": "dfMgE_hSIuXV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(img)"
      ],
      "metadata": {
        "id": "a2i780GDI6bx"
      },
      "id": "a2i780GDI6bx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensions Same resize karo"
      ],
      "metadata": {
        "id": "eINdtkDSJFZ5"
      },
      "id": "eINdtkDSJFZ5"
    },
    {
      "cell_type": "code",
      "source": [
        "img_resized= cv2.resize(img,(100,100))\n",
        "img_resized"
      ],
      "metadata": {
        "id": "lj1chlDBJHhx"
      },
      "id": "lj1chlDBJHhx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare x and y"
      ],
      "metadata": {
        "id": "OMdQmB5RLRE0"
      },
      "id": "OMdQmB5RLRE0"
    },
    {
      "cell_type": "code",
      "source": [
        "x,y = [],[]\n",
        "# x=[rose1,rose2,tulip2,tulip2]\n",
        "# y=[0,0,1,1]\n",
        "for flower_name, images in flowers_images_dict.items():\n",
        "    for image in images:\n",
        "        img = cv2.imread(str(image))\n",
        "        resized_img = cv2.resize(img,(180,180))\n",
        "        x.append(resized_img)\n",
        "        y.append(flowers_labels_dict[flower_name])"
      ],
      "metadata": {
        "id": "wGv8BEaGLTM-"
      },
      "id": "wGv8BEaGLTM-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x= np.array(x)\n",
        "y=np.array(y)"
      ],
      "metadata": {
        "id": "5zsfwjF_Lw_U"
      },
      "id": "5zsfwjF_Lw_U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "id": "S3c9mzXgMBtn"
      },
      "id": "S3c9mzXgMBtn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "id": "FmGdU1g2MCuJ"
      },
      "id": "FmGdU1g2MCuJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training using CNN"
      ],
      "metadata": {
        "id": "495IvHu8MKBW"
      },
      "id": "495IvHu8MKBW"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, random_state=0)"
      ],
      "metadata": {
        "id": "tSK-SSJKMMCA"
      },
      "id": "tSK-SSJKMMCA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling Images 255 RGB"
      ],
      "metadata": {
        "id": "eN3aTqyiMPKg"
      },
      "id": "eN3aTqyiMPKg"
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled = X_train / 255\n",
        "X_test_scaled = X_test / 255"
      ],
      "metadata": {
        "id": "0b9G9pGaMRJm"
      },
      "id": "0b9G9pGaMRJm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build ANN Model"
      ],
      "metadata": {
        "id": "NWnp32ctMlxr"
      },
      "id": "NWnp32ctMlxr"
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 5\n",
        "\n",
        "model = Sequential([\n",
        "    # 16 is no. of filter and 3*3 is filter size\n",
        "  layers.Conv2D(16, 3, padding='same', activation='relu',input_shape=(180, 180, 3)),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Flatten(),\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(num_classes)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train_scaled, y_train, epochs=1)"
      ],
      "metadata": {
        "id": "1_hck5mkMoxm"
      },
      "id": "1_hck5mkMoxm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test_scaled,y_test)"
      ],
      "metadata": {
        "id": "h3vGKg1kNzHM"
      },
      "id": "h3vGKg1kNzHM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(X_test_scaled)\n",
        "predictions"
      ],
      "metadata": {
        "id": "JOK8-MV1N3yt"
      },
      "id": "JOK8-MV1N3yt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_classes= [np.argmax(element) for element in predictions]\n",
        "prediction_classes[0]"
      ],
      "metadata": {
        "id": "nQL8_lnpN-7M"
      },
      "id": "nQL8_lnpN-7M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert Final output y prection into softmax i.e. a/a+b+c+d+e bas aise hi"
      ],
      "metadata": {
        "id": "vAL3PVwSOTf3"
      },
      "id": "vAL3PVwSOTf3"
    },
    {
      "cell_type": "code",
      "source": [
        "# Normal array of probability convert into softman\n",
        "score = tf.nn.softmax(predictions[0])\n",
        "score"
      ],
      "metadata": {
        "id": "6YpLYTAjOceV"
      },
      "id": "6YpLYTAjOceV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmax(score)"
      ],
      "metadata": {
        "id": "H6tLb5F3OpIe"
      },
      "id": "H6tLb5F3OpIe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test[0]"
      ],
      "metadata": {
        "id": "dUwFqLIhOP7M"
      },
      "id": "dUwFqLIhOP7M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Augmentation\n",
        "- img hai numpy and can display\n",
        "- convert rotate, zoom, flip on that img generate new image"
      ],
      "metadata": {
        "id": "ACPBe0aUQarr"
      },
      "id": "ACPBe0aUQarr"
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomZoom(0.5)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "QxwKNbvxQjYO"
      },
      "id": "QxwKNbvxQjYO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(x[0])"
      ],
      "metadata": {
        "id": "qlfkwIBlREjW"
      },
      "id": "qlfkwIBlREjW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_images_x=data_augmentation(x)"
      ],
      "metadata": {
        "id": "5Cfn4s_ARKMS"
      },
      "id": "5Cfn4s_ARKMS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.axis('off')\n",
        "plt.imshow(augmented_images_x[0].numpy().astype(\"uint8\"))"
      ],
      "metadata": {
        "id": "MBsO2K4eRQeG"
      },
      "id": "MBsO2K4eRQeG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the model using data augmentation and a drop out layer\n",
        "- create keras layer of data augmentation"
      ],
      "metadata": {
        "id": "8DpPQh1mSMFQ"
      },
      "id": "8DpPQh1mSMFQ"
    },
    {
      "cell_type": "code",
      "source": [
        "x[0]"
      ],
      "metadata": {
        "id": "OdxWblFSSc9v"
      },
      "id": "OdxWblFSSc9v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "  [\n",
        "    layers.RandomFlip(\"horizontal\",\n",
        "                                                 input_shape=(180, 180, 3)),\n",
        "    layers.RandomRotation(0.1),\n",
        "    layers.RandomZoom(0.1),\n",
        "  ]\n",
        ")"
      ],
      "metadata": {
        "id": "3IA5-1ZsSP9a"
      },
      "id": "3IA5-1ZsSP9a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 5\n",
        "\n",
        "model = Sequential([\n",
        "    # x_train- > data_augment -> those new_x_train->further layer to train\n",
        "    # use all the layers defined above and use those images for training too\n",
        "  data_augmentation,\n",
        "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.Flatten(),\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(num_classes)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# first layer data augmentation jo banaya\n",
        "model.fit(X_train_scaled, y_train, epochs=1)"
      ],
      "metadata": {
        "id": "1x_gWQC2SicQ"
      },
      "id": "1x_gWQC2SicQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test_scaled,y_test)"
      ],
      "metadata": {
        "id": "rdTK7vUKSngC"
      },
      "id": "rdTK7vUKSngC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning\n",
        "- Use PRetrained model which is made by solving one problem\n",
        "- use to solve another problem\n",
        "- Tensorflow Hub : Saves computation"
      ],
      "metadata": {
        "id": "mn6fpNubFOlh"
      },
      "id": "mn6fpNubFOlh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model we use:\n",
        "- Mobilenet V2 model TensflowHub -> google trained 1000 classes fish, animals, flowers\n",
        "- freeze layers of mobilenet toh weights directly unka use kar liye\n",
        "- can add our own some layer-> GET OUR OWN FLOWER Classficiation-> 5 classes 5 types of flower\n",
        "\n",
        "- can omit last layer of pretrained model and add our layer:\n",
        "- last layer generally is signmoid softmax type\n",
        "\n"
      ],
      "metadata": {
        "id": "2x70FYFGFY27"
      },
      "id": "2x70FYFGFY27"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "import PIL.Image as Image\n",
        "import os\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "# to download model mobilenet\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras import Input, Model\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "metadata": {
        "id": "E7LhoZEFF0BY"
      },
      "id": "E7LhoZEFF0BY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"TF Hub version:\", hub.__version__)"
      ],
      "metadata": {
        "id": "wPHCGooxJAtA"
      },
      "id": "wPHCGooxJAtA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the model instance\n",
        "- download the model .pd\n",
        "- use online url\n",
        "- give your input layer to the model and create ouput"
      ],
      "metadata": {
        "id": "ZxzJmDmRLrFc"
      },
      "id": "ZxzJmDmRLrFc"
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SHAPE = (128, 128)\n",
        "# Given on website what type of image they used for training\n",
        "\n",
        "# Method-1 Using online given model\n",
        "# classifier = tf.keras.Sequential([\n",
        "#     hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4\", input_shape=IMAGE_SHAPE+(3,))\n",
        "# ])\n",
        "\n",
        "\n",
        "\n",
        "# Method-2 Download the models .pb file and use it\n",
        "# Define input shape expected by your model\n",
        "inputs = Input(shape=(128, 128, 3))  # or whatever the model expects\n",
        "\n",
        "# Wrap SavedModel as a layer\n",
        "mobilenet_layer = keras.layers.TFSMLayer(\"/content/mobilenet\", call_endpoint=\"serving_default\")\n",
        "\n",
        "# Use it in a Keras model\n",
        "outputs = mobilenet_layer(inputs)\n",
        "model = Model(inputs, outputs)\n"
      ],
      "metadata": {
        "id": "y85MAEf_GukX"
      },
      "id": "y85MAEf_GukX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SHAPE+(3,)"
      ],
      "metadata": {
        "id": "HZze-NWTPtZw"
      },
      "id": "HZze-NWTPtZw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt\n",
        "- Classes this model is trained on\n",
        "- labels = [\"background\", \"tench\", \"goldfish\", \"great white shark\", \"tiger shark\", \"hammerhead\", \"electric ray\", \"stingray\", \"cock\", \"hen\", \"ostrich\", \"brambling\", \"goldfinch\", \"house finch\", \"junco\", \"indigo bunting\", \"robin\", \"bulbul\", \"jay\", \"magpie\", \"chickadee\", \"water ouzel\", \"kite\", \"bald eagle\", \"vulture\" , etc]\n"
      ],
      "metadata": {
        "id": "WUFwyXOSL4wi"
      },
      "id": "WUFwyXOSL4wi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check the model using one class image"
      ],
      "metadata": {
        "id": "mx7zTikVMCf6"
      },
      "id": "mx7zTikVMCf6"
    },
    {
      "cell_type": "code",
      "source": [
        "gold_fish=Image.open('/content/goldfish (1).jpg').resize(IMAGE_SHAPE)\n",
        "gold_fish.show()\n",
        "# If it looks blank/gray: the image is corrupted or invalid."
      ],
      "metadata": {
        "id": "F7ak38VsMR6Y"
      },
      "id": "F7ak38VsMR6Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gold_fish = gold_fish.convert('RGB')\n",
        "gold_fish"
      ],
      "metadata": {
        "id": "CJ5Csn88NkWX"
      },
      "id": "CJ5Csn88NkWX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(gold_fish)"
      ],
      "metadata": {
        "id": "FPjZySS_Noum"
      },
      "id": "FPjZySS_Noum",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalize the image /255"
      ],
      "metadata": {
        "id": "XZhCI_bUM_MR"
      },
      "id": "XZhCI_bUM_MR"
    },
    {
      "cell_type": "code",
      "source": [
        "gold_fish= np.array(gold_fish)/255.0\n",
        "gold_fish.shape"
      ],
      "metadata": {
        "id": "k0jToe84NBXG"
      },
      "id": "k0jToe84NBXG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction of a model\n",
        "- accepts multiple images as input\n",
        "- dimension (number of images, each image dimension)\n",
        "- for one image need to add one dimension\n",
        "  - gold_fish[np.newaxis, ...]\n",
        "  - (1, 224,224,3) means one image to prediction function"
      ],
      "metadata": {
        "id": "91Jsv9B5ODdn"
      },
      "id": "91Jsv9B5ODdn"
    },
    {
      "cell_type": "code",
      "source": [
        "gold_fish_with_correct_prediction_dimension=gold_fish[np.newaxis, ...]"
      ],
      "metadata": {
        "id": "5Odz7WYEOOHj"
      },
      "id": "5Odz7WYEOOHj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gold_fish_with_correct_prediction_dimension.shape"
      ],
      "metadata": {
        "id": "f0eAQ_nrOjmj"
      },
      "id": "f0eAQ_nrOjmj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediciton with model"
      ],
      "metadata": {
        "id": "-mUKNeEuOkf4"
      },
      "id": "-mUKNeEuOkf4"
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.predict(gold_fish_with_correct_prediction_dimension)\n",
        "type(result['logits'])"
      ],
      "metadata": {
        "id": "V3fOAtaWOmOS"
      },
      "id": "V3fOAtaWOmOS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result['logits']\n",
        "# 1001 classes probability"
      ],
      "metadata": {
        "id": "uF87YaejP63d"
      },
      "id": "uF87YaejP63d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_index= np.argmax(result['logits'])\n",
        "predicted_index"
      ],
      "metadata": {
        "id": "EsSQC5ctQQ8u"
      },
      "id": "EsSQC5ctQQ8u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image label/classes given on website as .txt"
      ],
      "metadata": {
        "id": "lEgMP4-hQZSE"
      },
      "id": "lEgMP4-hQZSE"
    },
    {
      "cell_type": "code",
      "source": [
        "image_labels=[]\n",
        "with open(\"/content/ImageNetLabels.txt\",\"r\") as f:\n",
        "  image_labels=f.read().splitlines()\n",
        "image_labels[:5]\n",
        "# ['background', 'tench', 'goldfish',\n",
        "#  so index 2 is goldfish"
      ],
      "metadata": {
        "id": "jX3b6dTqQcz2"
      },
      "id": "jX3b6dTqQcz2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now take pre-trained model and retrain it using flowers images\n",
        "- since flower data has ouput\n",
        "  - roses': 0,\n",
        "    'daisy': 1,\n",
        "    'dandelion': 2,\n",
        "    'sunflowers': 3,\n",
        "    'tulips': 4,\n",
        "  - these are not in 1001 classes of pretrained model\n",
        "  - remove the last layer and add Dense layer with 5 output\n",
        "- Model aate hai feature vector name- Last layer removed hota hai"
      ],
      "metadata": {
        "id": "a_rsVpa_SEGC"
      },
      "id": "a_rsVpa_SEGC"
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor_model = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
        "\n",
        "pretrained_model_without_top_layer = hub.KerasLayer(\n",
        "    feature_extractor_model, input_shape=(224, 224, 3), trainable=False)"
      ],
      "metadata": {
        "id": "jviRISjwXjg9"
      },
      "id": "jviRISjwXjg9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_of_flowers = 5\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    # adding a dense layer in the end of the feaure model\n",
        "  pretrained_model_without_top_layer,\n",
        "  tf.keras.layers.Dense(num_of_flowers)\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "AobvucfvXkKX"
      },
      "id": "AobvucfvXkKX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flowers data wala code again X and Y create Same as above"
      ],
      "metadata": {
        "id": "7Il69xbfXvXR"
      },
      "id": "7Il69xbfXvXR"
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "  optimizer=\"adam\",\n",
        "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "  metrics=['acc'])\n",
        "\n",
        "model.fit(X_train_scaled, y_train, epochs=5)"
      ],
      "metadata": {
        "id": "HYYzxMS5Xrr2"
      },
      "id": "HYYzxMS5Xrr2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test_scaled,y_test)"
      ],
      "metadata": {
        "id": "rHGi1IRmX13M"
      },
      "id": "rHGi1IRmX13M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sliding Window Object Detection\n",
        "How Boxes are Draw\n",
        "- whether a dog and a person\n",
        "- try different sizes iteratively -> rectangle\n",
        "- box of some size slides in complete image\n",
        "- eahc time cnn se ask is this dog-> YES correct box\n",
        "\n",
        "#### Sliding Window Object Detection -> R CNN -> Fast R CNN -> Faster R CNN\n",
        " #### Nowayas YOLO You only look once\n",
        " - R CNN means one boundary rectangle and lside the box to detect image\n",
        " - problem too much computation for each image"
      ],
      "metadata": {
        "id": "6wJrnuTgrPNg"
      },
      "id": "6wJrnuTgrPNg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YOLO Algo Object Detection\n",
        "- Boxes on images detection\n",
        "- standard way of detecting object in computer vision\n",
        "- History:\n",
        "  - Yolov1 2016 video frames work"
      ],
      "metadata": {
        "id": "lKrqCNz_QV4e"
      },
      "id": "lKrqCNz_QV4e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pretrained Model Use Object Detection\n",
        "- Coco Dataset\n",
        "- Github repo YOLOv4 and its setup for object detection"
      ],
      "metadata": {
        "id": "9BhDLi-vb2QM"
      },
      "id": "9BhDLi-vb2QM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN - Word embedding - Keras Embedding\n",
        "- RNNs consume embeddings as input, including Word2Vec or learned ones\n",
        "  - Designed for sequential data, like text or time series.\n",
        "- convert words into numbers preserving relationship between words\n",
        "\n",
        "- After words into numbers can use those as inputs to other model\n",
        "- Word Embedding\n",
        "  - Word2Vec Its a method for training word embeddings.\n",
        "  - GloVe\n",
        "\n",
        "  - FastText\n",
        "\n",
        "  - BERT (contextual embeddings)\n",
        "\n",
        "- model.wv['nlp']  [0.23, -0.14, ..., 0.65]  # 100 dimensions\n",
        "\n",
        "      Raw Text  [Tokenize]  [Embed Words]\n",
        "                                \n",
        "              (Word2Vec OR trainable embeddings)\n",
        "                                \n",
        "       [Embedding Vectors Sequence]  [RNN/LSTM/GRU/Transformer]\n",
        "                                \n",
        "                    [Text Classification / Prediction]"
      ],
      "metadata": {
        "id": "RNxxhlaWGIWN"
      },
      "id": "RNxxhlaWGIWN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method-1 Word Embedding- Supervised Learning"
      ],
      "metadata": {
        "id": "478tOTDdOfYO"
      },
      "id": "478tOTDdOfYO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theory of implementation in colab notes"
      ],
      "metadata": {
        "id": "JfAILkQMHGbI"
      },
      "id": "JfAILkQMHGbI"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "reviews = ['nice food',\n",
        "        'amazing restaurant',\n",
        "        'too good',\n",
        "        'just loved it!',\n",
        "        'will go again',\n",
        "        'horrible food',\n",
        "        'never go there',\n",
        "        'poor service',\n",
        "        'poor quality',\n",
        "        'needs improvement']\n",
        "\n",
        "sentiment = np.array([1,1,1,1,1,0,0,0,0,0])"
      ],
      "metadata": {
        "id": "q6Abzv5eIHIY"
      },
      "id": "q6Abzv5eIHIY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "vocal_size  = Total number of words expecting\n",
        "embedd vector size = 4 like want each word 4 features"
      ],
      "metadata": {
        "id": "PtRPlTcEIJTs"
      },
      "id": "PtRPlTcEIJTs"
    },
    {
      "cell_type": "code",
      "source": [
        "# Give random number 1 to 30 to each word\n",
        "one_hot(\"amazing restaurant\",30)"
      ],
      "metadata": {
        "id": "HeWbdGhqIITm"
      },
      "id": "HeWbdGhqIITm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 30\n",
        "encoded_reviews = [one_hot(d, vocab_size) for d in reviews]\n",
        "print(encoded_reviews)\n",
        "# each value in our x get assigned using one_hot"
      ],
      "metadata": {
        "id": "ZtNUJdGuIWyr"
      },
      "id": "ZtNUJdGuIWyr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Padding according to maximum sentence length input"
      ],
      "metadata": {
        "id": "mYKyW8ItIfl9"
      },
      "id": "mYKyW8ItIfl9"
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 4\n",
        "padded_reviews = pad_sequences(encoded_reviews, maxlen=max_length, padding='post')\n",
        "print(padded_reviews)"
      ],
      "metadata": {
        "id": "aInLOSiFIizb"
      },
      "id": "aInLOSiFIizb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While Training the model-> Embedding layer\n",
        "- convert workds into one hot -> then padding and all internally"
      ],
      "metadata": {
        "id": "LqIFHg44IpvM"
      },
      "id": "LqIFHg44IpvM"
    },
    {
      "cell_type": "code",
      "source": [
        "embeded_vector_size = 4 # number of features\n",
        "\n",
        "model = Sequential()\n",
        "# 30 size 4 each feature\n",
        "model.add(Embedding(vocab_size, embeded_vector_size, input_length=max_length,name=\"embedding\"))\n",
        "# flatten the ouput and give to sigmoit\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "t5ZPIxyFIpgH"
      },
      "id": "t5ZPIxyFIpgH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = padded_reviews\n",
        "# aFter one hot encoding and padding all words data into numbers\n",
        "y = sentiment\n",
        "# already in array form"
      ],
      "metadata": {
        "id": "REcerhI5JEjW"
      },
      "id": "REcerhI5JEjW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "v9Gel_7LJOf3"
      },
      "id": "v9Gel_7LJOf3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, y, epochs=50, verbose=0)"
      ],
      "metadata": {
        "id": "I418HDbiJVpo"
      },
      "id": "I418HDbiJVpo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the model\n",
        "loss, accuracy = model.evaluate(X, y)\n",
        "accuracy"
      ],
      "metadata": {
        "id": "ZoKbqIONJdW3"
      },
      "id": "ZoKbqIONJdW3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Got all weights of each word->\n",
        "Basically got the feature vector of each word"
      ],
      "metadata": {
        "id": "wr4l-Cs2JfrD"
      },
      "id": "wr4l-Cs2JfrD"
    },
    {
      "cell_type": "code",
      "source": [
        "weights = model.get_layer('embedding').get_weights()[0]\n",
        "len(weights)"
      ],
      "metadata": {
        "id": "EJDU2O2BJkrF"
      },
      "id": "EJDU2O2BJkrF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights[13] # One hot encoding hua hai\n",
        "# All vocab ka weight"
      ],
      "metadata": {
        "id": "5BbM-P3DJmre"
      },
      "id": "5BbM-P3DJmre",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method-2 Word2Vec - Project7: Amazon Review Genism Library\n",
        "- Genism NLP library\n",
        "- Train a model\n",
        "- Amazon customer reiver\n",
        "  - The dataset we are using here is a subset of Amazon reviews from the Cell Phones & Accessories category. The data is stored as a JSON file and can be read using pandas.\n",
        "  - .gz file zipped file\n",
        "  - Link to the Dataset: http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Cell_Phones_and_Accessories_5.json.gz\n",
        "  - donwload using\n",
        "    - curl -O http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Cell_Phones_and_Accessories_5.json.gz\n",
        "\n",
        "- Colab noted theory given"
      ],
      "metadata": {
        "id": "7ukPiDAqOjbT"
      },
      "id": "7ukPiDAqOjbT"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        " !pip install gensim\n",
        " !pip install python-Levenshtein"
      ],
      "metadata": {
        "id": "OR0OCceotS-0"
      },
      "id": "OR0OCceotS-0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "oiRaVNbHtH2D"
      },
      "id": "oiRaVNbHtH2D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unzip the gz file"
      ],
      "metadata": {
        "id": "tweXoOTpwLYm"
      },
      "id": "tweXoOTpwLYm"
    },
    {
      "cell_type": "code",
      "source": [
        "!gunzip /content/reviews_Cell_Phones_and_Accessories_5.json.gz"
      ],
      "metadata": {
        "id": "7vWSnKnlwNoE"
      },
      "id": "7vWSnKnlwNoE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_json(\"/content/reviews_Cell_Phones_and_Accessories_5.json\", lines=True)\n",
        "df"
      ],
      "metadata": {
        "id": "ZvcW9Dt3vkwv"
      },
      "id": "ZvcW9Dt3vkwv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Review Text only as X"
      ],
      "metadata": {
        "id": "d7IHAOxUwjv-"
      },
      "id": "d7IHAOxUwjv-"
    },
    {
      "cell_type": "code",
      "source": [
        "df.reviewText[0]"
      ],
      "metadata": {
        "id": "IEDjLO0lwlTb"
      },
      "id": "IEDjLO0lwlTb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing gensim.utils.simple_preprocess\n",
        "- stopwords\n",
        "- lowercase\n",
        "- remove punctualtion\n",
        "- tokenizing the sentence array of words convert\n",
        "- For NLP, we apply various processing like converting all the words to lower case, trimming spaces, removing punctuations. This is something we will do over here too."
      ],
      "metadata": {
        "id": "fOx93Qe_wrbG"
      },
      "id": "fOx93Qe_wrbG"
    },
    {
      "cell_type": "code",
      "source": [
        "review_text = df.reviewText.apply(gensim.utils.simple_preprocess)"
      ],
      "metadata": {
        "id": "DfhDebdvw3rN"
      },
      "id": "DfhDebdvw3rN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_text"
      ],
      "metadata": {
        "id": "Db1WNqRUw5FV"
      },
      "id": "Db1WNqRUw5FV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training the Word2Vec Model\n",
        "Train the model for reviews. Use a window of size 10 i.e. 10 words before the present word and 10 words ahead. A sentence with at least 2 words should only be considered, configure this using min_count parameter."
      ],
      "metadata": {
        "id": "tXXai-uLxuGi"
      },
      "id": "tXXai-uLxuGi"
    },
    {
      "cell_type": "code",
      "source": [
        "model = gensim.models.Word2Vec(\n",
        "    window=10, # Theory mein kitna window slide karnege usme hi x and y decide\n",
        "    min_count=2, # if sentecnce less than 2 words dont take into training\n",
        "    workers=4, # how many cpu thread\n",
        ")"
      ],
      "metadata": {
        "id": "4b4MY40_xvxH"
      },
      "id": "4b4MY40_xvxH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build Vocabulary\n",
        "- Building unique number for words"
      ],
      "metadata": {
        "id": "KirNMSlCx9vn"
      },
      "id": "KirNMSlCx9vn"
    },
    {
      "cell_type": "code",
      "source": [
        "model.build_vocab(review_text, progress_per=1000)\n",
        "# after how many words you need to see progresss bar"
      ],
      "metadata": {
        "id": "SnsaAvZTx6CV"
      },
      "id": "SnsaAvZTx6CV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the Word2Vec Model"
      ],
      "metadata": {
        "id": "Gbn6dblryMdx"
      },
      "id": "Gbn6dblryMdx"
    },
    {
      "cell_type": "code",
      "source": [
        "model.corpus_count"
      ],
      "metadata": {
        "id": "3qv-72HGybJ5"
      },
      "id": "3qv-72HGybJ5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train(review_text, total_examples=model.corpus_count, epochs=model.epochs)\n"
      ],
      "metadata": {
        "id": "ogM4A-rYyM7M"
      },
      "id": "ogM4A-rYyM7M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get Vector for a word"
      ],
      "metadata": {
        "id": "vHVXu0jUzgpK"
      },
      "id": "vHVXu0jUzgpK"
    },
    {
      "cell_type": "code",
      "source": [
        "vector = model.wv['works']\n",
        "print(vector)"
      ],
      "metadata": {
        "id": "_gLlBDWtzc8s"
      },
      "id": "_gLlBDWtzc8s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding Similar Words and Similarity between words"
      ],
      "metadata": {
        "id": "XewWeCANzyg8"
      },
      "id": "XewWeCANzyg8"
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar(\"bad\")"
      ],
      "metadata": {
        "id": "lG_a3JGVzzWm"
      },
      "id": "lG_a3JGVzzWm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarity between two words"
      ],
      "metadata": {
        "id": "gzy_rdNcz3Ar"
      },
      "id": "gzy_rdNcz3Ar"
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.similarity(w1=\"cheap\", w2=\"inexpensive\")"
      ],
      "metadata": {
        "id": "WiAR0e65z1I4"
      },
      "id": "WiAR0e65z1I4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the model to a file\n",
        "- use this pretrained model"
      ],
      "metadata": {
        "id": "YGPhGWYmylDe"
      },
      "id": "YGPhGWYmylDe"
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/content/save model/modelmy\")"
      ],
      "metadata": {
        "id": "XtK2gcVFznfN"
      },
      "id": "XtK2gcVFznfN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distributed Training\n",
        "- 50k images 1000 batch -> 4CPU type\n",
        "- same image classfication using CNN\n",
        "- just see the spliiting and providing to different gpu syntax\n",
        "\n",
        "## Tensorflow Pipeling use\n",
        "- tensorflow mirror strategy\n",
        "- tf dataset create -> Split dataset and send to each gpu\n",
        "\n",
        "- Convert all x and y into tf dataset\n",
        "- Then use this tf dataset for replica batch size, strategy etc"
      ],
      "metadata": {
        "id": "qVUtIR6A4Uby"
      },
      "id": "qVUtIR6A4Uby"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Bzf3qZO54dxt"
      },
      "id": "Bzf3qZO54dxt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.experimental.list_physical_devices()"
      ],
      "metadata": {
        "id": "Sav-Cin4FgO0"
      },
      "id": "Sav-Cin4FgO0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.test.is_built_with_cuda()"
      ],
      "metadata": {
        "id": "sisqAdn1FjBe"
      },
      "id": "sisqAdn1FjBe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the dataset\n",
        "- Our dataset contains 60000 small training images that belongs to one of the below 10 classes"
      ],
      "metadata": {
        "id": "oB1hcG7eFjUu"
      },
      "id": "oB1hcG7eFjUu"
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test,y_test) = tf.keras.datasets.cifar10.load_data()"
      ],
      "metadata": {
        "id": "BF6_qbvHFn1R"
      },
      "id": "BF6_qbvHFn1R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "u0vOJY8uFpK9"
      },
      "id": "u0vOJY8uFpK9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "id": "WjoTb3f7FqlE"
      },
      "id": "WjoTb3f7FqlE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"]"
      ],
      "metadata": {
        "id": "cyYotcbUFsQF"
      },
      "id": "cyYotcbUFsQF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.shape"
      ],
      "metadata": {
        "id": "sCldVRz7FuKf"
      },
      "id": "sCldVRz7FuKf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing: Scale images"
      ],
      "metadata": {
        "id": "zVGFQnwYFupv"
      },
      "id": "zVGFQnwYFupv"
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled = X_train / 255\n",
        "X_test_scaled = X_test / 255"
      ],
      "metadata": {
        "id": "loSnH1tkFwxs"
      },
      "id": "loSnH1tkFwxs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_categorical = keras.utils.to_categorical(\n",
        "    y_train, num_classes=10\n",
        ")\n",
        "y_test_categorical = keras.utils.to_categorical(\n",
        "    y_test, num_classes=10\n",
        ")"
      ],
      "metadata": {
        "id": "LAW_KkfGF_wi"
      },
      "id": "LAW_KkfGF_wi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model building and training"
      ],
      "metadata": {
        "id": "8UttH9xzGFye"
      },
      "id": "8UttH9xzGFye"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model():\n",
        "    model = keras.Sequential([\n",
        "            keras.layers.Flatten(input_shape=(32,32,3)),\n",
        "            keras.layers.Dense(3000, activation='relu'),\n",
        "            keras.layers.Dense(1000, activation='relu'),\n",
        "            keras.layers.Dense(10, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "    model.compile(optimizer='SGD',\n",
        "                  # since y converted into categories\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "eX7FSmjiGGKc"
      },
      "id": "eX7FSmjiGGKc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensorflow Pipeling Use\n",
        "- first tf dataset\n",
        "- strategy - uses gpu number and replicas\n",
        "- batch size and replica for training"
      ],
      "metadata": {
        "id": "QvQAHTGZGK_i"
      },
      "id": "QvQAHTGZGK_i"
    },
    {
      "cell_type": "code",
      "source": [
        "train_tf_dataset = tf.data.Dataset.from_tensor_slices((X_train_scaled, y_train_categorical))\n",
        "test_tf_dataset = tf.data.Dataset.from_tensor_slices((X_test_scaled, y_test_categorical))"
      ],
      "metadata": {
        "id": "raT7OtPAGV96"
      },
      "id": "raT7OtPAGV96",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For batches and parallel processing among gpu\n",
        "strategy = tf.distribute.MirroredStrategy()"
      ],
      "metadata": {
        "id": "iq_k2ShYGXan"
      },
      "id": "iq_k2ShYGXan",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "strategy.num_replicas_in_sync"
      ],
      "metadata": {
        "id": "lBEU0QyQGYmY"
      },
      "id": "lBEU0QyQGYmY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE_PER_REPLICA = 250\n",
        "BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
        "#train_tf_dataset contains 10,000 samples\n",
        "#train_dataset = train_tf_dataset.batch(500)\n",
        "# train_dataset has 20batches\n",
        "# Each epoch = all 20 batches (entire 10,000 samples)\n",
        "# and each loop 50 epoch uses all 4 gpus\n",
        "\n",
        "# autoTUNE each GPU or CPU for trainign un batches ko de rahe\n",
        "train_dataset = train_tf_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_tf_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "# train_dataset divided into batches\n",
        "# Until all 20 batches are processed (this is called one epoch)"
      ],
      "metadata": {
        "id": "PS9vi_PLGg9y"
      },
      "id": "PS9vi_PLGg9y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(train_dataset)"
      ],
      "metadata": {
        "id": "ybvORW8TGrQo"
      },
      "id": "ybvORW8TGrQo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n1 -r1\n",
        "# Now complete train_dataset is trained but with different gpu\n",
        "# Inside the strategy.scope(), you are creating and training gpu_model using all available GPUs (based on your tf.distribute.Strategy\n",
        "with strategy.scope():\n",
        "    gpu_model = get_model()\n",
        "    gpu_model.fit(train_dataset, epochs=5)"
      ],
      "metadata": {
        "id": "TEz-q-reHKlV"
      },
      "id": "TEz-q-reHKlV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Measure training time on a CPU"
      ],
      "metadata": {
        "id": "1sQ1TgrEIUhH"
      },
      "id": "1sQ1TgrEIUhH"
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n1 -r1\n",
        "# if seperately train on particular cpu\n",
        "with tf.device('/CPU:0'):\n",
        "    cpu_model = get_model()\n",
        "    cpu_model.fit(train_dataset, epochs=5)"
      ],
      "metadata": {
        "id": "aH2UmWyaIVTI"
      },
      "id": "aH2UmWyaIVTI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensorflow Pipeline\n",
        "- data Step by step read\n",
        "- lots of data 2tb then create a steps take 10gb trainf it somehting like this"
      ],
      "metadata": {
        "id": "YYTkMrNAExvS"
      },
      "id": "YYTkMrNAExvS"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
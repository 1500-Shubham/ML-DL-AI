{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Handwritten Digits - Image Project\n",
        "- Tensorflow and Keras to create neural network\n",
        "  - takes input output and layers details like\n",
        "    - activcation fucntion\n",
        "    - loss fucntion\n",
        "    - accuracy metrics\n",
        "- Confusion matrix between predcited and test\n",
        "- plot using sns"
      ],
      "metadata": {
        "id": "zzJ3VuJ_i3eh"
      },
      "id": "zzJ3VuJ_i3eh"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "#It tells the notebook to display matplotlib plots directly below the code cells that produce them, rather than in a separate window.\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "-ufdhgfNi7na"
      },
      "id": "-ufdhgfNi7na",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist"
      ],
      "metadata": {
        "id": "ekH16FdAkBgR"
      },
      "id": "ekH16FdAkBgR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Databaset using Keras\n",
        "- Train and Split in Keras"
      ],
      "metadata": {
        "id": "F7BYfvKplv5b"
      },
      "id": "F7BYfvKplv5b"
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "THJm4jCkkRA_"
      },
      "id": "THJm4jCkkRA_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Each Image as 2D array of 28x28\n",
        "\n",
        "len(x_train)\n",
        "len(y_train)\n",
        "len(x_test)\n",
        "# X train and y_train are 2d arrays"
      ],
      "metadata": {
        "id": "YqR4ZzMvkl35"
      },
      "id": "YqR4ZzMvkl35",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MNIST\n",
        "- Handwritten digits data in 2D form\n",
        "- 0 means black 255 means white"
      ],
      "metadata": {
        "id": "u430bXOckukr"
      },
      "id": "u430bXOckukr"
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x_train[0]))\n",
        "print(x_train[0])\n",
        "# This is 2D array"
      ],
      "metadata": {
        "id": "KBkqqsfQk18g"
      },
      "id": "KBkqqsfQk18g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### See Image using Matplot\n",
        "- plt.matshow(2D array)"
      ],
      "metadata": {
        "id": "dXyHJd_vlMOT"
      },
      "id": "dXyHJd_vlMOT"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.matshow(x_train[100])\n",
        "print(y_train[100])"
      ],
      "metadata": {
        "id": "_ecDtgXFlQUR"
      },
      "id": "_ecDtgXFlQUR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Flatten 2D array 28X28 into 1D array"
      ],
      "metadata": {
        "id": "4auSI99-l1ZN"
      },
      "id": "4auSI99-l1ZN"
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape\n",
        "# number of rows\n",
        "# each rows is 2d array"
      ],
      "metadata": {
        "id": "LRl59aQGl6rT"
      },
      "id": "LRl59aQGl6rT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Want (60000,784)\n",
        "x_train_flattened = x_train.reshape(len(x_train),28*28)\n",
        "x_test_flattened = x_test.reshape(len(x_test),28*28)\n",
        "x_train_flattened.shape"
      ],
      "metadata": {
        "id": "G8uE_FGTl53q"
      },
      "id": "G8uE_FGTl53q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_flattened[0]\n",
        "print(len(x_train_flattened[0]))"
      ],
      "metadata": {
        "id": "n3vm7uMkmXJW"
      },
      "id": "n3vm7uMkmXJW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scaling the x_train data between 0 to 1\n",
        "- Divide by 255"
      ],
      "metadata": {
        "id": "OBbKyyCEpoNF"
      },
      "id": "OBbKyyCEpoNF"
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_flattened = x_train_flattened/255"
      ],
      "metadata": {
        "id": "5f1ULrfJptF0"
      },
      "id": "5f1ULrfJptF0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keras to Create Neural Network\n",
        "- want 784 as x1 x2 ... .x784 to connect with 10 ouputs 0 to 1\n",
        "- acitvation function = sigmoid\n",
        "- Then training using data yeh sab\n",
        "\n",
        "- Single layer input and output for now"
      ],
      "metadata": {
        "id": "c7QkkqnOnE7t"
      },
      "id": "c7QkkqnOnE7t"
    },
    {
      "cell_type": "code",
      "source": [
        "# Dense(10): A fully connected layer with 10 neurons\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(10,input_shape=(784,),activation='softmax')\n",
        "])\n",
        "\n",
        "# If dont want to flatten image tell its 28*28\n",
        "# Multiple layers hit and trial\n",
        "# model = keras.Sequential([\n",
        "      # keras.layers.Flatten(input_shape=(28,28))\n",
        "#     keras.layers.Dense(100,input_shape=(784,),activation='relu'),\n",
        "#     keras.layers.Dense(10,activation='softmax')\n",
        "# ])\n",
        "\n",
        "\n",
        "# optimizer backward going to optain weight attain global minima\n",
        "# loss fucntion ouput and predicted difference used to attain weights\n",
        "# Gradient decent and cost function\n",
        "\n",
        "# metric when neural network compile goal is accuracy input output differtence\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        "    )\n",
        "# epochs number of iteration the neural network going to train data\n",
        "model.fit(x_train_flattened,y_train,epochs=5)"
      ],
      "metadata": {
        "id": "gj94-wPHnNlx"
      },
      "id": "gj94-wPHnNlx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating | Predicition Keral Model on Test Dataset"
      ],
      "metadata": {
        "id": "s3rI94vup9Ex"
      },
      "id": "s3rI94vup9Ex"
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x_test_flattened,y_test)"
      ],
      "metadata": {
        "id": "S_nEsCthqARu"
      },
      "id": "S_nEsCthqARu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.matshow(x_test[50])"
      ],
      "metadata": {
        "id": "02__G9G_qrrV"
      },
      "id": "02__G9G_qrrV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict= model.predict(x_test_flattened)\n",
        "print(y_predict[50])\n",
        "# This is 10 neurons output probability"
      ],
      "metadata": {
        "id": "D7x7ovHQrXty"
      },
      "id": "D7x7ovHQrXty",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exact ouput based on 10 neurons\n",
        "# np.argmax returns the index of the max probability, i.e., the predicted class.\n",
        "print(np.argmax(y_predict[50]))"
      ],
      "metadata": {
        "id": "Co7wulzurnLG"
      },
      "id": "Co7wulzurnLG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test[0:5]"
      ],
      "metadata": {
        "id": "kpWS0X_wtEGz"
      },
      "id": "kpWS0X_wtEGz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert y_predicted into concrete class similar to y_test"
      ],
      "metadata": {
        "id": "mwHX7hH_tNsk"
      },
      "id": "mwHX7hH_tNsk"
    },
    {
      "cell_type": "code",
      "source": [
        "y_predicted_labels = np.argmax(y_predict, axis=1)"
      ],
      "metadata": {
        "id": "4AmrvBjatVGS"
      },
      "id": "4AmrvBjatVGS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predicted_labels[0:5]"
      ],
      "metadata": {
        "id": "2v1sOa6ytWk0"
      },
      "id": "2v1sOa6ytWk0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Confusion Matrix"
      ],
      "metadata": {
        "id": "DT5ArGQes7Rv"
      },
      "id": "DT5ArGQes7Rv"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_predicted_labels)\n",
        "print(conf_matrix)\n",
        "\n",
        "# confusion_matrix = tf.math.confusion.matrix(labels=y_test, predictions=y_predicted_labels)\n",
        "# confusion_matrix"
      ],
      "metadata": {
        "id": "xa-FD69Ms-Q0"
      },
      "id": "xa-FD69Ms-Q0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize using seaborn"
      ],
      "metadata": {
        "id": "ij_jRKB2t2Bm"
      },
      "id": "ij_jRKB2t2Bm"
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "VIs7_8wwt4z9"
      },
      "id": "VIs7_8wwt4z9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=np.arange(10), yticklabels=np.arange(10))\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('MNIST Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WVZur1qZuJh_"
      },
      "id": "WVZur1qZuJh_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PKQFAFOYvT46"
      },
      "id": "PKQFAFOYvT46"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activation Functions\n",
        "- Use Sigmoid at output layer = [probability 0,1]\n",
        "- Between layers use tanh\n",
        "\n",
        "- Not sure of activation Use Relu max(0,x)\n",
        "- Leaky Relu max(01.x,x)"
      ],
      "metadata": {
        "id": "FRWn90lg-x26"
      },
      "id": "FRWn90lg-x26"
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "neNPjeFr_Ajf"
      },
      "id": "neNPjeFr_Ajf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid\n",
        "def sigmoid(x):\n",
        "  return 1/(1+math.exp(-x))"
      ],
      "metadata": {
        "id": "GQyZg7fs-xag"
      },
      "id": "GQyZg7fs-xag",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sigmoid(-0)"
      ],
      "metadata": {
        "id": "ZR8ikMnD_HGg"
      },
      "id": "ZR8ikMnD_HGg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tanh\n",
        "def tanh(x):\n",
        "  return (math.exp(x)-math.exp(-x))/(math.exp(x)+math.exp(-x))"
      ],
      "metadata": {
        "id": "DyWzbfP4_I5W"
      },
      "id": "DyWzbfP4_I5W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tanh(100)"
      ],
      "metadata": {
        "id": "E6zWV6VH_VKh"
      },
      "id": "E6zWV6VH_VKh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RELU\n",
        "def relu(x):\n",
        "  return max(0,x)\n",
        "def leaky_relu(x):\n",
        "  return max(0.1*x,x)"
      ],
      "metadata": {
        "id": "VEvHJrOp_W-I"
      },
      "id": "VEvHJrOp_W-I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relu(0.5)\n",
        "leaky_relu(-0.5)"
      ],
      "metadata": {
        "id": "ixanqQEn_aZq"
      },
      "id": "ixanqQEn_aZq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent in Neural\n",
        "- Keras directly model leke train and fit\n",
        "- get weights and bias after epochs\n",
        "- Python simple implementation for x and y matrix to get weights"
      ],
      "metadata": {
        "id": "oanPGZ6X0x9n"
      },
      "id": "oanPGZ6X0x9n"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "vtQOfIdh063d"
      },
      "id": "vtQOfIdh063d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"insurance_data.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "6JwCLWHT2Hn7"
      },
      "id": "6JwCLWHT2Hn7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = df[['age','affordibility']]\n",
        "x.head()\n",
        "y= df['bought_insurance']\n",
        "y.head()\n",
        "x.shape"
      ],
      "metadata": {
        "id": "1xAyVnXX2I4H"
      },
      "id": "1xAyVnXX2I4H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and Test and Scaling Data"
      ],
      "metadata": {
        "id": "T5LKOnTd2ZKF"
      },
      "id": "T5LKOnTd2ZKF"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[['age','affordibility']],df.bought_insurance,test_size=0.2, random_state=25)\n"
      ],
      "metadata": {
        "id": "ZOr0v6wl2XYe"
      },
      "id": "ZOr0v6wl2XYe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Scaling Data Age / 100"
      ],
      "metadata": {
        "id": "T5pcCCtC2nOS"
      },
      "id": "T5pcCCtC2nOS"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train_scaled = X_train.copy()\n",
        "X_train_scaled['age'] = X_train_scaled['age'] / 100\n",
        "\n",
        "X_test_scaled = X_test.copy()\n",
        "X_test_scaled['age'] = X_test_scaled['age'] / 100"
      ],
      "metadata": {
        "id": "NTcrwM2V2e3N"
      },
      "id": "NTcrwM2V2e3N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled.head()"
      ],
      "metadata": {
        "id": "feS8dMx32jwL"
      },
      "id": "feS8dMx32jwL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model-1 Keras and Tensorflow"
      ],
      "metadata": {
        "id": "kZIGsH_F2rOE"
      },
      "id": "kZIGsH_F2rOE"
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs to one output can use model from sklearn like logisitc but can be converted into\n",
        "# neurals if written in this way\n",
        "\n",
        "# Logistic Regression in Keras = Neural Network with:\n",
        "# 1 Dense layer\n",
        "\n",
        "# No hidden layers\n",
        "\n",
        "# Sigmoid activation (for binary classification)\n",
        "\n",
        "# Binary crossentropy loss\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(1, input_shape=(2,), activation='sigmoid', kernel_initializer='ones', bias_initializer='zeros')\n",
        "])\n",
        "\n",
        "# since logistic regression hai loss fucntion is log loss\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train_scaled, y_train, epochs=500)"
      ],
      "metadata": {
        "id": "xVj-5sNq2ubP"
      },
      "id": "xVj-5sNq2ubP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate the model"
      ],
      "metadata": {
        "id": "Cg3m7PMb3yQZ"
      },
      "id": "Cg3m7PMb3yQZ"
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test_scaled,y_test)"
      ],
      "metadata": {
        "id": "tyfGopKJ31Sb"
      },
      "id": "tyfGopKJ31Sb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "A9SAZx8y33MT"
      },
      "id": "A9SAZx8y33MT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_scaled"
      ],
      "metadata": {
        "id": "-BfefXEX37HE"
      },
      "id": "-BfefXEX37HE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "id": "Uduloo1k4Gk8"
      },
      "id": "Uduloo1k4Gk8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coeff and Bias from the model which was trained\n",
        "- This means w1=5.060867, w2=1.4086502, bias =-2.9137027"
      ],
      "metadata": {
        "id": "55lII7064LAy"
      },
      "id": "55lII7064LAy"
    },
    {
      "cell_type": "code",
      "source": [
        "coef, intercept = model.get_weights()\n",
        "coef, intercept"
      ],
      "metadata": {
        "id": "Kc2aWP_k4OTK"
      },
      "id": "Kc2aWP_k4OTK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model-2 Using Python Done in Siddharsan Video\n",
        "- Loss function know\n",
        "- x y w1 w2 b and gradiet descent is know\n",
        "- learning rate and number of iteration known"
      ],
      "metadata": {
        "id": "3EMoc4FK4UlB"
      },
      "id": "3EMoc4FK4UlB"
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_numpy(X):\n",
        "   return 1/(1+np.exp(-X))\n",
        "\n",
        "sigmoid_numpy(np.array([12,0,1]))"
      ],
      "metadata": {
        "id": "1pBwfTfJ4ekF"
      },
      "id": "1pBwfTfJ4ekF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_loss(y_true, y_predicted):\n",
        "    epsilon = 1e-15\n",
        "    y_predicted_new = [max(i,epsilon) for i in y_predicted]\n",
        "    y_predicted_new = [min(i,1-epsilon) for i in y_predicted_new]\n",
        "    y_predicted_new = np.array(y_predicted_new)\n",
        "    return -np.mean(y_true*np.log(y_predicted_new)+(1-y_true)*np.log(1-y_predicted_new))"
      ],
      "metadata": {
        "id": "ntTMVSqx5Mtf"
      },
      "id": "ntTMVSqx5Mtf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(age, affordability, y_true, epochs, loss_thresold):\n",
        "    w1 = w2 = 1\n",
        "    bias = 0\n",
        "    rate = 0.5 #Learning rate fix\n",
        "    n = len(age)\n",
        "    for i in range(epochs):\n",
        "        weighted_sum = w1 * age + w2 * affordability + bias\n",
        "        y_predicted = sigmoid_numpy(weighted_sum)\n",
        "        loss = log_loss(y_true, y_predicted)\n",
        "\n",
        "        w1d = (1/n)*np.dot(np.transpose(age),(y_predicted-y_true))\n",
        "        w2d = (1/n)*np.dot(np.transpose(affordability),(y_predicted-y_true))\n",
        "\n",
        "        bias_d = np.mean(y_predicted-y_true)\n",
        "        w1 = w1 - rate * w1d\n",
        "        w2 = w2 - rate * w2d\n",
        "        bias = bias - rate * bias_d\n",
        "\n",
        "        print (f'Epoch:{i}, w1:{w1}, w2:{w2}, bias:{bias}, loss:{loss}')\n",
        "\n",
        "        if loss<=loss_thresold:\n",
        "            break\n",
        "\n",
        "    return w1, w2, bias"
      ],
      "metadata": {
        "id": "9c3YiUK25XuG"
      },
      "id": "9c3YiUK25XuG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_descent(X_train_scaled['age'],X_train_scaled['affordibility'],y_train,500, 0.4631)"
      ],
      "metadata": {
        "id": "UdpONgyB5h2B"
      },
      "id": "UdpONgyB5h2B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coef, intercept"
      ],
      "metadata": {
        "id": "hblwdfW552MY"
      },
      "id": "hblwdfW552MY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network Implement Python\n",
        "- want fit, predict functions like models have inbuild"
      ],
      "metadata": {
        "id": "tJTB98TfoBYX"
      },
      "id": "tJTB98TfoBYX"
    },
    {
      "cell_type": "code",
      "source": [
        "class my_neural_network():\n",
        "  def __init__(self):\n",
        "    self.w1 = 1\n",
        "    self.w2=1\n",
        "    self.bias=0\n",
        "\n",
        "  def gradient_descent(self,age, affordability, y_true, epochs, loss_thresold):\n",
        "    w1 = w2 = 1\n",
        "    bias = 0\n",
        "    rate = 0.5 #Learning rate fix\n",
        "    n = len(age)\n",
        "    for i in range(epochs):\n",
        "        weighted_sum = w1 * age + w2 * affordability + bias\n",
        "        y_predicted = self.sigmoid_numpy(weighted_sum)\n",
        "        loss = self.log_loss(y_true, y_predicted)\n",
        "\n",
        "        w1d = (1/n)*np.dot(np.transpose(age),(y_predicted-y_true))\n",
        "        w2d = (1/n)*np.dot(np.transpose(affordability),(y_predicted-y_true))\n",
        "\n",
        "        bias_d = np.mean(y_predicted-y_true)\n",
        "        w1 = w1 - rate * w1d\n",
        "        w2 = w2 - rate * w2d\n",
        "        bias = bias - rate * bias_d\n",
        "        if i%10==0:\n",
        "          print (f'Epoch:{i}, w1:{w1}, w2:{w2}, bias:{bias}, loss:{loss}')\n",
        "\n",
        "        if loss<=loss_thresold:\n",
        "            break\n",
        "\n",
        "    return w1, w2, bias\n",
        "\n",
        "  def fit(self, x, y, epochs, loss_threshold):\n",
        "    self.w1 , self.w2, self.bias =self.gradient_descent(x['age'],x['affordibility'],y, epochs, loss_threshold)\n",
        "\n",
        "  def log_loss(self,y_true, y_predicted):\n",
        "    epsilon = 1e-15\n",
        "    y_predicted_new = [max(i,epsilon) for i in y_predicted]\n",
        "    y_predicted_new = [min(i,1-epsilon) for i in y_predicted_new]\n",
        "    y_predicted_new = np.array(y_predicted_new)\n",
        "    return -np.mean(y_true*np.log(y_predicted_new)+(1-y_true)*np.log(1-y_predicted_new))\n",
        "\n",
        "  def sigmoid_numpy(self,X):\n",
        "   return 1/(1+np.exp(-X))\n",
        "\n",
        "  def predict(self,x_test):\n",
        "    weighted_sum = self.w1*x_test['age'] + self.w2*x_test['affordibility'] + self.bias\n",
        "    return self.sigmoid_numpy(weighted_sum)"
      ],
      "metadata": {
        "id": "Ftm-7fnxoATm"
      },
      "id": "Ftm-7fnxoATm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customModel= my_neural_network()\n",
        "customModel.fit(X_train_scaled,y_train,epochs=100,loss_threshold=0.4631)"
      ],
      "metadata": {
        "id": "ataWKT6Iow0Q"
      },
      "id": "ataWKT6Iow0Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customModel.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "HVZpfAw0q69Z"
      },
      "id": "HVZpfAw0q69Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradien Descent Types\n",
        "- Bathc Gradient Descent\n",
        "  - All data points for every epoch goes through and calculate loss and adjust weight like we did before\n",
        "  - Problem 10millions rows and 5000 featutes then too much computation\n",
        "  - Good for small data\n",
        "- Stochastic Gradient\n",
        "  - use one(randomply picked) - loss y predict and adjust weight on every epochs\n",
        "  - good for high data\n",
        "- Mini Batch\n",
        "  - Similar to stochatic\n",
        "  - instead of one data use batch of some data"
      ],
      "metadata": {
        "id": "T_lznNSysN6C"
      },
      "id": "T_lznNSysN6C"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "r6DmOU9zsNaQ"
      },
      "id": "r6DmOU9zsNaQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"homeprices_banglore.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "7YAb1oQzugvo"
      },
      "id": "7YAb1oQzugvo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing data - Standard Scaler"
      ],
      "metadata": {
        "id": "PIFJvTMZvJ46"
      },
      "id": "PIFJvTMZvJ46"
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "4e79jUFvvO1F"
      },
      "id": "4e79jUFvvO1F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "c0y4So5VwLK1"
      },
      "id": "c0y4So5VwLK1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "sx = preprocessing.MinMaxScaler()\n",
        "sy = preprocessing.MinMaxScaler()\n",
        "\n",
        "x= df.drop('price',axis=1)\n",
        "y= df['price']\n",
        "scaled_X = sx.fit_transform(x)\n",
        "scaled_y = sy.fit_transform(y.values.reshape(-1,1))\n",
        "# -1 all rows and 1 means one column (rows,feature)\n",
        "scaled_X\n",
        "scaled_y"
      ],
      "metadata": {
        "id": "rMRi7wyBvSsd"
      },
      "id": "rMRi7wyBvSsd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert y to single array again\n",
        "scaled_y = scaled_y.flatten()\n",
        "scaled_y"
      ],
      "metadata": {
        "id": "LSDCTQ6dvxEx"
      },
      "id": "LSDCTQ6dvxEx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.ones(shape=(2,2))"
      ],
      "metadata": {
        "id": "Oue4RgD957p4"
      },
      "id": "Oue4RgD957p4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch Descent Gradient\n",
        "- use all data point of x to go through weights and bias\n",
        "- since our data is continuos\n",
        "- cost function is mean squared error\n",
        "- dJ/dw and dJ/db\n",
        "  - w_grad = -(2/total_samples)*(X.T.dot(y_true-y_predicted))\n",
        "  - b_grad = -(2/total_samples)*np.sum(y_true-y_predicted)\n",
        "        "
      ],
      "metadata": {
        "id": "yEyqE5Q85-Hr"
      },
      "id": "yEyqE5Q85-Hr"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "number_of_features = scaled_X.shape[1]\n",
        "print(scaled_X.T.shape)\n",
        "  # numpy array with 1 row and columns equal to number of features. In\n",
        "    # our case number_of_features = 2 (area, bedroom)\n",
        "w = np.ones(shape=(number_of_features))\n",
        "w.shape\n",
        "w"
      ],
      "metadata": {
        "id": "YpKOlMKf6_RN"
      },
      "id": "YpKOlMKf6_RN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C35iZirQ7Rg_"
      },
      "id": "C35iZirQ7Rg_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_gradient_descent(X, y_true, epochs, learning_rate = 0.01):\n",
        "\n",
        "    number_of_features = X.shape[1]\n",
        "    # numpy array with 1 row and columns equal to number of features. In\n",
        "    # our case number_of_features = 2 (area, bedroom)\n",
        "    w = np.ones(shape=(number_of_features))  # 1Xfeautures\n",
        "    b = 0\n",
        "    total_samples = X.shape[0] # number of rows in X\n",
        "\n",
        "    cost_list = []\n",
        "    epoch_list = []\n",
        "\n",
        "    for i in range(epochs):\n",
        "        y_predicted = np.dot(w, X.T) + b\n",
        "\n",
        "        w_grad = -(2/total_samples)*(X.T.dot(y_true-y_predicted))\n",
        "        b_grad = -(2/total_samples)*np.sum(y_true-y_predicted)\n",
        "\n",
        "        w = w - learning_rate * w_grad\n",
        "        b = b - learning_rate * b_grad\n",
        "\n",
        "        cost = np.mean(np.square(y_true-y_predicted)) # MSE (Mean Squared Error)\n",
        "\n",
        "        if i%10==0:\n",
        "            cost_list.append(cost)\n",
        "            epoch_list.append(i)\n",
        "\n",
        "    return w, b, cost, cost_list, epoch_list\n",
        "\n",
        "w, b, cost, cost_list, epoch_list = batch_gradient_descent(scaled_X,scaled_y.reshape(scaled_y.shape[0],),500)\n",
        "w, b, cost"
      ],
      "metadata": {
        "id": "OEFk2a0V6JcY"
      },
      "id": "OEFk2a0V6JcY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"cost\")\n",
        "plt.plot(epoch_list,cost_list)"
      ],
      "metadata": {
        "id": "K5Sp4-_a8QJg"
      },
      "id": "K5Sp4-_a8QJg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Predict Values using this model weight and bias"
      ],
      "metadata": {
        "id": "H3VqZ9xz82cC"
      },
      "id": "H3VqZ9xz82cC"
    },
    {
      "cell_type": "code",
      "source": [
        "check = sx.transform([[2400, 4]])[0]\n",
        "check\n",
        "# array([[0.55172414, 0.75      ]])\n",
        "# need first array\n",
        "\n",
        "price = w[0] * check[0] + w[1] * check[1] + b\n",
        "price"
      ],
      "metadata": {
        "id": "VqG-T5b_9hTz"
      },
      "id": "VqG-T5b_9hTz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn import preprocessing\n",
        "# sx = preprocessing.MinMaxScaler()\n",
        "# sy = preprocessing.MinMaxScaler()\n",
        "# used for training and standarisation\n",
        "\n",
        "def predict(area,bedrooms,w,b):\n",
        "\n",
        "    scaled_X = sx.transform([[area, bedrooms]])[0]\n",
        "    # pass array of 2d return array of 2d need firrst element\n",
        "\n",
        "    # here w1 = w[0] , w2 = w[1], w3 = w[2] and bias is b\n",
        "    # equation for price is w1*area + w2*bedrooms + w3*age + bias\n",
        "    # scaled_X[0] is area\n",
        "    # scaled_X[1] is bedrooms\n",
        "    # scaled_X[2] is age\n",
        "    scaled_price = w[0] * scaled_X[0] + w[1] * scaled_X[1] + b\n",
        "    # once we get price prediction we need to to rescal it back to original value\n",
        "    # also since it returns 2D array, to get single value we need to do value[0][0]\n",
        "    return sy.inverse_transform([[scaled_price]])[0][0]\n",
        "\n",
        "predict(1056,2,w,b)\n",
        "# w and b from gradient descent"
      ],
      "metadata": {
        "id": "J88oY4cH86qn"
      },
      "id": "J88oY4cH86qn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stochastic Descent Gradient\n",
        "- one random sample each epochs"
      ],
      "metadata": {
        "id": "lLh51fXg-mUf"
      },
      "id": "lLh51fXg-mUf"
    },
    {
      "cell_type": "code",
      "source": [
        "# we will use random libary to pick random training sample.\n",
        "import random\n",
        "random.randint(0,6)\n",
        "\n",
        "\n",
        "# scaled_y.reshape(scaled_y.shape[0],)\n",
        "\n",
        "#2d array (20, 1) needed for tranformation\n",
        "scaled_y.shape\n",
        "\n",
        "# convert to single array (20,)\n",
        "a= scaled_y.reshape(scaled_y.shape[0],)\n",
        "a.shape\n",
        "\n",
        "number_of_features = scaled_X.shape[1]\n",
        "w = np.ones(shape=(number_of_features))\n",
        "sample_x = scaled_X[3]\n",
        "\n",
        "c= np.dot(w, sample_x.T)\n",
        "c"
      ],
      "metadata": {
        "id": "Ds1hDFrs-vGQ"
      },
      "id": "Ds1hDFrs-vGQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stochastic_gradient_descent(X, y_true, epochs, learning_rate = 0.01):\n",
        "\n",
        "    number_of_features = X.shape[1]\n",
        "    # numpy array with 1 row and columns equal to number of features. In\n",
        "    # our case number_of_features = 3 (area, bedroom and age)\n",
        "    w = np.ones(shape=(number_of_features))\n",
        "    b = 0\n",
        "    total_samples = X.shape[0]\n",
        "\n",
        "    cost_list = []\n",
        "    epoch_list = []\n",
        "\n",
        "    for i in range(epochs):\n",
        "        # here randomly one sample le rahe\n",
        "        random_index = random.randint(0,total_samples-1) # random index from total samples\n",
        "        sample_x = X[random_index]\n",
        "        sample_y = y_true[random_index]\n",
        "        # Both sample y and y_predicted is single float value\n",
        "        y_predicted = np.dot(w, sample_x.T) + b\n",
        "\n",
        "        w_grad = -(2/total_samples)*(sample_x.T.dot(sample_y-y_predicted))\n",
        "        b_grad = -(2/total_samples)*(sample_y-y_predicted)\n",
        "\n",
        "        w = w - learning_rate * w_grad\n",
        "        b = b - learning_rate * b_grad\n",
        "\n",
        "        cost = np.square(sample_y-y_predicted)\n",
        "\n",
        "        if i%100==0: # at every 100th iteration record the cost and epoch value\n",
        "            cost_list.append(cost)\n",
        "            epoch_list.append(i)\n",
        "\n",
        "    return w, b, cost, cost_list, epoch_list\n",
        "\n",
        "w_sgd, b_sgd, cost_sgd, cost_list_sgd, epoch_list_sgd = stochastic_gradient_descent(scaled_X,scaled_y.reshape(scaled_y.shape[0],),10000)\n",
        "w_sgd, b_sgd, cost_sgd"
      ],
      "metadata": {
        "id": "1lTI4_yA-v7T"
      },
      "id": "1lTI4_yA-v7T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"cost\")\n",
        "plt.plot(epoch_list_sgd,cost_list_sgd)"
      ],
      "metadata": {
        "id": "pMuIVhyBBuX2"
      },
      "id": "pMuIVhyBBuX2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(1000,2,w_sgd, b_sgd)"
      ],
      "metadata": {
        "id": "He4H07jPCo_p"
      },
      "id": "He4H07jPCo_p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mini-Batch Gradient Descent\n",
        "- Batch of 20 samples aise lo"
      ],
      "metadata": {
        "id": "1wss5SXIC3qs"
      },
      "id": "1wss5SXIC3qs"
    },
    {
      "cell_type": "code",
      "source": [
        "random_indices= np.random.permutation(20)\n",
        "X_tmp = scaled_X[random_indices]\n",
        "X_tmp\n",
        "y_tmp = scaled_y.reshape(scaled_y.shape[0],)[random_indices]\n",
        "y_tmp"
      ],
      "metadata": {
        "id": "_6sRwnCuC5yl"
      },
      "id": "_6sRwnCuC5yl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mini_batch_gradient_descent(X, y_true, epochs = 100, batch_size = 5, learning_rate = 0.01):\n",
        "\n",
        "    number_of_features = X.shape[1]\n",
        "    # numpy array with 1 row and columns equal to number of features. In\n",
        "    # our case number_of_features = 3 (area, bedroom and age)\n",
        "    w = np.ones(shape=(number_of_features))\n",
        "    b = 0\n",
        "    total_samples = X.shape[0] # number of rows in X\n",
        "\n",
        "    if batch_size > total_samples: # In this case mini batch becomes same as batch gradient descent\n",
        "        batch_size = total_samples\n",
        "\n",
        "    cost_list = []\n",
        "    epoch_list = []\n",
        "\n",
        "    num_batches = int(total_samples/batch_size)\n",
        "\n",
        "    for i in range(epochs):\n",
        "        random_indices = np.random.permutation(total_samples)\n",
        "        X_tmp = X[random_indices]\n",
        "        y_tmp = y_true[random_indices]\n",
        "\n",
        "        for j in range(0,total_samples,batch_size):\n",
        "            Xj = X_tmp[j:j+batch_size]\n",
        "            yj = y_tmp[j:j+batch_size]\n",
        "            y_predicted = np.dot(w, Xj.T) + b\n",
        "\n",
        "            w_grad = -(2/len(Xj))*(Xj.T.dot(yj-y_predicted))\n",
        "            b_grad = -(2/len(Xj))*np.sum(yj-y_predicted)\n",
        "\n",
        "            w = w - learning_rate * w_grad\n",
        "            b = b - learning_rate * b_grad\n",
        "\n",
        "            cost = np.mean(np.square(yj-y_predicted)) # MSE (Mean Squared Error)\n",
        "\n",
        "        if i%10==0:\n",
        "            cost_list.append(cost)\n",
        "            epoch_list.append(i)\n",
        "\n",
        "    return w, b, cost, cost_list, epoch_list\n",
        "\n",
        "w, b, cost, cost_list, epoch_list = mini_batch_gradient_descent(\n",
        "    scaled_X,\n",
        "    scaled_y.reshape(scaled_y.shape[0],),\n",
        "    epochs = 120,\n",
        "    batch_size = 5\n",
        ")\n",
        "w, b, cost"
      ],
      "metadata": {
        "id": "AnWNYKbdDD23"
      },
      "id": "AnWNYKbdDD23",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"cost\")\n",
        "plt.plot(epoch_list,cost_list)"
      ],
      "metadata": {
        "id": "0z-4FBmZD5Ys"
      },
      "id": "0z-4FBmZD5Ys",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}